% TODO:
% * 打开\makecover
% * 添加数组融合的图片
% * 把SC论文中其他部分添加进来（还有不少新内容）
%
\documentclass[degree=doctor]{thuthesis}
% 选项：
%   degree=[bachelor|master|doctor|postdoctor], % 必选
%   secret,                                     % 可选
%   pifootnote,                                 % 可选（建议打开）
%   openany|openright,                          % 可选，基本不用
%   arial,                                      % 可选，基本不用
%   arialtoc,                                   % 可选，基本不用
%   arialtitle                                  % 可选，基本不用

% 所有其它可能用到的包都统一放到这里了，可以根据自己的实际添加或者删除。
\usepackage{thuthesis}
\usepackage{bm}
\usepackage{lscape}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{silence}
\WarningFilter{latex}{Float too large}
% 定义所有的图片文件在 figures 子目录下
\graphicspath{{figures/}}

% 可以在这里修改配置文件中的定义。导言区可以使用中文。
% \def\myname{薛瑞尼}
\begin{document}

%%% 封面部分
\frontmatter
\input{data/cover}
% 如果使用授权说明扫描页，将可选参数中指定为扫描得到的 PDF 文件名，例如：
% \makecover[scan-auth.pdf]
% TODO:
%\makecover
%% 目录
\tableofcontents

%% 符号对照表
\input{data/denotation}


%%% 正文部分
\mainmatter
\chapter{基于十亿亿次神威超算的高分辨率唐山大地震模拟}
\label{chap:earthquake}

\section{背景知识和算法概述}
\label{sec:earth_algorithm}

在中国传统文化中，对学者的最高评价是“上知天文，下知地理”。自然科学与技术在上世纪取得了长足的进步，然而，地球内部的构造、运动模式、可预测性等问题对于科学家来说仍旧充满未知与神秘。因此，地震等对人类社会生命财产安全造成巨大损失的重大灾害则成为了等待着科学家破译的最终科学挑战之一\citep{anderson1989theory}。

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{seismoscope.png}
\caption{张衡在公元132年设计的地震仪的内部结构和外观\citep{hsiao2009review}。}
\label{fig:heng-scope}
\end{figure}

公元132年，东汉著名天文学家张衡设计的地动仪可能是中国历史上最早的抗震减灾科学工作之一\citep{stein2009introduction}。如图~\ref{fig:heng-scope}所示，地动仪有八个方位，每个方位上均有口含龙珠的龙头，在每条龙头的下方都有一只蟾蜍与其对应。任何一方如有地震发生，该方向龙口所含龙珠即落入蟾蜍口中，由此便可测出发生地震的方向\cite{seismoscopewiki}。张衡在约2000年前设计的地动仪已展现出与起源于1880年代的地震测量仪器极其相似的技术特征。

在20世纪，随着科技的发展，天然地震信号的采集和测量设备的测量精度越来越准确，这也使得基于统计的方法逐渐发展成为各种形式的地震风险分析预测的主要工具。

在最近的二三十年里，随着超级计算机越来越强大，数值模拟成为了探索科学问题不可或缺的一个工具。地震模拟工具就像一个“数字地动仪”，可以让科学家“模拟”或“预测”地震，从而提供对地震相关风险的定量评估，并提高我们对天然地震和底层结构和演变机制的理解。从工程的角度出发，这种工具还能够与其他机械模型耦合，并在不同的场景中进行相关测试，用以指导地震活跃地区抗震救灾的公用事业系统设计。

虽然数值地震模拟提供了一个具有宝贵研究功能的独特“实验平台”，但它也是超级计算社区传统的“巨大挑战”。

在地理空间上，地震模拟的范围需覆盖在水平面上数百公里，沿深度数十公里。 即使网格大小超过100米，这样的问题也会涉及数十亿到数万亿的未知数 \citep{cui2010scalable}。 在现在的地震工程计算需求中，要求支持10Hz以上的广泛的频率范围、网格大小在20米以内、时间步长在毫秒范围内  \citep{cui2013physics}。 尽管我们只关心地震几十秒的时间，但即使是使用最领先的超级计算机系统，这样的空间范围和时空分辨率也会给计算带来巨大的挑战。

此外，在每个网格中，计算和存储压力的也很高。 对于一个线性地震模拟，我们求解一个速度和应力张量方程的耦合系统：
\begin{equation}
\partial_{t}\bm{v}=\rho\nabla\cdot\bm{\sigma}
\end{equation}

\begin{equation}
  \partial_{t}  \bm{\sigma} = \lambda (\nabla    \cdot \bm{v}) \bm{I} + \mu \cdot (\nabla \bm{v}  + (\nabla \bm{v})^{T})
\end{equation}
其中$\bm{v}$=({$v_{x}$}, {$v_{y}$}, {$v_{z}$}) 是速度的三个空间分量，$\bm{\sigma}$=( $\sigma_{xx}$, $\sigma_{yy}$, $\sigma_{zz}$,$\sigma_{xy}$, $\sigma_{xz}$, $\sigma_{yz}$)是应力的六个分量。 对于频率较高的情况，岩石和土壤中的非线性的响应以及盆地内浅层沉积岩的非线性行为会成为重要考虑因素。 为了适应这些非线性效应，我们需要结合Drucker-Prager塑性 \citep {roten2016high}，得到如下产量应力方程：
\begin{equation}
Y(\sigma)={\rm max}(0, c \cos \varphi - ( \sigma_{m} + P_{f} ) \sin\varphi)
\end{equation}
其中$c$为凝聚量（cohesion）, $\varphi$为摩擦角度（friction angle）, $P_{f}$为流体压力（fluid
pressure），$\sigma_{m}$为平均压力（mean stress）。产量函数也用以判断是否更新应力：
\begin{equation}
\sigma_{ij} = \sigma_{m}^{\rm trial} \delta_{ij} + r s_{ij}^{\rm{trial}}
\end{equation}
其中，$r$ 是由产量应力$Y(\sigma)$计算所得。$s_{ij}$是应力偏导。因此，当从线性情况转变为非线性情况时，我们需要处理的3D矩阵则超过35个（在线性情况下，我们需要处理28个3D矩阵）\citep {roten2016high}，这几乎额外增加了内存容量和内存25％的带宽，这成为了目前在多核超级计算机获得高效率运算的最大挑战之一。

神威太湖之光在2016年的首次发布\citep{fu2016sunway}，标志着最领先的超级计算机的运算能力首次超过了100Pflops，运算核心首次超过1000万。而太湖之光的计算能力达到前所未有的水平，其性能相当于天河二号超级计算机的3倍，美国Titan超级计算机的5倍，然而其内存系统相对比较一般。 如图\ref{tb:supercomputer-comp}所示，神威太湖之光的总内存大小与其他系统相似（只比两个基于GPU的系统Piz Daint和Titan稍好），byte-to-flop的比例缺非常低，只有其他异构系统的五分之一，日本“京（K)"超级计算机的十分之一）。 这样的一个体系成为了将科学应用扩展到下一个层次的潜在而又严峻的挑战，特别是对于需要大内存空间和高内存带宽的地震模拟问题，打破内存的局限（内存墙）成为这个课题最大的挑战。

\begin{table}[!t]
\footnotesize
\caption{神威太湖之光超级计算机与其他领先超算系统的简要比较。$size_{MEM}$和$BW_{MEM}$分别代表内存的总量和带宽$\frac{BW_{MEM}}{PEAK}$表示计算内存带宽与系统峰值计算性能的比率。}
\label{tb:supercomputer-comp}
\center
\begin{tabular*}{0.8\columnwidth}{cccccc}
\hline\hline
   & PEAK & LINPACK & $size_{MEM}$  & $BW_{MEM}$ & $\frac{BW_{MEM}}{PEAK}$ \\
   & (Pflops) & (Pflops) & (TB) & (TB/s) & {BYTE per flop} \\
   \hline\hline
   TaihuLight & 125 & 93 & 1,310 & 4,473 & 0.038 \\\hline
   Tianhe-2 & 54.9 & 33.9 & 1,375 & 10,312 & 0.188 \\\hline
   Piz Daint & 25.3 & 19.6 & 425.6 & 4,256 & 0.168 \\\hline
   Titan & 27.1 & 17.6 & 710 & 5,475 & 0.202 \\\hline
   Sequoia & 20.1 & 17.2 & 1,572 & 4,188 & 0.208 \\\hline
   K & 11.28 & 10.51 & 1,410 & 5,640 & 0.5 \\\hline
\hline
\end{tabular*}
\end{table}

在AWP-ODC \citep{cui2010scalable}和CG-FDM\citep{zhang2014three}代码的基础上，我们提出了一个全面优化的设计和框架，细致地将算法和实现技术相结合，并设法把神威太湖之光超级计算机的各种性能指标——内存受限（memory bound）问题的内存相关功能，如DRAM空间，本地数据存储器（LDM）空间（SW26010系统用户控制器缓存），有效内存带宽——发挥到硬件上限的80％到90％。

我们还提出了一种实时压缩算法，能够在提供数据压缩的同时有效减少压缩引入的计算和内存开销。我们的方法可以使问题的最大规模增加一倍，性能进一步提高24％。在极端的仿真情况下，我们的系统能够为线性地震模拟提供14.2Pflops的持续性能，在非线性情况下提供了18.9Pflops的持续性能。虽然神威太湖之光的byte-to-float比例只有美国泰坦（Titan）超算的五分之一，但是我们所达到的计算效率高达15％，超过了以前在Titan \citep{roten2016high}上所获得的效率——11.8％。

通过充分发挥神威太湖之光的所有可能的性能，我们能够对唐山地震（M7.2，1976）进行一系列模拟，模拟的区域为水平面320km乘以320km，深度为40km，空间分辨率提高 从500米到8米，支持的最高频率高达18赫兹。就我们所知，这是第一次在这样的尺度上进行非线性塑性地震模拟，而频率和分辨率也是在同等模拟规模下达到最高。唐山地震的塑性地震动模拟首次使我们能够定量估计唐山地震灾区的危害，为华北地区建筑物的抗震工程的标准设计提供指导。

\section{当前研究现状与相关工作}

作为高性能计算的传统“巨大挑战”，与地震相关的最早的研究工作是在Cray T3D \citep {bao1996earthquake}的256个处理器上完成的。该工作使用非结构化网格模拟了区域大小为140公里乘以100公里乘以20公里，并且性能达到了8Tflops，这是最早的地震数值模拟工作之一。

从发生地震的概率出发，多数大规模的地震模拟工作都来自日本和美国湾区等地震活跃地区。例如，美国加州理工学院和日本JAMSTEC\citep {es-gb-2003}合作的地震模拟工作第一次获得了戈登贝尔奖（Gordon Bell Prize）。这个工作使用的数值方法是谱元法（一种高度有限元技术），最小计算单元是平均网格间距为2.9km的立方体，在地球模拟器上使用了1944个处理器对全球进行了地震波模拟，达到的性能为5TFlops。这项工作随后进行了扩展，开发成了名为SPECFEM3D\_GLOBE的软件包，它又随着新一代超级计算机的发展而不断演变。

2008年，SPECFEM3D\_GLOBE进一步将网格间距提高到800米左右，频率范围达到0.5赫兹左右，并在Ranger超级计算机上使用32000个计算核心达到了28.7Tflops的性能，以及在Jaguar超级计算所使用29000个计算核心达到35.7Tflops的性能。（Jaguar超算的性能之所以更好，主要是因为这是一个内存受限的问题，而Jaguar的内存带宽更高）。 到了2012年，SPECFEM3D\_GLOBE的代码进一步更新，能够支持GPU设备的高效利用，并扩展到896个CPU-GPU节点，性能达到35Tflops \citep {rietmann2012forward}。

另一种能够进行大规模地震模拟的数值方法是使用SeisSol软件中的任意高阶导数不连续Galerkin有限元方法（DG-FEM）。SeisSol在天河2号的8192个节点上实现了8.6Pflops的持续性能\citep{tianhe2-2014gb}，拥有1.91亿个四面体元素和960亿个自由度，实现了对1992年Landers M7.2地震的模拟。与其他方法相比，SeisSol中的DG-FEM将数值问题转化为计算受限的稠密矩阵运算，从而使Intel Xeon Phi加速器的使用效率更高。然而，高阶数值方法也增加了计算和内存的复杂性，这再次限制了可解问题的最大尺寸和计算时间。 SeisSol最近升级到了EDGE软件包 \citep {breuer2017edge}，包含了类似输入的融合地震模拟特性，这进一步将DG-FEM方法的性能在Cori-II超级计算机上提高到了10.4Pflops。

近期，在日本“京”超级计算机上也有相关的工作在探索隐式有限元求解器的潜力。 T. Ichimura和他的同事们提出了GAMERA \citep {ichimura2014physics}和GOJIRA \citep {ichimura2015implicit}，后者以29.7秒的时间步长解决了超过1万亿次的自由度。利用日本“京”超级计算机的全部（663,552）个CPU核心，该求解器的全部性能达到了1.97PFlops。然而，这两种实现更多地集中在城市中的建筑物（通常大小为几十公里）和地震波放大模拟的场景上，他们并不能在数百公里范围内进行大规模的地震模拟。

作为世界领先的地震研究联盟，也许也是跨学科、跨国家的世界上最大的地球科学合作，南加州地震中心（Southern California Earthquake Center，SCEC）主导了地震模拟领域的一些最重要的发展。 SCEC在2004年启动了了TeraShake项目，该项目从NSF资助的TeraGrid \citep{teragrid}的2,048个处理器开始，并升级为更大规模的4万个BlueGene处理器。 TeraShake项目发现了南圣安德烈亚斯断层的破裂指向性是一个震源效应，它可能与沉积盆地的激发（场地效应）耦合，从而大大增加了洛杉矶的地震危险。他们清楚地展示了大规模地震模拟的科学效益。该项目的另一个重要成果是开源软件AWP-ODC（Anelastic Wave Propagation by Olsen, Day and Cui），后来支持众多的地震研究项目。 2010年左右，AWP-ODC的仿真能力进一步提升到了能够在千万亿次超级计算机上进行模拟\citep{cui2010scalable}。通过使用Jaguar超级计算机的223,074个计算核心，在南加州的800公里乘以400公里区域实现了一个地震模拟，模拟空间分辨率为40米，最大频率为2赫兹，持续性能为220Tflops。 2013年，AWP-ODC扩展到支持GPU加速器。对于20,480×20,480×2,048网点的问题，在Titan \citep {cui2013physics}超级计算机上通过使用16,384个GPU可以实现2.33 Pflops的持续性能。 2016年SCEC进一步完善了支持非线性效应的模拟，这是高频地震模拟中要考虑的关键因素，使用Titan \citep {roten2016high}超级计算机的一半资源，性能达到了1.6 Pflops。

表 \ref{tb:rw-comp}总结了大规模地震模拟的现有工作。在大约二十年的时间里，随着机器从256个处理器发展到1000万个计算核心，问题从数百万个元素扩展到数十亿个元素（占用内存空间从GB量级扩展到了PB量级），地震模拟性能也从Gflops提高到15 Pflops。不同的软件包（SPECFEM3D的SEM，SeisSol和EDGE的DG-FEM，GAMERA和GOJIRA的隐式FEM，以及AWP-ODC的FD）采用不同的数值方法，因此我们也不可能用同一个衡量标准对他们进行统一比较。一般而言，基于有限元的方法具有更好的地形处理能力，可以用少量的网格模拟复杂的场景，但是对于非线性问题的收敛性可能会面临严重的效率问题。相比之下，有限差分（FD）方法由于其对计算和存储容量的高度需求而被认为是不切实际的解决方案，现在因为其计算规则，存储器访问和节点间通信的友好性被认为更适合于大规模并行计算环境。

综合考虑所有主流的软件代码，AWP-ODC在过去的几十年里累积了SCEC的在地球物理和计算机软件的工作，提供了最先进的功能，具有良好的塑性和非弹性衰减物理特性，能够在半天内完成大规模的地震模拟。因此，在我们的工作中，我们重新设计了基于神威太湖之光硬件架构的AWP-ODC，这是和过去完全不同的硬件架构。在几乎所有方面（计算、内存容量、内存带宽、IO带宽、通信带宽）将机器的性能压缩到极限之后，我们将地震模拟的性能从Titan超级计算机的2.3Pflops提高到太湖光的15.2Pflops，并支持模拟比原来更大4到5倍的问题。结合我们的新提出的压缩方案，神威太湖之光超算的地震模拟的性能将进一步提高到18.9 Pflops，并且可以支持18赫兹频率和8米分辨率的场景。总而言之，与AWP的Titan相比，我们的工作将科学家对地震的模拟能力提高了8倍，仿真性能提高了9倍，最大的问题尺寸提高了9倍到10倍。值得注意的是，尽管神威太湖之光超级计算机的byte-to-float只有其他超算的五分之一或者十分之一，但是经过我们的创新和努力，使得AWP-ODC这种内存限制的应用程序也能在太湖之光上达到上述的性能。

\begin{landscape}
\begin{table*}[p]
\caption{超级计算机大规模地震模拟现有工作总结。这些数字是从已发表的论文中获得的。未报告的值被标记为“-”。对于数值方法，FD指的是有限差分法，SEM指的是谱元法，DG-FEM指不连续的Galerkin有限元法。}
\label{tb:rw-comp}
\centering
\resizebox{1.55\textwidth}{!}{%
\begin{tabular}{ccccccccccc}
\hline\hline
  \multicolumn{2}{c}{Work} & Year & Machine & Arch & Scale & \# grid points & \# DOFs & Flops & Mem & Method\\\hline\hline
  \multicolumn{2}{c}{\citep{bao1996earthquake}} & 1996 &   Cray T3D & Alpha CPU  & 256 & 13.4 million & 40.2 million & 8 Gflops & 16 GB & FD \\
  &&&&& processors &  &&&& \\\hline\hline

  SPECFEM3D & \citep{es-gb-2003} & 2003 & Earth  & NEC SX-6 & 1,944 & 5.5 billion & 14.6 billion & 5 Tflops & 2.5 TB & SEM  \\
  &&&Simulator&& processors &&&& \\\cline{2-11}
   & \citep{carrington2008high} & 2008 & Ranger & 4-core Opteron & 32,000 cores & -- & -- & 28.7 Tflops & -- & SEM \\
  & & & Jaguar &  & 29,000 cores & & & 35.7 Tflops & & \\\cline{2-11}
  & \citep{rietmann2012forward} & 2012 & Cray XK6 & 16-core Opteron and & 896 GPUs & 8 billion & 22 billion & 135 Tflops & 3.5 T & SEM \\
  &&&& Fermi M2090 GPU &&&&&& \\\hline\hline
  SeiSol & \citep{tianhe2-2014gb} & 2014 & Tianhe-2 & 12-core Xeon and & 196,608 cores & 191 million & 96 billion & 8.6 Pflops & -- & DG-FEM \\
  &&&& 59-core Xeon Phi & 1,400,832  cores &  tetrahedrons &&&& \\\cline{2-11}
  EDGE & \citep{breuer2017edge} & 2017 & Cori-II & 68-core Xeon Phi & 612,000 cores & 341 million & -- & 10.4 Pflops & 32 TB & DG-FEM \\
  &&&&&  &  tetrahedrons &&&& \\\hline\hline
  GAMERA & \citep{ichimura2014physics} & 2014 & K Computer & 8-core SPARC64 & 663,552 cores & -- & 27 billion  & 0.804 Pflops & -- & implicit \\
  &&&&&  &  &&&& FEM \\\cline{2-11}
  GOJIRA & \citep{ichimura2015implicit} & 2015 & K Computer & 8-core SPARC64 & 663,552 cores & 270 billion & 1.08 trillion  & 1.97 Pflops & -- & implicit \\
  &&&&& &  &&&& FEM \\\hline\hline
  AWP-ODC & \citep{cui2010scalable} & 2010 & Jaguar & 6-core Istanbul & 223,074 cores & 436 billion & 1.31 trillion & 220 Tflops & 127 TB & FD \\
  &&&&&  &  &&&& linear\\\cline{2-11}
  & \citep{cui2013physics} & 2013 & Titan & 16-core Opteron and & 229,376 SMXs & 859 billion & 2.58 trillion & 2.33 Pflops & 250TB & FD  \\
  &&&&k20x GPU & 16,384 GPUs &  &&&& linear\\\cline{2-11}
  & \citep{roten2016high} & 2016 & Titan & same as above & 114,688 SMXs & 329 billion & 987 billion & 1.6 Pflops & 129TB & FD \\
 &&&&&  8,192 GPUs &  &&&& non-linear \\\hline\hline
 our work & without & 2017 & Sunway & 260-core SW26010 & 1,014,000 cores & 3.99 trillion & 11.98 trillion & 15.2 Pflops & 892 TB & FD \\
 &compression&&TaihuLight&& &  &&&& non-linear \\
 & with compression &  &  &  & & 7.8 trillion & 23.4 trillion &  18.9Pflops & 724TB & \\\hline\hline
\end{tabular}
}
\end{table*}
\end{landscape}

\section{神威太湖之光及其在大规模科学应用的挑战}
\label{sec:sunway}

\subsection{体系结构}
神威太湖之光的计算能力是由我国自主研发的多核申威26010（SW26010）CPU提供，其中包括4个核组（Core Group，CG）\citep {fu2016sunway}，每个核组包括一个主核（Management Processing Element，MPE），64个从核（Computing Processing Element，CPE）和一个内存控制器（Memory Controller, MC）。 一个申威26010处理器拥有260个处理单元，并且提供超过3 TFlops的峰值性能。 神威太湖之光系统共有40960个SW26010 CPU，提供125 Pflops的峰值性能，10644900个计算核心。 虽然太湖之光在计算部分具备优于其他系统的显着优势，但内存容量相对较小，每个节点只有32 GB的内存和136 GB/s的内存带宽。

Table \ref{tb:system-comp}显示了神威太湖之光超算系统与世界上其他顶级超级计算机的比较。 虽然每个神威节点只包含一个处理器，但处理器本身采用了独特的片上异构设计。它包含了与传统CPU核心类似的MPE，用以处理计算、管理、通信和IO等任务。除此以外，他还包括与GPU类似的CPE计算核心。CPE只负责处理高密度的并行计算任务，不负责管理和通信等工作。在计算方面，神威太湖之光超算系统表现出了优于其他系统的显着优势（比如用于Top500排名的Linpack应用，神威超算系统排名第一），但是在内存容量和内存带宽方面却比较普通（byte-to-float比例只有美国泰坦超算的1/5，且在HPCG标准测试中仅排名第三）。

\begin{table*}[h]
\footnotesize
\caption{神威太湖之光超算系统与其他超算系统的比较。}
\label{tb:system-comp}
\begin{tabular*}{\textwidth}{ccccccccccc}
\hline\hline
  system & $R_{peak}$ & $R_{max}$ & \# nodes & \# cores & processor & memory & power\\
  & (Pflops) & (Pflops) &&& architecture & size (TB) & (kW)\\\hline
  TaihuLight & 125  & 93 & 40,960 & 10,649,600 & 4 MPEs & 1,310 & 15,371 \\
  & & & & & and 256 CPEs \\\hline
  Tianhe-2 & 55 & 34 & 16,000 & 3,120,000 & two 12-core CPUs & 1,375 & 17,808\\
  & & & & &and three 57-core Intel MICs \\\hline
  Titan & 27 & 18 & 18,688 & 560,640 & one 16-core CPU & 710 & 8,209 \\
  & & & & &and one K20x GPU \\\hline
  Sequoia & 20 & 17 & 98,304 & 1,572,864 & 16-core CPU & 1,572 & 7,890 \\\hline
  Cori & 28 & 14 & 9,152 & 622,336 & 68-core CPU & 878 & 3,939\\\hline
\hline
\end{tabular*}
\end{table*}

\subsection{主要设计挑战}
\label{sec:sunway-challenge}

作为全球第一个拥有超过1000万个内核的超算系统，神威太湖之光上设计应用的第一个挑战就是推导出正确的并行化方案，将我们的目标应用程序映射到系统中的进程和线程中。 与其他异构多核加速器的超级计算机类似，我们也采用了两级的“MPI + X”方法。其中每个核组通常对应一个MPI进程。 在每个核组中，我们有两个不同的选择：一个是神威 OpenACC，这是一个专用的并行编译工具，支持OpenACC 2.0语法，能够将特点的代码块以从核集群为目标进行并行; 另一个是名为$ Athread$的高性能轻量级线程库，它提供与$ Pthread $类似的接口来完成细粒度的并行。 在我们的工作中，我们采用$ Athread $的方法，这意味着需要更多更复杂的编程，但同时也有更大的空间来调整计算和内存访问的方案。

第二个挑战是要打破这个超算系统的内存墙，特别是对于这种基于有限差分运算的地震模拟的内存限制问题，正如表 \ref {tb:supercomputer-comp}中所强调的那样，神威超算系统的byte-to-float比例比其他领先超算系统低5-10倍，在这种情况下，我们需要非同寻常的与内存相关的创新来充分发挥神威超算峰值性能为125 Pflops的仿真计算能力。





申威处理器独特的内存层次结构以及每个层次中不同的特性也给程序设计带来非常严峻的挑战。图~\ref {fig:sunway_mem}描述了CPE的内存层次结构。虽然每个CG中的MPE采用与L1指令和数据高速缓存相似的存储器层次结构设计，但64个从核中的每一个从核都使用64 KB本地数据存储器（LDM）作为scratch-pad缓存而不是硬件上的高速数据缓存。


如图~\ref {fig:sunway_mem}所示，内存层次结构的顶层是每个CPE的32个浮点寄存器。其中本地的寄存器只需要一个周期来访问。申威处理器中的64个从核按照8乘8的网格进行排列。其中每一行、每一列都有通信总线。总线为神威处理器的寄存器提供了寄存器通信的功能，寄存器通信的延迟为11个周期。后续的章节将进一步讨论关于如何利用此功能来改进数据重用，减少Halo读写以及进一步提高性能。

第二个级别是每个CPE的64 KB LDM，需要由程序员以细粒度的方式手动管理。LDM内存的大小约束和读写调度都给类stencil操作带来了严峻的挑战。

第三层则是每个核组只有8GB的主存，每个处理器有32 GB内存空间。 在目前这一代申威处理器中，每个核组连接到一个带宽为34 GB/s的DDR3接口。 因此，每个申威处理器在提供3 Tflops峰值性能的同时，同时提供了仅为136 GB/s的内存带宽。这大约是Intel最新多核芯片内存带宽的1/3或者是NVIDIA GPU内存带宽的1/5（如表\ref {tb:proc-comp}所示）。因此，我们的许多努力都集中在了如何通过精心设计算法来充分利用神威超算的的内存系统，该方案利用了DDR3，LDM和寄存器的所有可用功能和特性。

\begin{table}[ht]
%\footnotesize
\caption{申威26010芯片与其他多核芯片的简要对比。}
\label{tb:proc-comp}
\centering
\begin{tabular*}{0.8\columnwidth}{cccc}
\hline\hline
  Chip & SW26010 & Intel KNL 7250 & NVIDIA P100 \\\hline
    $R_{peak} (Tflops)$ & 3.06 & 3 & 5.3 \\\hline
    memory   & DDR3 & DDR4 + MCDRAM & HBM2 \\
    technology \\\hline
    $size_{mem}$ (GB) & 32 & 96 + 16 & 16 \\\hline
    $BW_{mem}$ (GB/s)  & 136 & 102 + 460 & 732 \\\hline
\hline
\end{tabular*}
\end{table}

由于与Sunway处理器的3 Tflops计算性能相比，第三级的内存大小和带宽都相对较小，因此我们在这方面的很多努力都集中在最大程度地利用64 KB LDM、寄存器、以及第二级和顶层的寄存器通讯功能。即使在我们设计并行化方案时，我们也仔细地将计算部分与我们在不同级别上可以承受的内存占用情况进行映射。更多细节在Section \ref{sec:contribution}中给出。

\begin{figure}[ht]
\centering
\includegraphics[width=.9\columnwidth]{memory_hierarchy.pdf}
\caption{CPE的内存层次结构。顶层是高效的寄存器，访问或通讯需要花费1或11个周期；中间是大小为64KB的LDM高速缓存，访问LDM需要4个周期；底层是所有从核共享的8GB主存，访问需要120+个周期，带宽为136GB/s。}
\label{fig:sunway_mem}
\end{figure}

\section{主要技术创新点}
\label{sec:contribution}

\subsection{贡献小结}


我们的主要贡献是提出并开发了基于神威太湖之光的一个软件框架，它可以同时支持动态破裂的产生（用以反演地震的震源）和地震波传播的模拟（大规模地震模拟的完整周期）。

该软件框架设法突破了神威太湖之光的存储器系统所带来的约束，并实现了高效的仿真能力。即便在byte-to-float比例只有其他领先超级计算机系统的1/5至1/10的情况下，该软件框架仍能够充分利用神威太湖之光前所未有的125Pflops计算能力以及其他各项资源。

我们的主要创新点是：

\begin{itemize}
\item 一个完整统一的软件框架，包括地震震源动态破裂发生器，地震波传播部分和其他辅助功能，例如地震划分功能、3D模型生成功能、计算重启控制功能以及并行I/O功能。

\item 针对地震波模拟定制的并行化方案，该方案可在进程级别和线程级别高效使用神威太湖之光所有的1000万个计算核心，并获得几乎线性的加速效果。

\item 精心设计的多层级内存利用方案，包括通过寄存器通信来实现片上stencil halo交换、通过建模分析，推导出最优的blocking划分配置，以及对齐的数组融合DMA（直接内存访问）访问。

\item 一套实时压缩/解压方案，将应用程序可用内存大小和带宽提升到一个全新的水平，该方案同时能够扩大神威太湖之光的最高性能以及能够处理的最大问题的大小。
\end{itemize}

\subsection{完整统一地震软件框架}


大规模地震模拟的整个工作流程是一个极其复杂的过程，由不同的组成部分组成，包括从动态破裂源生成、数值计算网格的生成到最耗时的地震波传播部分。这些不同的组件给超级计算机的各个方面（计算、内存、通信、存储和I/O）都带来了巨大的挑战。为了解决这些挑战，我们构建了一个集成了不同功能的统一软件框架，如图\ref{fig:framework}所示。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{architecture.pdf}
\caption{基于神威太湖之光的统一地震模拟软件框架。该框架由动态破裂震源生成器、数值计算网格的生成器、地震波传播模块以及其他辅助模块组成。}
\label{fig:framework}
\end{figure}

我们软件框架中的动态破裂发生器是基于CG-FDM代码\citep{zhang2014three}，该模块具有初始化断层应力、执行摩擦定律控制以及通过波传播生成震源的功能。

为了支持震源和地震波传播的大规模模拟，我们开发了一个震源划分模块，它能够将一个单一的大型源输入映射到不同的源文件中，以用于不同的相应MPI进程。我们还提供了3D模型插值生成器，可将速度和密度模型重新映射到目标网格。

地震波传播部分源于AWP-ODC \citep {cui2010scalable}，但我们完全针对神威太湖之光的特殊体系结构进行了全新的设计，这个模块消耗了大部分计算周期并集成了大部分创新和优化。地震波传播主要功能包括：速度更新、应力更新、震源注入和塑性应力调整等。

I/O部分是大规模地震模拟的另一个重要因素。最棘手的挑战来自重启的检查点（checkpoint）。在我们的研究中发现，在16米分辨率的情况下，检查点所需的所有波场总计为108 TB，这显然超出了该系统所能提供的I/O带宽和容量。因此，我们整合了LZ4压缩以减小存储开销，以实现更平滑的运行，并采用诸如“组I/O”和“平衡I/O转发”等技术，实现了120GB/s的峰值I/O带宽，达到了该文件系统峰值带宽的92.3％。

\subsection{地震模拟的核心计算流程}

地震波传播中的速度、应力张量的求解是一个复杂的过程。它涉及三个速度分量和六个应力分量的标量值方程。 为了实现非线性的模拟引入了塑性特征，这使得整个系统变得更加复杂。

图~\ref{fig:awp-workflow}描述了地震模拟的核心计算流程。每一个计算核心（kernels）都几乎涵盖了20-40个三维网格，其中大部分网格的大小相同。为了支持这样复杂的工作流程，特别是为了促进未来可能的算法调整，在我们的重新设计和移植过程中，我们将每个内核封装为一个标准模块，可以轻松地与其他模块通过接口进行衔接。

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth]{awp_chart.jpg}
\caption{地震模拟的核心计算流程，其中变量以数组表示，计算以kernel表示。}
\label{fig:awp-workflow}
\end{figure}

在一次典型的地震模拟中，每一个迭代步的工作流程由以下五个主要步骤组成：

\begin{itemize}
\item {\bf velocity update}：基于上一个时间步的速度和应力数组，完成速度的更新，包括Halo区域 （由kernel $dvelcy$完成）和中心区域（由kernel $dvelcx$ 完成）。

\item {\bf stress update}：使用速度，应力阵列，Lam系数和频率相关的地震波震级衰减参数Q等等数组来计算Halo区域和中心的应力更新（kernel $dstrqc$）;

\item {\bf stress adjustment for the fault}：检查屈服应力是否越界（kernel $drprecprc\_calc$），并对断层区域进行相应的调整（kernel $drprecprc\_app$）;

\item {\bf injection of source}：注入震源（kernel $addsrc$）来更新当前时间步的波场。

\item {\bf stress adjustment for the free surface}：对自由边界进行应力调整（kernel $fstr$）。
\end{itemize}

以上描述的每一个步骤都涉及到了大量的大型三维数组读写。尽管每个内核都涉及高密度的算术运算，在神威太湖之光超算上性能的主要限制条件却来自于内存带宽的瓶颈。

\subsection{定制的并行化设计}
\label{sec:parallel}

在耗时最多的波传播部分，我们需要处理的计算内核（kernels）中包括了涉及读取和写入超过20个覆盖整个网格的变量数组。在这种情况下，许多先前的优化技术，如3.5D blocking等方案\citep {nguyen20103}，都由于极高的内存容量要求而变得不切实际，无法在神威超算上使用。因此，我们为这些不同的内核应用定制了区域分解方案，这为1000万个计算核心提供了足够的并行性，同时最大限度地降低了相关内存成本。

图~\ref{fig:dd}显示了我们的多层级并行划分方法，它首先将整体计算区域分解为每个MPI进程的不同进程，并进一步分解为每个CPE线程的不同区域，具体如下：

（1）MPI进程的二维分解：对于所有三维数组的存储，我们将$ z $轴（垂直方向）作为最快轴，将$ y $轴作为第二快轴，并将$ x $轴作为最慢的轴。在典型的地震模拟情景中，$ x $和$ y $（一般是几百公里）的尺寸通常比$ z $（一般是几十公里）大得多。因此，为了尽量减少不同进程之间的通信，在第一层级的划分中，我们并不是采用三维划分，而是将水平面分解为$ M_x $ $ M_y $个不同的分区，每个分区对应一个特定的MPI进程。在精心设计的MPI方案中，通过从AWP-ODC \citep{cui2010scalable}继承而来的计算与通信重叠功能，在极端情况下，我们可以将多达160,000（400×400）个MPI进程扩展到整个机器。

（2）每个核组内分解：在第二层级中，不是将所有网格点分配给CG内的不同核心，而是沿$ y $和$ z $轴添加一个blocking机制，以将合适大小的块分配给每一个核组，以便更有效地利用每个CPE的64 KB LDM，详见Section \ref{sec:mem-redesign}。核组内分家之后，每个核组将遍历这些不同的块来完成计算。

（3）Athreads的二维分解：我们进一步将第二层级所得到的计算单元划分为每个CPE，划分的方式是沿着$ y $和$ z $维度（每个线程则沿着$ x $的方向迭代），以便实现不同线程高效访问高速缓存。

（4）LDM缓存方案：对于每个CPE，我们使用DMA操作将合适大小的计算区域，包括中心部分和晕圈（halo）部分加载到LDM中，以便随后执行计算。 DMA操作被设计为异步的，以便与计算部分重叠。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{blocking.pdf}
\caption{多层级区域分解方案：（1）MPI进程的二维分解； （2）每个核组内分解；（3）Athreads的二维分解；（4）LDM缓存方案。}
\label{fig:dd}
\end{figure}

\subsection{平衡与优化的内存设计方案}
\label{sec:mem-redesign}

申威26010的每个CG连接到带宽为34 GB/s的DDR3接口。 但是，我们使用DMA操作读取或写入的数据块的块大小（block size）很大程度上决定了可以有效利用的带宽部分。

如表 \ref{tb:sw-bw}显示了我们根据不同连续读写的字节数测量出来的DMA内存带宽。对于128字节以下的块大小，神威26010的存储器访问的带宽不足理论带宽的一半。只有当每次读写的连续字节数超过512时，我们开始看到合理的内存带宽利用率。

\begin{table}[!t]
\small
\caption{根据不同连续读写的字节数测量出来的DMA内存带宽。}
\label{tb:sw-bw}
\centering
\begin{tabular}{ccccc}
\hline\hline
  block & \multicolumn{2}{c}{DMA Get (DDR3 to LDM)} & \multicolumn{2}{c}{DMA Put (LDM to DDR3)} \\
  size (BYTEs) & 1 CG & 4 CGs & 1 CG & 4 CGs \\\hline
  32 & 3.28 & 13.21 & 2.58 & 8.07 \\
  64 & 8.71 & 34.13 & 9.42 & 39.01 \\
  128 & 17.81 & 72.02 & 19.05 & 77.10 \\
  256 & 21.42 & 89.16 & 25.80 & 104.86 \\
  512 & 27.8 & 104.86 & 30.48 & 107.88 \\
  1024 & 29.8 & 115 & 33.4 & 126.6 \\
  2048 & 31.3 & 119.2 & 34.2 & 133 \\
  4096 & 32.1 & 118 & 34.01 & 134 \\
  \hline
\end{tabular}
\end{table}

因此，在我们开始采用各种与内存相关的优化策略来提高数据重用率之前，面对memory-bound的问题，我们的首要优化任务是保证DMA访问时能够进行大块的内存访问，以便高效利用申威26010的内存带宽。

为了突破内存限制，解决方案的关键则是高效利用SW26010处理器的内存层次结构。

SW26010的一个独特功能是在每个CG中的64个CPE之间能够进行寄存器通信，这为stencil-like计算中的数据重用提供了完美的解决方案。使用基于寄存器通信的Halo交换，在每个CG内部，CPE线程只需要加载其相应的中心计算区域，并且可以通过寄存器通信操作从相邻线程获取所需的Halo区域。但同一核组内的不同线程间的寄存器通信并不能完成所有Halo的交换，跨不同核组的边界通信仍然需要所对应的线程通过共享内存的方式从主存中通过DMA方式进行加载。不过，通过DMA方式进行加载的边界所占的比例很低（具体的推导请见下文），大量的边界交换通过寄存器完成了，因此显著提升了性能。

结合第\ref {sec:parallel}节中详述的并行化方案和寄存器通信方案，我们可以推导出一个分析模型来确定用于CPE分解和LDM缓冲的优化配置。

我们的第一组设计参数是$ C_z $和$ C_y $，如图~\ref{fig:dd}所示，它决定了每个CG中并行CPE线程的布局。第二组是$ W_z $，$ W_y $，$ W_x $，如图 \ref {fig:dd}所示，它决定了加载到每个CPE线程的LDM中的区域的大小。

$C_z$、$C_y$、$W_z$和$W_y$需要满足以下条件：

\begin{equation}
C_z \cdot C_y = 64
\label{eq:czy}
\end{equation}
\begin{equation}
W_z \cdot W_y \cdot W_x \cdot N_{array} \cdot N_{bytes\_per\_variable} < 64 \times 1024\\
\label{eq:wzy}
\end{equation}
其中$ N_{array} $表示当前计算核心（kernel）所需的3D数组的数量，$N_{bytes\_per\_variable}$表示我们用于每个变量的字节数（4表示单精度浮点数）。

我们设计目标是：（1）尽量减少冗余Halo区域读取所需的DMA负载数量; （2）通过使用最大的连续读取大小来最大化有效内存带宽。

为了实现第一个目标——最小化冗余DMA负载数量，我们进一步推导出额外的DMA负载数量（CG内的Halo是通过寄存器通信执行的，在此不予考虑）：

\begin{eqnarray}
N_{redundant\_DMA\_load} = 2 \times H \cdot N_y \cdot (\frac{N_z}{C_z\cdot W_z}-1) \notag \\
+ 2 \times H \cdot N_z \cdot (\frac{N_y}{C_y\cdot W_y}-1).
\label{eq:load}
\end{eqnarray}
其中$ H $是stencil Halo所需的点数，$ N_y $和$ N_z $是由CG处理的block的尺寸。

结合等式\ref{eq:czy}、\ref{eq:wzy}和\ref{eq:load}，我们可以很容易推导出，为了实现光晕的最小冗余DMA负载，我们需要确保$C_y\cdot W_y = C_z\cdot W_z $。



在我们的模型中，连续DMA传输的块大小由$ W_z $确定，它表示沿着最快轴的尺寸大小。因此，我们需要保持$ W_z $轴尽可能大。结合我们模型的第一个和第二个目标，我们可以推导出一个较小的值$ C_z $是相对较优的，可以实现有效的内存访问行为。

因此，在大多数情况下，我们推导出$ C_z = 1 $和$ C_y = 64 $是CG中最合适的配置。 64个CPE线程中的每一个线程将初始化DMA加载操作，以便于之后获取对应的立方区域（$W_z \cdot W_y \cdot W_x$)）和用于stencil计算的$ yz $平面。

\subsection{共位数组融合}
即使在采用上述的最佳并行化方案之后，在大多数情况下，由于我们需要访问大量的数组，在只有64KB的LDM中，能够分配给每个数组的缓存是极其有限的。对于每一个三维数组，只有做一个维度（快轴）的数据在内存中是连续的，按照第\ref{sec:parallel}节描述的并行划分方案，我们能够连续读取的块很小，使得DMA读取的效率很低。

例如，对于$ dvelcx $（速度更新）的内核，我们需要读取10个不同的数组（$ u $，$ v $，$ w $，$ xx $，$ yy $，$ zz $，$ xy $，$ xz $，$ yz $，$ d $，它们是不同方向上的速度和应力变量）。根据等式\ref {eq:wzy}，为了计算这个内核中空间四阶stencil，我们需要在LDM中加载至少5个slice，所以$ W_x $的最小值是5。对于参数$ W_y $，因为$（W_y-2H）$是加载到LDM中的有效区域，所以我们需要将$ W_y $设置为至少9（对于$ H = 2 $），以保持halo成本处于合理的水平。

因此，在10个输入数组的情况下，我们可以从公式\ref{eq:wzy}推导出：
\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 10 \cdot 4 < 64 \times 1024\\
\label{eq:wzy1}
\end{equation}
其中我们得到$ W_z $的最大值为32。在这种情况下，DMA块的大小为128字节，导致内存带宽利用率低，仅为峰值带宽的50％。
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{awp_before.png}
\caption{共位数组融合之前，函数$dvelcx$和$dstrqc$的内存访问模式。}
\label{fig:array-fusion-before}
\end{figure}

为了解决上述问题，一个直观的想法则是减少数组的数量。数组数量的减少能够使得每个数组可使用的LDM空间增大。在我们分析了所有不同的相关内核（kernel），并确定了一组展示大多数内核常见行为的共位数组（co-located array）。 在$ dvelcx $和其他类似内核的例子中，我们确定了数组（$ u $，$ v $，$ w $，$ xx $，$ yy $，$ zz $，$ xy $，$ xz $和 $ yz $）是在不同内核之间展示相同内存访问模式的共位数组。 因此，我们制定策略将$ u $，$ v $和$ w $融合成一个包含三个元素的向量数组，并将$ xx $，$ yy $，$ zz $，$ xy $，$ xz $和$ yz $转换为一个包含六个元素的向量数组。共位数组融合的过程如图\ref{fig:array-fusion-before}和图\ref{fig:array-fusion-after}所示。

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{awp_after.png}
\caption{共位数组融合之后，函数$dvelcx$和$dstrqc$的内存访问模式。}
\label{fig:array-fusion-after}
\end{figure}

在完成了数组融合之后，从等式\ref{eq:wzy}中可以看到，只有3个单独的数组可以读取，我们得到：

\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 3 \cdot 4 < 64 \times 1024\\
\label{eq:wzy2}
\end{equation}
它给出的最大$ W_z $值约为108。在这种情况下，DMA块的大小为432字节，将内存带宽利用率提高到80％左右。 在$ dstrqc $内核的极端情况下，数组融合技术可以将DMA块的大小从84字节增加到512字节，从而将有效内存带宽从50.47 GB/s提高到104.82 GB/s。


在将上述所有技术相结合之后，我们推出了一种均衡的多层级内存使用方案，可以有效利用SW26010的存储器层次结构。

\subsection{实时压缩解压缩}

在充分发挥了神威太湖之光硬件的所有性能之后，我们的下一个创新就是即时压缩方案，它不仅使可用内存空间增加了一倍，而且在相同的物理内存带宽下也释放出更多的计算能力。

\begin{figure*}[ht]
\centering

\includegraphics[width=1.0\textwidth]{compression.pdf}
\caption{实时压缩方案。}
\label{fig:compression}
\end{figure*}

如图\ref{fig:compression}所示，我们将我们的压缩方案阐述为四个主要部分。 （a）部分是一个预处理步骤，它使用较粗糙的分辨率执行完整模拟，以便生成统计数据（例如变量的最大值和最小值），以便随后在其压缩过程中使用高分辨率模拟。

（b）部分解释了我们的压缩方案的工作流程。 CPE按照以下顺序执行任务：使用DMA指令将压缩的变量加载到LDM中、解压缩、计算并将结果压缩为16位结果、并使用DMA指令将压缩结果存储回主存储器。

（c）部分描述了解压缩、计算和压缩工作流的缓存方案。类似于在图\ref{fig:dd}中所描述的方案，我们执行 $ yz $ 平面中的点的压缩。每次，我们将一个平面加载到LDM中，并在向量化（vectorization）启动的情况下执行解压缩。

考虑到地震波传播的特点，我们在工作中采用了三种不同的压缩方法，如（d）部分所示，所有这些方法都将32位浮点压缩为16位。为了通过压缩来提高计算性能，我们选择应用有损压缩而不是无损压缩方案，以便将我们引入的额外计算最小化。

方法（1）直接使用由IEEE 754标准定义的16位半精度浮点数，其中5位用于指数，10位用于尾数。单精度到半精度转换的固有支持使得压缩部分高效。但是，对于动态范围大于5位指数的变量，压缩方案会产生数值问题。相反，对于动态范围较小的变量，5位指数则变成浪费。

为了解决方法（1）中的描述的问题，我们的方法（2）根据第一部分中记录的最大动态范围确定所需的指数位宽，并使用余数位作为尾数。方法（2）确保全动态范围的覆盖，并且可以为具有小动态范围的尾数部分的变量保留更多位。唯一的缺点是计算成本相对较高。

方法（3）在精度和效率之间是最平衡的。根据第一部分的统计数据，我们将同一数组的所有值归一化到1到2之间的范围，这对应于指数值为零。因此，在归一化之后，我们可以移位这些比特以直接得到尾数部分作为压缩值，这显着地简化了压缩过程。

压缩方案本身会引入额外的计算，主要是整数运算，以及密集的内存和高速缓存访问。即使访问全都发生在LDM中，频繁的加载和存储可能会大大降低CPE的计算效率。因此，我们的第一个压缩版本只能达到不压缩的1/3性能。为了降低压缩方案的性能损失，我们进行了一系列优化：

\begin{itemize}
  \item 进一步增加DMA块的大小（由$ W_z $决定），以减少DMA 70％的负载量;
  \item 针对仍能满足精度要求的情况，采用较为不复杂的算法；
  \item 将解压缩和计算代码紧密地（在汇编代码级别）耦合以最大化寄存器中的变量的寿命，从而减少对LDM的加载和存储；
\end{itemize}

在我们的最终设计中，我们对大多数速度和应力变量采用方法（3）。将压缩方案与我们的波传播计算内核相结合，我们可以获得超过24％的计算性能提升，这是因为我们的压缩方案可以使得在相同的物理带宽情况下处理更多的数据，换而言之，对于地震波传播这种memory bound的问题，我们的压缩方案使得需要处理的数据的总量降为原来的1/2，极大的缩短了所需的DMA访问时间。而且，在压缩比为2的情况下，我们可以将这台超级计算机上能够解决的最大问题扩大一倍。这是一种通过软件的形式突破硬件的限制的方法。


为了验证我们的压缩方案，我们在不同分辨率下对使用和不使用压缩方案产生的仿真结果进行了广泛比较。图\ref {fig:compress_valid}显示了宁河和沧州两个台站的压缩前后的地震图的对比。位于唐山地震断层附近的宁河县在地震期间承受了巨大的破坏，图中的蓝色实线是未压缩的结果，红色的虚线是压缩的结果。压缩结果和未压缩结果两者的尖锐起点彼此相似，而压缩结果的尾波由于压缩的准确性损失而不能完全匹配参考波形。尽管如此，压缩结果的大部分细节仍然​​保留得很好，这对于强大的地面运动模拟来说足够精确。此外，我们还比较了远离震中的沧州站。由于较长的时间传播和误差积累，压缩后的结果可以承受稍微更高的精度损失，但直到120秒仿真结束时，这些线路仍然可以很好地匹配。这些比较结果表明，虽然压缩在一定程度上引入了错误，但我们仍然可以使用这套实时压缩方案，不仅能够获得更好的性能，还能够模拟更大规模的地震波传播问题。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{CompareCompress.eps}
\caption{分辨率为500米的唐山地震模拟压缩方案验证。}
\label{fig:compress_valid}
\end{figure}

\section{性能与扩展性}

\subsection{如何测量性能}

我们将程序运行100个时间步之后，记录运行100个时间步所花费的时间来计算每一个时间步所花费的平均时间。对于浮点数运算次数，我们使用两种不同的方法来测量，分别是数计算汇编代码中的所有浮点运算指令以及使用伴随神威太湖之光编译器一起提供的硬件性能监视器PERF。这两种方法都产生了类似的浮点数操作次数。本研究我们使用PERF工具来测量本程序所执行的平均浮点运算。请注意，为优化目的而添加的所有操作（例如压缩相关操作）都不计入FLOP的数量中，这能够更加公平地与其他类似工作做对比。

\subsection{计算核心优化结果}

对于速度物理量的更新，计算中心区域和Halo区域的内核$ dvelcx，dvelcy $是计算量最大的两个计算核心。对于应力物理量的更新，$ dstrqc $是计算量最大的计算核心。对于塑性部分，$ drprecprc\_calc $是整个程序中最耗时的部分。剩下的内核包括$ fstr $，$ drprecprc\_app $，MPI的预处理和后处理（$ unpack\_VY $，
$ gather\_VX $和$ unpack\_VX $），这消耗了总运行时间的1-2％。我们也对其进行了极致的优化，以便实现最高的性能。

图~\ref{fig:kernel-result}演示了当采用不同的优化方法时，这些不同计算核心的性能和带宽改进结果。我们可以观察到，几乎所有的计算核心经过优化之后所获得的加速比都在30x左右，并且DMA传输带宽达到了总带宽的70％到80％左右。唯一的例外是$ fstr $内核，由于其非常低的计算密度，只能实现4倍至5倍的加速。因此，经过了一系列的优化后，不同内核在总执行时间内的分布变化与优化前相比并无太大变化。

在不同的优化方案中，协位数组的融合起着重要作用，对于最耗时的内核来说性能提高了4倍。

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{awp_performance.pdf}
\caption{当采用了不同的优化技术时，不同计算内核（主要是地震波传播部分）的加速比和内存带宽。 `MPE'代表仅使用MPE的原始版本。 `PAR'是指应用我们特定的并行化方案并使用MPE和64个CPE进行计算的版本。 `MEM'是指采用所有与内存相关的优化的版本。 “CMPR”是指进一步应用即时压缩方案的版本。}
\label{fig:kernel-result}
\end{figure}

\subsection{弱扩展性结果}

图~\ref {fig:weak-scaling}描述了线性和非线性情况下的地震模拟程序的弱扩展性结果。 对于这两种情况，我们使用每个CG来计算大小为160×160的网格，并逐渐扩展到整个机器。 对于弱扩展性，我们看到从8000个进程到160,000个进程中，我们几乎实现了完美的线性加速。 在没有使用实时压缩以及进行非线性地震模拟的情况下，我们可以通过使用160,000个进程来实现了15.2 Pflops的持续性能；而在线性地震模拟的情况下则为10.7 Pflops。 采用实时压缩方案后，相同的存储器带宽能够处理更多的数据，这分别进一步将线性和非线性地震模拟的性能提高至14.2 Pflops和18.9 Pflops。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{weak_scaling.pdf}
\caption{线性和非线性地震模拟的弱扩展性结果。测量的MPI的进程数从8000个扩展到160,000个，其中每个CG对应于一个MPI进程。}
\label{fig:weak-scaling}
\end{figure}

\subsection{强扩展性结果}

图~\ref {fig:strong-scaling}显示了基于三种不同网格大小的线性，非线性，含压缩以及不含压缩情况下的强扩展性测试结果。 我们可以看到，不管是线性或非线性地震模拟，含压缩或者是不含压缩，地震模拟软件都达到了类似的强扩展性结果。但随着进程数量的不断增加，性能出现了下降。性能的下降是由两个方面造成的：（1）计算与通信的比例减小；（2）外部Halo区域与每个进程内计算的网格体积比例的减小，这降低了AWP软件的计算和通信重叠的效果。


\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{strong_scaling.pdf}
\caption{线性和非线性地震模拟在三种不同的问题规模中的强扩展性结果。测量的MPI进程从8,000个扩展到16,000 个，其中每个CG上对应一个MPI进程。}
\label{fig:strong-scaling}
\end{figure}

\section{基于神威太湖之光的神威大地震模拟}

以前对剧烈地震的模拟通常局限于低频信息，因为高频模拟需要计算机提供巨大的内存和强大的计算能力。凭借神威太湖之光的强大计算能力，我们成功模拟了唐山大地震导致的地震波在华北地区的传播，地震波的最大频率为18 Hz。本次模拟的计算域是320公里乘以312公里（水平面）乘以40公里（深度）。本研究的地震模拟利用复杂几何断层引发的破裂动力源来推动地震波传播，估算唐山地震在华北地区的地震危险性分布。唐山断裂的几何构造以及构造应力场是通过观测和合理的推断推导出来的。在动态和地面运动模拟中，实现了垂直方向上分辨率为25公里，垂直方向上的分辨率为2公里的华北地区的三维速度模型。通常情况下，沉积结构被添加到强大的地面运动模拟中。这些复杂性的三位模型使我们对唐山地震的模拟接近现实。

\subsection{动态破裂源}

为了重现由唐山大地震造成的地震灾害，我们需要尽可能准确地描述地震环境和地震过程。地震发生后，唐山市南部出现了8至11公里的地表破裂带。这个破裂带由十多条东北方向的走滑右旋（right-lateral）、走滑左步（left-stepping）梯形断裂构成，总体走向为 N30$^\circ$E，水平方向上的位移为1.5至2.3米，垂直方向上的位移为0.2至0.7米。在地表破裂带的南部，垂直位移从西部上升到东部。在发现新的地表破裂带\citep {Qiu_discovery_2005}之后，更多的地质证据证明，1976年唐山地震的地表破裂带延伸至城市西南部超过47公里 \citep{guo_new_2011}，正如图\ref{fig:tangshan_geomap}a 所示。此外，深部地震反射剖面表明，唐山断裂系统极为复杂，可能深入到莫霍界面\citep{liu_seismogenic_2007}。

根据之前对唐山地震的调查和观测，我们构建了唐山断裂的三维几何结构，如图\ref{fig:tangshan_geomap}b 所示。非平面断层分别沿着走向（strike）和倾向（dip）方向延伸约70公里和35公里。两个水平原理的压缩应力与图\ref{fig:tangshan_geomap}a 所示的方向一样用作动力学模拟中的驱动力。

描述断层几何的复杂性需要强大的数值方法和计算能力才能实现不规则断层面上的动态条件。我们在神威太湖之光超级计算机上面进行了基于复杂几何断层的唐山大地震动力破裂模拟，这位后续的地震波传播模拟提供了震源。图 \ref{fig:tangshan_geomap}b 表示了T=10.5秒时唐山地震破裂的绝对滑动速率（absolute slip rate）快照。由于断层走向的曲率变动的原因，破裂断层的东北侧表现出更多的复杂性。这证实了动态破裂模拟过程中复杂三维断层几何的重要性。

\begin{figure}[t]
\begin{tabular}{cc}
(a) & (b) \\
    \includegraphics[width=0.5\columnwidth]{Tangshan_geomap.eps} &
    \includegraphics[width=0.5\columnwidth]{Tangshan_Fault1.eps}
\end{tabular}
    \caption{
（a）唐山地震模拟的强地面运动，模拟的区域为（320km×312km×40km）。 该地区的沉积深度以颜色表示; 显示了震中（红星），地震断层（红色曲线）和应力场（红色和蓝色矢量）。 （b）通过CG-FDM方法计算的动态源（T = 10.5秒时的断层的绝对滑移率）。}
    \label{fig:tangshan_geomap}
\end{figure}


\subsection{强地面运动}

为了能够完全重现唐山大地震震源附近较大区域的地震危险性分布，我们将动力破裂源作为地震波传播过程的输入，计算出较大区域的强地面运动。 我们模拟的区域是320公里乘以312公里乘40公里（约115.7$^\circ$E$\sim$119.7$^\circ$E, 38.0$^\circ$N$\sim$41.7$^\circ$N，如图\ref{fig:tangshan_region}所示），这个区域包括了唐山，北京，天津等主要城市。在本研究的地震模拟中，空间分辨率逐渐从500米增加到8米，最大频率从0.5 Hz增加到18 Hz。

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{GeoMapTangshan.pdf}
        \caption{唐山地震模拟区域。}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{fault.pdf}
        \caption{唐山地震破裂带。}
    \end{subfigure}
    \caption{唐山地震模拟区域和地震破裂带。}
    \label{fig:tangshan_region}
\end{figure*}

由于动态破裂、复杂的三维介质和地质结构，地震可能产生高频能量（高达至少10 Hz）。因此具有高效的高频分量的地震图是工程地震分析的重要数据，能够为建筑物的抗震设计提供了合适的标准。

\begin{figure*}[ht]
    \includegraphics[width=1.0\columnwidth]{Snap_Vx_tangshan.pdf}\\
    \caption{速度分量（东西方向）分别在（a）T = 10秒，（b）T = 20秒，（c）T = 30秒等不同时刻地震波传播的波场快照。}
    \label{fig:strong_motion_2}
\end{figure*}

图\ref{fig:strong_motion_2}描述了在（a）T = 10秒，（b）T = 20秒，（c）T = 30秒等不同时刻地震波传播的波场快照。破裂从断层中心开始，向双向传播。 由于近地表的弯曲断层，北方波场显示出更复杂的模式，这表明了大地震模拟期间非平面断层几何的重要性。

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\columnwidth]{CompareDifferentSpatialResolution.eps}\\
  \caption{
不同空间分辨率的地震模拟结果比较。左栏是空间分辨率为200m的地震模拟，右栏相应的16m分辨率。 （a $ \thicksim $ b）宁河县和沧州市速度（W-E分量）地震记录图; （c $ \thicksim $ d）地震发生后60秒处的速度场快照，右上角的缩放子图描述了具有沉积作用的区域的细节（虚线方框）; （e $ \thicksim $ f）地震震害分布图，由水平方向上的地面峰值速度来计算获得。}
  \label{fig:strong_motion}
\end{figure}

如图\ref{fig:strong_motion}a$\sim$b 所示，不同的空间分辨率模拟之间存在明显的差异，较低的空间分辨率（如200米）不足以很好地描述盆地结构（如图\ref{fig:tangshan_geomap}a 所示，其中最大沉积深度为800米）。


因此，严格来说，使用较低的空间分辨率来计算尾波是不准确的。本研究的使用高分辨率进行模拟，可以看到沧州市主要地震能量在较高空间分辨率下模拟的尾波振动比较低（如图\ref{fig:strong_motion}b所示）。此外，由于唐山地震的震中位于沉积盆地，地震的主峰甚至无法准确计算出来（见图\ref{fig:strong_motion}a$\sim$b，宁河）。总而言之，空间分辨率的高低直接关系着复杂地震模拟的准确度，16米以上的高分辨率模拟是三维复杂结构的地震模拟的关键。

我们还比较了地震波波场快照和震害图（hazard map)用以说明不同结果的细节。 图\ref{fig:strong_motion}c$\sim$d 描述了唐山地震发生60秒后地震波波场的快照。 虽然波场的主要行为是相似的，但我们在小尺度上看到明显的差异。 图\ref{fig:strong_motion}c$\sim$d右上角的放大图片说明了这次地震中的沉积物效应。我们可以看到，在空间低分辨率下沉积物效应不能很好地被描述出来。

唐山地震的震害图（用地震烈度表示）可以通过计算水平方向上的地面峰值速度来获得。对此，我们比较了200米和16米分辨率下的震害图（如图\ref{fig:strong_motion}e$\sim$f 所示）。其中，红色（烈度为9$\sim$11）表示地震中最严重的破坏，这非常依赖破裂断层的位置。但由于沉积作用，高强度的破坏影响被重新分配了：位于唐山市的滦南县虽然不在地震断层的周围，也遭受了严重的破坏。对于不同的分辨率下地震模拟结果，震害图显示了高分辨率模拟（如图\ref{fig:strong_motion}f 所示）和低分辨率模拟（如图\ref{fig:strong_motion}e 所示）的巨大差异。例如，武清地区的烈度在左图中为6级（蓝色），右图为7级（绿色）。这再次证明了高分辨率模拟在实现准确地震危险图方面的重要性。




\section{总结}

我们在这里要强调的一个信息是，本研究设法将神威太湖之光超级计算机的各个方面的性能都发挥到了极限。 如Table \ref{tb:push-limit}所示，对于每一项指标，我们都几乎将其发挥到了极限，特别是对于内存系统。 因此，即便神威太湖之光超算系统的内存系统与美国的泰坦超算系统相比处于劣势（太湖之光的byte-to-float之比仅为美国泰坦超算的1/5，而每个计算密集的kernel需要30个3D数组），我们仍然能够在不使用压缩方案的情况下使用神威太湖之光的10,400,000个计算核心，实现了性能为15.2-Pflops非线性唐山大地震模拟，达到了全机峰值性能的12.2％。

\begin{table}[!t]
%\footnotesize
\caption{
神威太湖之光能够支持的最大地震模拟测试用例下的计算与内存性能（无压缩情况下）。此处描述的是每个核组的计算性能，内存大小和带宽；单个CPE从核的LDM大小。请注意，在每个核组的8 GB内存中，我们必须为整个机器的正常运行和MPI缓冲区预留2.5 GB的空间。}
\label{tb:push-limit}
\centering
\begin{tabular}{cccc}
\hline\hline
  & Effectively used & Peak & \% \\

  Computing Performance & 98.7 Gflops & 765 Gflops & 12.9\% \\
  Memory Size & 5.2 GB & 5.5 GB & 94.5\% \\
  Memory Bandwidth & 25 GB/s & 34 GB/s & 73.5\% \\
  LDM Size & 60 KB & 64 KB & 93.8\% \\\hline
\hline
\end{tabular}
\end{table}

更令人兴奋的创新当然是实时压缩方案，它以可接受的准确度损失为代价，将我们的仿真性能和功能扩展到超出机器物理约束的范围。 通过利用10,485,760个CPE进行神威超算整机模拟，并且采用实时压缩策略，本研究的模拟规模相当于使用两台神威超算进行的模拟，这几乎是历史上最大规模的一次科学计算数值模拟。采用实时压缩方案，我们获得了两个好处，在物理内存限制的情况下能够进行更大规模的科学模拟，或者在相同的问题规模下，将地震模拟的性能进一步提升了24％。

我们的压缩方案将我们的计算性能扩展到了18.9 Pflops（峰值的15％），并且使我们能够支持18 Hz、8米分辨率的地震模拟。这对于以前的技术水平来说是一个巨大的飞跃。 虽然目前的压缩方案主要是针对我们的特定应用和神威架构定制的，但我们相信这个想法有很大的潜力应用于其他应用和其他架构，特别是在内存带宽成为主要限制的时代。

\chapter{基于神威太湖之光的集合全波形反演方法}

人工地震是由人类通过工业爆破、地下核爆炸、机械撞击等等引起的地震。与天然地震相比，人工地震具有更高的可控性，一般不会对人类经济活动造成巨大的损害。人工地震常用于研究天然地震原因和探测当地矿产资源，是地球物理勘探过程中必不可少的手段。

集合全波形反演方法是地球物理勘探领域非常前沿的地质结构反演方法，与传统的全波形反演方法相比，具备更大的收敛域和更强的抗噪能力，同时也面临着更大的计算挑战。本章通过使用神威太湖之光超算来设计高效率、高可扩展性的集合全波形反演方法并行模型。

\section{背景知识与算法概述}

在地球物理勘探中，人们通常使用地震车或空气枪制造人工地震，地震波在地下或海洋中的不同的介质（如岩石、盐丘、水等）的分界处发生反射、折射（如图\ref{fig:offshoreseismicsurvey}所示\footnote{图片来源：http://oilnow.gy/exploration/ratio-announce-start-3d-seismic-work-kaieteur-block/，并进行适度修改。}），并最后被位于地表的地震数据采集阵列接收，接收到的数据称为地震记录。通过对人工震源函数、地震区域模型、地震记录的分析，采取一系列地震资料成像方法，如逆时偏移算法、全波形反演方法，人们可以反演“看到”所研究区域的地下介质分布，从而确定石油、天然气等重要矿产资源的位置和产量，为后续的钻井勘探提供可行参照。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\columnwidth]{offshoreseismicsurvey.pdf}
  \caption{地球物理勘探过程。}
  \label{fig:offshoreseismicsurvey}
\end{figure}

\subsection{集合全波形反演方法}

全波形反演方法（full waveform inversion; FWI）于1984年由Tarantola提出，能准确地从人工地震记录中提取大量地下介质信息\cite{tarantola1984inversion,plessix2012full,brossier2009seismic}。该方法通过对比数值模拟合成的地震记录和实际观测的人工地震记录误差，迭代更新模型参数，从而反演地下介质模型\cite{yushu}。然而，由于该方法伴随着巨大的计算开销，直到2010年超级计算机和NVIDIA GPU，Intel MIC等加速器的兴起，为全波形反演方法提供了足够的算力，该方法才逐渐成为研究热点，并为各大石油公司采用。

尽管全波形反演方法在理论上能够很好地反演地下介质模型，但在实际生产中却面临着两大问题。一方面，全波形反演方法需要非常准确的初始速度模型作为输入，否则该算法很容易陷入局部最优\cite{virieux2009overview}。虽然地震记录中的低频信号成分能够降低对初始速度模型的要求，但是在实际生产场景中往往很难捕捉到低频信号\cite{sirgue2006importance}。另一方面全波形反演方法对地震记录数据中的噪音非常敏感，这严重影响了该算法的实际成像效果。

集合全波形反演方法（EnKF）\cite{yushu,he2015ensemble}在传统完全反演\cite{tarantola1982generalized}（total inversion）的基础上，通过使用集合卡尔曼滤波\cite{evensen2003ensemble}中的集合协方差来近似完全反演中协方差算子，这降低了在完全反演中更新协方差算子的计算开销，使之可以在一般的分布式集群中进行计算。然而，由于速度模型中需要更新的参数数量比集合样本数量大2-4个数量级，集合卡尔曼滤波的更新增量受到很大的限制。对此，集合全波形反演方法再次引入了震源编码技术\cite{krebs2009fast}。震源编码技术将所有的震源和地震记录分别进行叠加，每次叠加时不同的震源和地震记录分别使用不同的震源编码，在有利于克服局部收敛的同时\cite{castellanos2014fast}，也克服了集合卡尔曼滤波低秩空间的局限性。

均匀介质的声波全波形反演方法可以通过方程\ref{eq:fwi}表示：
\begin{equation}
\label{eq:fwi}
  \min_{m} \phi(m) = \sum_{i=1}^S \rho(\mbox{H}_i(m) - d_{i} )，
\end{equation}
其中$m$表示背景速度模型，$\rho(\cdot)$是一个补偿函数，$d_i=d_1, \ldots, d_S$表示观测数据（实际接收的地震记录），$\mbox{H}_i=\mbox{H}_1, \ldots, \mbox{H}_S$则表示与之相对应的第$i{th}$个正演算子，$S$表示总勘探实验次数（每发射一炮代表一次实验）。这个优化问题通常使用如方程\ref{eq:steepest}形式的迭代算法进行求解：
\begin{equation}
\label{eq:steepest}
m_{k+1} = m_{k} + \alpha_{k}s_k，
\end{equation}
其中$s_k$表示搜索方向，$\alpha_{k}$表示步长。常用的方法包括最速下降法、共轭梯度法等等。

集合全波形反演方法在在每次速度模型更新之后，使用集合卡尔曼滤波对速度模型集合进一步更新，如方程\ref{eq:enkfupdate}所示：
\begin{equation}
\label{eq:enkfupdate}
\left(\mbox{H}\Psi_k^{'}(\mbox{H}(\Psi_k^{'})^T)+D_k^{'}(D_k^{'})^T\right)^{-1}\approx\left((\mbox{H}\Psi_k^{'}+D_k^{'})(\mbox{H}\Psi_k^{'}+D_k^{'})^T\right)^{-1}.
\end{equation}
其中$k$表示第$k$个迭代步，$\Psi_k=(\psi_1^k,\psi_2^k,\ldots,\psi_N^k)$表示有$N$个速度模型样本组成的背景速度场，$D=(d_1^k,d_2^k,\ldots,d_N^k)$表示添加扰动之后的观测数据集合。通过SVD分解，我们有
\begin{equation}
\label{eq:svd}
\mbox{H}\Psi_k^{'}+D_k^{'}=USV，
\end{equation}
其中$U$和$V$是正交矩阵，$S$是对角矩阵。结合方程\ref{eq:enkfupdate}和\ref{eq:svd}，我们获得集合更新方程\ref{eq:enkf2}
\begin{equation}
\label{eq:enkf2}
(\Psi_a)_k=\Psi_k+\Psi_k^{'}(\mbox{H}\Psi_k^{'})^TU(SS^T)^{-1}U^T(D_k-H\Psi_k).
\end{equation}
其中$\Psi_a$表示分析速度场。

\subsection{算法分析与优化挑战}

\subsection{集合全波形反演算法分析}
算法 \ref{alg:enfwicode} 以伪代码的形式描述了集合全波形反演算法。与传统的全波形反演不同，集合全波形反演算法首先需要初始化EnFWI分析步所需要的相关输入，如添加随机扰动后的背景速度场集合、观测数据以及EnFWI分析步所需要的参数（第\ref{ln:enfwiinit}行）。由于背景速度场集合是由同一个初始速度场添加扰动所得，剧烈的扰动会极大改变背景速度场，从而影响全波形反演的成像结果。因此，在进行正式的全波形反演迭代之前，需要执行一次EnFWI分析步计算（第\ref{ln:enfwi1enkf}行），以便为后续的迭代提供一个差异化且较为平滑的初始速度集合。

\begin{algorithm}[ht]
%\scriptsize
%\footnotesize
\small
\caption{集合全波形反演算法伪代码}\label{alg:enfwicode}
\begin{algorithmic}[1]
\State \textbf{//初始化背景速度场集合(vels)、观测数据(obsdata)、EnFWI分析步所需参数(lambdas, ratios)}
\State [vels, obsdata, lambdas, ratios] = init4enkf(); \label{ln:enfwiinit}
\State
\State \textbf{//根据初始速度场进行首次EnFWI分析步计算}
\State enkf\_analyze(vels, obsdata, lambdas, ratios); \label{ln:enfwi1enkf}
\State
\State \textbf{for} (iter = 1; iter < niter; iter++) \{ \textbf{//EnFWI收敛所需运行的总迭代次数} \label{ln:enfwiiter}
\State
\State \quad\quad \textbf{//nsamples为集合样本数目，每一个样本执行独立的震源编码FWI}
\State \quad\quad \textbf{for} (isample = 0; isample < nsamples; isamples++) \{ \label{ln:enfwisample}
\State \quad\quad\quad\quad \textbf{//生成震源编码，并对震源和观测数据进行统一编码}
\State \quad\quad\quad\quad codes = gen\_codec(nshots); \label{ln:enfwicodec}
\State \quad\quad\quad\quad [ensrc, endata, resd, prevgrad, curgrad] = init4essfwi();
\State \quad\quad\quad\quad \textbf{for} (ishot = 0; ishot < nshots; ishots++) \{
\State \quad\quad\quad\quad\quad\quad ensrc += codes[ishot] * srcs[ishot];
\State \quad\quad\quad\quad\quad\quad endata += codes[ishot] * obsdata[ishot];
\State \quad\quad\quad\quad \} \label{ln:enfwicodecend}
\State
\State \quad\quad\quad\quad \textbf{//基于震源编码的地震波正演，计算量最大的函数之一}
\State \quad\quad\quad\quad syndata = forward\_modeling(ensrc, vels[ismaple]); \label{ln:enfwiforward}
\State \quad\quad\quad\quad vsrc = endata - syndata; \label{ln:vsrc}
\State \quad\quad\quad\quad resd += cal\_residual(endata, syndata); \label{ln:resd}
\State
\State \quad\quad\quad\quad \textbf{//基于震源编码的伴随波场反传，计算量最大的函数之一}
\State \quad\quad\quad\quad [vdata, curgrad] = backward\_propagate(vsrc, vels[isamples]); \label{ln:enfwibackward}
\State \quad\quad\quad\quad updatedir = conj\_gradient(prevgrad, curgrad); \textbf{//计算更新梯度方向}
\State \quad\quad\quad\quad swap(prevgrad, curgrad);
\State
\State \quad\quad\quad\quad \textbf{//计算更新步长，计算量最大的函数之一}
\State \quad\quad\quad\quad alpha = cal\_steplen(ensrc, vels[isample], updatedir, resd);
\State \quad\quad\quad\quad update\_vel(alpha, updatedir, vels[isample]); \textbf{//根据梯度和步长更新背景速度场} \label{ln:enfwibackwardend}
\State \quad\quad \} \label{ln:enfwisampleend}
\State
\State \quad\quad \textbf{if} (iter \% enkf\_interval == 0) \{ \textbf{//每隔enkf\_interval迭代步进行EnFWI分析步计算} \label{ln:enfwienkfbegin}
\State \quad\quad\quad\quad enkf\_analyze(vels, lambdas, ratios);
\State \quad\quad \} \label{ln:enfwienkfend}
\State \} \label{ln:enfwiiterend}
\end{algorithmic}
\end{algorithm}

集合全波形反演方法与传统全波形反演的计算流程类似，都是通过迭代算法来求解（第\ref{ln:enfwiiter}到\ref{ln:enfwiiterend}行）。但由于集合全波形反演方法使用集合样本来替代传统全波形反演中的独立样本，集合全波形反演方法需要对集合中的每一个样本独立执行完成的FWI计算（第\ref{ln:enfwisample}到\ref{ln:enfwisampleend}行）。这是集合全波形反演方法引入额外计算开销的最主要原因。集合中的每一个样本都是一个独立的完整全波形反演计算，因此集合全波形反演的计算复杂度与集合的样本数量成线性相关。为了降低计算复杂度，集合全波形反演方法引入了震源编码，在每一个迭代步每一个样本中对所有震源和观测数据进行震源编码（第\ref{ln:enfwicodec}到\ref{ln:enfwicodecend}行），然后对编码后的震源进行线性叠加。未进行震源编码时，全波形反演需要遍历每一炮勘探实验，而进行震源编码之后，不再需要逐炮进行遍历，而是将不同位置的震源根据震源编码同时添加到地震波场中。震源编码技术直接将全波形反演的计算复杂度将为原来的$1/nshots$，其中$nshots$表示炮的数目。这在大规模三维石油勘探中的加速效果非常明显，因为大规模石油勘探中炮击的次数通常为成千上万甚至十万个。值得注意的是，在每个迭代步每个样本的全波形反演中，所有炮击的震源和观测数据使用相同的编码，但在不同迭代步不同样本中，却需要重新生成编码。

随后，以编码后的震源和当前样本速度场作为输入进行地震波正演（forward\_modeling），模拟地震波从震源发射，传播经过不同地下介质，最后到达地震记录接收器的整个过程（时间从$t=0$到$t=nt-1$时刻，第\ref{ln:enfwiforward}行）。由于当前速度场并不是真实的速度场，以此速度场进行正演得到的地震记录成为合成地震记录（syndata）。通过实际观测地震记录和当前合成地震记录，计算可得伴随波场震源（vsrc）和目标函数残差（resd，第\ref{ln:vsrc}行和第\ref{ln:resd}行）。全波形反演的目标则在于最小化目标函数残差，残差越小，代表当前速度模型与真实速度模型越接近。以伴随波场地震和当前速度模型作为输入进行地震波反传（时间从$t=nt-1$到$t=0$时刻），并与对应的正传波场进行相关（correlation）操作，可以求得速度场更新的梯度和步长，用以更新当前速度模型（第\ref{ln:enfwibackward}到\ref{ln:enfwibackwardend}行）。在整个基于震源编码的全波形反演中，计算量最大的三个过程分别是地震波正传、伴随波场反传和计算更新步长。每一个过程都需要对整个波场进行从$t=0:nt-1$或$t=nt-1:0$进行更新。时间步长越短、计算网格分辨率越高或者有限差分算子长度越大都会急剧增大波场更新的计算量，给现代并行计算机带来巨大的挑战。更新波场的过程将在下文详细介绍。对集合中的每一个速度场使用基于震源编码的全波形反演进行更新之后，每隔一定的迭代步，执行集合卡尔曼滤波分析以进一步更新背景速度场集合（第\ref{ln:enfwienkfbegin}到\ref{ln:enfwienkfend}行），然后进入下一个迭代步。

\subsection{集合卡尔曼滤波分析步计算分析}
集合卡尔曼滤波的分析步旨在利用集合替代样本，降低初始速度模型和数据噪音对对算法收敛的影响，提高算法的收敛域。集合全波形反演的额外开销一方面来源于使用集合替代单独样本，在全波形反演中需要为集合中的每个样本单独更新背景速度场。然而，在引入了震源编码之后，震源编码对所有的炮点进行叠加，极大降低了计算复杂度，减轻甚至抵消了集合样本带来的开销。集合全波形反演另一方面的开销则是集合卡尔曼滤波分析步（简称分析步）的计算，虽然并不需要在每个迭代步中都执行分析步计算，但深入分析有利于我们更好地理解集合卡尔曼滤波分析步，并为后续的优化提供可靠的思路。

公式\ref{eq:enkf2}描述了分析速度场的更新方式。结合算法 \ref{alg:enfwicode}，我们可以发现如下计算特性：
\begin{itemize}
  \item 分析速度场的更新是所有速度模拟样本的整体更新，而不是像震源编码全波形反演中的每个速度模型独立更新。这里对计算机的内存提出了重大的挑战，尤其是当集合样本数目巨大的时候。
  \item 分析速度场更新的主要计算是大型矩阵和向量代数运算，包括矩阵加法、乘法、转置、求逆等计算密集型的操作，这与神威太湖之光强大的计算能力十分吻合。
  \item 正演算子$\mbox{H}$，即算法 \ref{alg:enfwicode} 中的forward\_modeling函数仍旧在公式中扮演者重要的作用。通过程序用时解剖（profile）发现，正演算子$\mbox{H}$依旧是EnFWI分析步最耗时的函数，但与算法 \ref{alg:enfwicode}不同，分析步中的正演算子无需设计IO存储，因为分析步中不需要进行地震波反传操作并进行相关（correlation）操作。
\end{itemize}

\begin{table}[ht]
\centering
\caption{集合卡尔曼滤波分析步中不同计算操作耗时百分比。}
\label{tb:enkfprofile}
\begin{tabular}{ccccccc}
\hline
计算操作  & 矩阵加法  & 矩阵乘法  & 矩阵求逆   & 矩阵转置  & 正演算子   & 其他    \\\hline
所占百分比 & 3.2\% & 8.5\% & 17.0\% & 4.3\% & 58.9\% & 8.1\% \\\hline
\end{tabular}
\end{table}
表\ref{tb:enkfprofile}总结了集合卡尔曼滤波分析步中不同计算操作耗时百分比。我们可以发现，优化分析步计算的核心是优化正演算子和矩阵求逆运算。同时，需要同时考虑神威超算系统单节点较小的内存，进行适当的任务划分。

\subsection{地震波波场更新计算分析}
集合全波形反演算法中计算量最大的模块是地震波波场更新，这在算法 \ref{alg:enfwicode} 中体现为$forward\_modeling$、$backward\_propagate$和$cal\_steplen$三个函数，在EnFWI分析步中体现为正演算子$\mbox{H}$。尽管在不同的函数中，更新地震波波场的方式略有不同（有的是正传，有的是反传，有的需要储存波场），但其核心计算都是求解偏微分方程。以$forward\_modeling$为例，离散时间步的波场正演可以用公式来表示：
\begin{equation}
\label{eq:enfwifd}
U_{i+1} = \left[ \Delta U -f(t) \right] v^2 \Delta t^2 + 2 U_{i} - U_{i-1}，
\end{equation}
其中 $U_{i-1}$, $U_{i}$ and $U_{i+1}$分别表示上一时间步、当前时间步和下一时间步的波场，$\Delta$表示Laplacian算子，
$f(t)$ 表示震源子波（通常为雷克子波），$v$ 表示速度。算法\ref{alg:acousticfdcode}是均匀介质三维声波正演伪代码，为了方便描述，使用的是基于2阶的有限差分算子\cite{fu2011eliminating}。但是2阶有限差分算子会带来严重的频散，实际生产和研究中，常常使用10阶或者12阶差分算子。



\begin{algorithm}[ht]
%\scriptsize
%\footnotesize
\small
\caption{均匀介质三维声波正演伪代码}\label{alg:acousticfdcode}
\begin{algorithmic}[1]
\State \textbf{for} ( it = 0; it < nt; it++ ) \{ \label{ln:fdntbegin}
\State \quad\quad \textbf{for} ( ix = 0; ix < nx; ix++ )
\State \quad\quad\quad\quad \textbf{for} ( iy = 0; iy < ny; iy++ ) \{
\State \quad\quad\quad\quad\quad\quad \textbf{for} ( iz = 0; iz < nz; iz++ )\{
\State \quad\quad\quad\quad\quad\quad\quad\quad  u(it,ix,iy,iz)=2*u(it-1,ix,iy,iz)-u(it-2,ix,iy,iz)+v(ix,iy,iz)*v(ix,iy,iz)*dt*dt*(
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad                                    u(it-1,ix,iy,iz)*(2/dx/dx+2/dy/dy+2/dz/dz)+
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad                                    (-u(it-1,ix,iy,ix-1)-u(it-1,ix,iy,ix+1))/dx/dx+
\State \quad\quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad                                    (-u(it-1,ix,iy-1,iz)-u(it-1,ix,iy+1,iz))/dy/dy+
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad                                    (-u(it-1,iz-1,iy,iz)-u(it-1,iz+1,iy,iz))/dz/dz);
\State
\State \quad\quad\quad\quad\quad\quad\quad\quad \textbf{if} ( is\_boundary(ix, iy, iz) ) \textbf{//吸收边界处理} \label{ln:fdboundary}
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad    apply\_absorb\_boundary\_condition(u);
\State \quad\quad\quad\quad\quad\quad                \} \label{ln:fdntend}
\State \quad\quad\quad\quad\quad\quad \textbf{if} (is\_record\_seismo) \textbf{//输出合成地震记录}
\State \quad\quad\quad\quad\quad\quad\quad\quad save\_seismo(u(it, ix, iy, :);
\State \quad\quad\quad\quad \}
\State
\State \quad\quad \textbf{if} ( is\_record\_wavefield ) \textbf{//输出波场快照}
\State \quad\quad\quad\quad output\_wavefield(u(it, :, :, :));
\State
\State \quad\quad \textbf{for} ( is = 0; is < ns; is++ ) \textbf{//在正传波场中添加震源信号}
\State \quad\quad\quad\quad u(it, src[is].x, src[is].y, src[is].z) += wavelet(is, it);
\State \}
\end{algorithmic}
\end{algorithm}

算法\ref{alg:acousticfdcode}仅描述了在一个速度模型样本，单个震源或者震源编码情况下的地震波正演。在传统的多震源正演和全波形反演中，不同震源之间是相互独立的，可以完全使用不同的节点分别对不同的震源进行地震波模拟，相互之间没有通信开销（全波形反演算法中有少量的通信）。因此，提升正演算子的性能只需要关注单个样本、单个震源下的地震波传播模拟即可。地震波正演过程中，核心计算是在$t=0$到$t=n-1$时刻不停往波场中注射震源激励信号、更新波场（第\ref{ln:fdntbegin}到\ref{ln:fdntend}行），并根据需要输出合成地震记录或者正传波场：
\begin{itemize}
  \item 在$forward\_modeling$函数中，需要同时输出合成地震记录和正传波场。合成地震记录用以计算目标函数残差，正传波场用以在$backward\_propagate$函数中与伴随波场进行相关（correlation）操作。
  \item 在$backward\_propagate$函数中，合成地震记录和正传波场都无需输出，但需要读取$forward\_modeling$中的正传波场。
  \item 在$cal\_steplen$函数中，只需要输出合成地震记录，用以确定更新步长。
\end{itemize}

在生产环境中，虽然我们在一定的范围内进行人工地震石油勘探，但地震波的传播并不会限定在特定范围，而是无穷无尽向四面八方传播。而在数值模拟过程中，由于人为规定了模拟的区域，地震波到达规定区域的边界时会进行反弹，这便于实际情况不符。因此，在数值模拟中，我们常在模拟区域的边界处添加吸收边界条件（absorb boundary condition，第\ref{ln:fdboundary}到\ref{ln:fdntend}行），消除地震波在边界处的反射现象。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{stencil7131927.pdf}
  \caption{不同stencil算子的空间结构\cite{zhang2013autogeneration}。（a）7点三维结构；（b）13点三维结构；（c）19点三维结构；（d）27点三维结构。}
  \label{fig:stencilstruct}
\end{figure}

正演算法在神威超算上的并行优化挑战主要有两个。第一个是有限差分算子需要较高的数据重用与申威处理器从核有限的LDM空间之间的挑战。算法\ref{alg:acousticfdcode}使用的是如图\ref{fig:stencilstruct}(a)中的7点星形stencil算子。对于每一个中心点进行更新时，需要访问中心点的上下左右前后方向各1个点。面对更复杂的stencil时，如图\ref{fig:stencilstruct}中的19点或者27点结构，则需要访问更多的邻接格点来更新中心格点。在集合全波形反演算法中，使用的是31点的10阶星形stencil。每一个点都需要复用5次。尽管每个邻接格点距离中心点很“近”，但在物理内存的连续存储中，不同的邻接格点却相距甚远，分别相距若干行、若干列甚至若干平面。在Intel CPU架构下直接进行访问时，会出现大量的cache miss，从而反复从内存中加载数据，计算效率非常低下。在Intel CPU架构中，高效的stencil运算操作需要充分利用L1、L2高速缓存，尽可能在一个点未完全访问完毕后锁定在cache中\cite{datta2008stencil,datta2009optimization,sellappa2004cache}。GPU架构下的多层级内存结构非常适合stencil运算，一方面来自于GPU高效的内存带宽，另一方面开发者手动可控的共享内存（shared memory）也为stencil的优化提供了巨大的便利\cite{meng2009performance,nguyen20103,micikevicius20093d}。对于神威超算系统而言，每个从核虽然具备类似GPU共享内存的LDM高速缓存，但是每个从核的LDM大小只有64KB，无法装载大量格点。此外，GPU显存的带宽显著高于神威超算的内存带宽，而stencil运算又是典型的访存受限问题，这也给stencil的优化带来巨大的挑战。

另一个挑战来源于正演算法的流程本身。波场更新的每一步都需要注射震源，在震源编码叠加之后，震源的位置分布不连续，在三维勘探中，震源分布在水平面。如果把添加震源的操作放在申威处理器的主核完成，则效率低下，如果交给从核计算，由于DMA加载数据不连续且块大小非常小，DMA效率也很低下。另一方面，在每个申威核组只有8GB内存的情况下，$forward\_modeling$函数无法将所有正演波场存储在内存中，必须通过IO输出到磁盘中。但输出波场到磁盘中的也面临两个挑战。首先，在三维正演模拟中，每个波场的大小约为0.5-4GB（根据区域划分确定），每次模拟需要执行$10^4$值$10^7$个时间步，如果将所有的波场输出到磁盘，磁盘将面临500GB-10PB的空间压力，这并不是一般的磁盘阵列能够承受的。其次，神威超算采用的是网络文件系统，每个节点并没有独立的本地磁盘，当进行成千上万甚至十万进程大规模IO输出是，如果均衡分配IO代理，调度IO进程则显得尤为重要。


\subsection{设计与优化思路}

针对上述集合全波形反演的诸多问题与挑战，结合神威太湖之光超算系统结构特性，我们从以下几方面考虑高效的并行设计优化策略。

合理的并行任务划分机制。神威超算系统的共有40960个计算节点，而排名第二的天河2号仅有16000个计算节点\cite{tianhe-2}。然而神威系统单节点主频只有1.45GHz，而天河2号则为2.2GHz。良好的并行任务划分策略能够充分利用神威超算的众多节点优势，同时克服单节点内存不足的劣势。集合全波形反演应用中，我们设计了高效的多层级并行任务划分机制，分别从集合样本、三维模型区域分解、单核组从核内部分块等三方面将集合全波形反演算法高效率扩展多8192个神威超算核组。

高效的LDM缓存数据重用。 高效利用神威从核LDM缓存是提升单节点stencil运算的重点。对于类stencil等访存受限算法，优化的核心是提高数据复用率，降低内存访问。神威从核LDM缓存是数据复用的最佳场所，因此我们尽可能让格点停留在有限的LDM中，直到对该格点的所有访问结束。从核通过DMA将数据从主存传输到LDM中，这是stencil计算的最主要开销。DMA的传输效率受每次DMA传输数据的块大小影响，我们通过数学建模，推导出在不同情况下最优的DMA传输配置。此外，为了进一步提高LDM的有效利用率，降低重复的DMA数据传输，我们还提出了函数融合、寄存器通信、从核ID重映射等优化方案。

随机边界处理与数据压缩。在$forward\_modeling$函数中，存储正演波场的目的是为了与在$backward\_propagate$函数中反传的地震波场做相关（correlation）操作，求解更新速度模型的梯度。在密集的计算流程中频繁执行IO操作极大地降低了整体计算效率。一种可行的方法是根据声波方程的可逆性，只存储$t=n-1$和$t=n-2$时刻的波场，并按照声波方程求解$t=n-3$到$t=0$时刻的波场。与前者相比，后者用重复计算换取IO开销，这通常能获得更高的效率。然而，声波方程的可逆性无法处理边界问题。后者方法仍然需要存储不同时间步波场的边界，并在反传时读取覆盖已有的边界，以保证与伴随波场的正确相关。为了进一步降低对IO的依赖，以更多的计算替代IO开销，更充分发挥神威超算的计算性能，本研究使用随机边界处理方法，该方法使用随机数消除了震源波场与伴随波场的耦合性，以可接受的边界吸收精度抵消了震源波场的边界存储，提升了整体计算性能。此外，我们还是用了LZ4算法对合成地震记录进行压缩，缩小合成地震记录的体积，节约IO带宽的同时缩短了IO时间。

\section{基于集合样本的多层级并行任务分解} % (fold)
\label{sec:基于集合样本的多层级并行任务分解}

本章首先将结合神威超算独特架构与集合全波形反演方法具体应用，提出高效的、高可扩展性的多层级并行任务划分方案。然后分析该并行方案的通信开销，并提出计算和通信重叠的策略。

\subsection{多层级任务划分} % (fold)
\label{sub:多层级任务划分}
当地球物理勘探应用需要处理较大区域的地震数据时，单节点计算机无法满足应用软件所需的内存和计算时间需求，并行计算机系统应运而生。并行计算处理的基本流程如下：通过把软件应用处理的大问题分解成若干个较小问题，分别派发到并行计算机的不同节点上参与运算，必要时在不同节点间通过网络进行数据交换，计算完毕后，将结果汇集到主计算节点。合理的任务划分是并行计算机高效运算的关键。应该遵循以下基本原则：
\begin{itemize}
  \item 分配给每个计算节点的任务尽量均衡。一般的并行计算机系统采用相同的节点连接而成，每个节点的处理能力相当，如果出现某个节点需执行大量任务，则必要会使其他节点闲置等待，造成负载不均衡现象，影响整体运算性能。
  \item 尽量降低节点间通信开销。将一个完整的大问题分解成若干小问题并让不同计算节点计算完成的过程中，难免需要引入节点间额外的数据交换开销。在小规模并行计算中，通信不会成为主要的应用的瓶颈，但是当并行规模达到成千上万甚至十万百万量级时，通信的开销将变得不可忽略，有限的通信带宽与随之而来的通信延迟将极大降低通信的效率，此时通信甚至会成为主要的瓶颈。因此，采取适当的划分方法降低节点间通信的开销能够很好地缓解这个问题。此外，在大规模的并行应用中，还应该尽可能避免大量集合通信。集合通信要求不同进程在程序运算时特定时间点进行同步，而不同进程几乎很难在同一时刻到达集合通信点，会出现先到达的进程等待的现象，而先到达的进程又有可能在下次集合通信点晚点到达，导致不同进程交叉等待，降低了超算不同资源的利用效率。使用异步的非集合通信能够有效地避免这个困扰。
\end{itemize}

根据以上划分原则，结合集合全波形反演算法，我们提出了基于集合样本的多层级任务分解方案。在算法\ref{alg:enfwicode}中的震源编码全波形反演中（第\ref{ln:enfwisample}到\ref{ln:enfwisampleend}行），集合中的不同速度模型相互独立，各自更新。而在集合卡尔曼滤波分析步中，不同速度模型集合整体参与复杂的矩阵运算。矩阵运算中，每一列是单独速度模型，不同列代表不同速度模型。因此我们对不同的集合样本进行划分，分配到超算的不同计算节点。

此时每个计算节点需要处理完成的二维或者三维区域。在一次典型的石油勘探中，勘探的区域通常为$100km\times100km\times60km$，空间分辨率为$20m$，在不考虑边界的情况下，计算网格的格点数为$7.5\times10^9$。石油物探应用通常使用单精度浮点数表示每个格点，因此每个表示完成勘探区域的数组的大小约为$3\times10^10$字节，即30TB。这远远超出了单个节点内存上限。因此，我们需要对勘探区域进行进一步区域分解。图\ref{fig:2ddecomposition}表示我们进行二维区域划分的方法，该方法可以扩展到三维MPI区域划分，分别适用于解决二维和三维的集合全波形反演。

\begin{figure}[th]
  \centering
  \includegraphics[width=1.0\columnwidth]{2ddecomposition.pdf}
  \caption{二维MPI区域划分与一维从核计算划分示例。}
  \label{fig:2ddecomposition}
\end{figure}

如图\ref{fig:2ddecomposition}所示，我们将完整的地震波传播波场按照不同颜色划分成四块，其中白色表示边界，也可表示Halo区域。每一个子波场派发给不同的计算核组（在使用神威超算大内存的情况下，也可以使用整个CPU替代四个核组），四个核组独立更新子波场，并在需要时更新Halo区域。在单核组处理中，我们调用64个从核加速运算。64个从核也可以按照不同的排列方式参与运算，不同的排列方式将影响数据从主存传输到从核LDM的开销以及不同从核间寄存器通信的开销。此处我们使用一维从核排列方式，可以以较简单的从核实现方案达到较高的效率。从核一维排列的详细的推导过程将在后续章节介绍。

% subsection 多层级任务划分 (end)

\subsection{计算与通信重叠} % (fold)
\label{sub:计算与通信重叠}

图\ref{fig:2ddecomposition}所示的区域划分中，每个MPI进程需要在每个迭代步结束之后统一执行Halo区域的更新，这涉及到同步的MPI操作，会导致不同的进程在不同的迭代步交叉等待。为了进一步提升整体性能，我们为每个MPI进程设计了Halo区域更新异步通信，如图\ref{fig:comp-comm-overlap}所示。

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{comp-comm-overlap.pdf}
        \caption{计算与通信重叠MPI划分示例。}
        \label{fig:comp-comm-overlap}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2in]{overlappipe.pdf}
        \caption{计算与通信重叠运算时间示例。}
        \label{fig:overlappipe}
    \end{subfigure}
\end{figure*}

与图\ref{fig:2ddecomposition}相比，图\ref{fig:comp-comm-overlap}中每个核组处理的区域中包含内部主要计算部分、Halo区域和Halo交换缓冲区。按照以下步骤完成Halo区域交换的计算和通信重叠过程(如图\ref{fig:comp-comm-impl}所示）。

\begin{itemize}
  \item 每个MPI进程调用异步MPI通信接受接口($MPI\_Irecv$)，接收发送给当前MPI进程的Halo数据，并将其存储在交换缓冲区中（绿色）。由于调用的是异步接口，主线程立即返回执行后续计算。
  \item 每个MPI核组的从核对内部区域（红色和蓝色）进行有限差分计算，更新波场。于此同时，主核可以承担少量的外部区域（蓝色）区域更新运算。主核和从核的计算比例是可调节的，在内部计算较小的情况下，主核甚至可以闲置不参与运算，以避免负载不均衡问题。
  \item 当一个核组的Halo区域（蓝色）更新完成之后，使用异步MPI接口（$MPI\_Isend$）把Halo区域发送给当前进程邻接的四个（二维划分）或六个（三维划分）进程，待发送的Halo区域将被复制到MPI缓冲区中独立发送，此时该进程无需等待Halo区域发送完毕而继续执行后续计算。
  \item 当需要更新下一个波场（别的变量）或者下一个迭代步时，调用MPI同步接口($MPI\_Waitall$)，保证上述的发送和接收数据达到指定的目的地。值得注意的是，在进程发出异步发送Halo区域数据时，MPI的通信和后续的计算并行执行，因此在调用$MPI\_Waitall$接口时，Halo区域数据已部分到达或者完全到达，取决于通信和计算的开销比例（如图\ref{fig:overlappipe}所示）。
  \item 将交换数据缓冲区中的Halo数据复制到实际Halo区域，并进入下一个迭代步。
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\columnwidth]{comp-comm-impl.pdf}
  \caption{计算与通信重叠实现步骤。}
  \label{fig:comp-comm-impl}
\end{figure}

这种区域划分和及通信方式能够获得很好的可扩展性。首先，每个MPI进程分配到的区域大小是相同的，需要进行通信的Halo区域数据大小也完全相同，不会出现负载不均衡问题。其次，完整区域的更新由每个小区域的更新构成，无需进行全局通信操作。不管应用扩展到多大的规模，每个MPI进程只需要发送和接收与其邻接的四个或者六个进程的数据，即便出现不同进程运行速度不同，相距较远的进程间由于不需要数据通信不会相互等待。进程间运行速度不同只会影响邻接进程。由于连接进程只有4-6个，因此影响很小。



% subsection 计算与通信重叠 (end)



% section 基于集合样本的多层级并行任务分解 (end)

\section{面向stencil运算的LDM高效数据复用} % (fold)
\label{sec:面向stencil运算的ldm高效数据复用}

\subsection{二维划分下最小DMA数据传输推导} % (fold)
\label{ sub:二维划分下最小DMA数据传输推导}

申威26010芯片高效的从核计算需要从LDM中读取数据而非从共享主存中获取，但每个从核仅有64KB高速缓存LDM，在涉及大量数组的kernel中，每个数组能够分配的LDM缓存非常有限。此外，对于stencil这类访存受限问题，从核优化的另外一个重点在于缩短数据从主存DMA传输到LDM的时间。缩短DMA传输的时间有两个方法，减少数据传输总量和提升DMA传输带宽。

在stencil的计算中，每更新一个中心点需要访问周围的其他格点。LDM空间有限，无法储存整个二维或者三维数组（波场），因此需要对波场进行划分。划分后的每个子波场包含中心波场以及边界出所需额外访问的Halo区域。不同的划分会对应不同的Halo区域，从而影响DMA数据传输总量。本小节推导出在stencil运算二维划分下最小的DMA数据传输总量。

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\columnwidth]{单从核2D划分最小DMA传输.pdf}
  \caption{单从核2D划分最小DMA传输。}
  \label{fig:cpe-2d-derive}
\end{figure}

如图\ref{fig:cpe-2d-derive}(a)所示，设数组的大小为$NZ\times NX$，其中Z轴是数组的快轴。Halo区域的宽度为$H$，每个数组在LDM所占的空间为$S=(wx+2H)\cdot(wz+2H)$，计算所需的格点都存储在这片空间中，我们称之为LDM窗口。窗口可以沿Z轴或者X轴滑动（如图\ref{fig:cpe-2d-derive}(b)(c)所示）：
\begin{itemize}
  \item 沿着X轴滑动时，原窗口最右列的Halo区域能够被新窗口完全复用，新窗口只需读取右边四列数据。
  \item 沿着Z轴滑动时，原窗口最下行的Halo区域能够被新窗口完全复用，新窗口只需读取下面四行数据。
\end{itemize}

虽然两种滑动方式的DMA传输数据量相同，但效率却有差异。但由于数组的快轴是Z轴，沿着X轴滑动时，新窗口需调用4次DMA，每次DMA的大小为5个元素；而沿着Z轴滑动时，新窗口需调用5次DMA，每次DMA的大小为4个元素。由表\ref{tb:sw-bw}可知，DMA传输的连续数据块越大能获得更大的带宽，因此窗口沿着X轴从左向右滑动更有利于提高DMA传输效率。

公式\ref{eq:单从核二维划分最小DMA传输}描述二维划分下最小的DMA数据传输总量
\begin{equation}
  \min_{wx,wz} D(wx,wz) = (wz+2H)\cdot(NX+2H)\cdot\frac{NZ}{wz}，
  \label{eq:单从核二维划分最小DMA传输}
\end{equation}
其中$(wz+2H)\cdot(NX+2H)$表示每次沿着X轴滑动所需传输的数据总量，$\frac{NZ}{wz}$表示再次从左至右滑动的次数（轮数）。公式\ref{eq:单从核二维划分最小DMA传输}化简后可得公式\ref{eq:单从核二维划分最小DMA传输2}。
\begin{equation}
  \min_{wz} D_{1cpe}(wz) = (1+\frac{2H}{wz})\cdot(NX+2H)\cdot NZ，
  \label{eq:单从核二维划分最小DMA传输2}
\end{equation}

我们可以看到，$wz$与$D_{1cpe}(wz)$呈反相关，当$wz$越大时，$\frac{NZ}{wz}$越小，即再次从左至右滑动的次数越少，由于每次从左至右滑动时都需要额外读取上下边界的Halo区域，因此轮数越少，传输总量$D_{1cpe}(wz)$越小。

公式\ref{eq:单从核二维划分最小DMA传输2}描述的是一个从核进行DMA传输的最优情况。一个核组中有64个从核，64个从核的不同排列和划分方案也会对DMA的数据传输总量造成影响。现推导单核组64从核共同进行计算和DMA传输时，最小的传输总量。

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\columnwidth]{多从核2D划分最小DMA传输.pdf}
  \caption{多从核2D划分最小DMA传输。}
  \label{fig:multi-cpe-2d-derive}
\end{figure}

如图\ref{fig:multi-cpe-2d-derive}所示，设每个MPI进程需要更新的网格为$NZ\times NX$，每个核组的从核按照$mz \times mx$进行划分。每个从核需要处理的网格需要进一步划分才能放进64KB的LDM中，因此对单从核内部进一步按照$pz\times px$划分，划分后的每一个小块能加载到LDM中，LDM窗口的大小为$(wz+2H)\times(wx+2H)$。我们需要推导出当$mz$和$mx$如何划分时，能够使得多从核划分下的DMA传输数据量最小。此时，每个从核LDM最小DMA数据传输的优化函数变为
\begin{equation}
  \min_{mx,mz,px,pz} D_{64cpe}(mx,mz,px,pz) = (\frac{NZ}{mz\cdot pz}+2H)\cdot(\frac{NX}{mx}+2H)\cdot pz，
  \label{eq:多从核二维划分最小DMA传输}
\end{equation}
化简可得
\begin{equation}
  \min_{mx,mz,pz} D_{64cpe}(mx,mz,pz) = (\frac{NZ}{mz}+2H\cdot pz)\cdot(\frac{NX}{mx}+2H),
  \label{eq:多从核二维划分最小DMA传2}
\end{equation}
且当64个从核都进行参与时，满足以下条件
\begin{equation*}
    \left\{\begin{matrix}
mx\times mz=64\\
1\le mz \le 64 \\
pz \ge 1 \\
1 \le \frac{NZ}{mz\cdot pz} \le Z_{max}
\end{matrix}\right.
\Leftrightarrow
  \left\{\begin{matrix}
mx\times mz=64\\
1\le mz \le 64 \\
1 \le pz \\
\frac{NZ}{mz\cdot Z_{max}} \le pz \le \frac{NZ}{mz}
\end{matrix}\right.。
\end{equation*}
其中，$Z_{max}$为每个从核LDM中最多可用来分配给Z轴的空间。将$mx=61/mz$代入到公式\ref{eq:多从核二维划分最小DMA传2}中，公式\ref{eq:多从核二维划分最小DMA传2}可得
\begin{equation}
  \min_{mz,pz} D_{64cpe}(mz,pz) = \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot mz \cdot pz}{64} \cdot 2H + pz\cdot 4H^2，
  \label{eq:多从核二维划分最小DMA传3}
\end{equation}

（1）若$pz \ge \frac{NZ}{mz\cdot Z_{max}} \ge 1$，代入公式\ref{eq:多从核二维划分最小DMA传3}可得
\begin{equation}
  \min_{mz} D_{64cpe}(mz) \ge \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot NZ}{64\cdot Z_{max}} \cdot 2H + \frac{NZ}{mz\cdot Z_{max}}\cdot 4H^2，
  \label{eq:多从核二维划分最小DMA传4}
\end{equation}
则$D_{64cpe}(mz) $与$mz$呈反相关，当$mz$越大，$D_{64cpe}(mz) $越小。当$mz=64, mx=1, pz = \frac{NZ}{64\cdot Z_{max}}$时，$\min_{mz} D_{64cpe}(mz=64)$最小。

（2）若$\frac{NZ}{mz\cdot Z_{max}} < 1$，则$pz\ge 1$，代入公式\ref{eq:多从核二维划分最小DMA传3}可得
\begin{equation}
\begin{aligned}
  \min_{mz,pz} D_{64cpe}(mz,pz) &\ge \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot mz}{64} \cdot 2H + 4H^2 \\
  &\ge \frac{NX\cdot NZ}{64}+2H\cdot2\sqrt{\frac{NZ}{mz}\cdot\frac{NX\cdot mz}{64} } + 4H^2 \\
  &=\frac{NX\cdot NZ}{64}+\frac{H}{2}\sqrt{NX\cdot NZ}+ 4H^2
  ，
\end{aligned}
  \label{eq:多从核二维划分最小DMA传5}
\end{equation}
当且仅当$\frac{NZ}{mz}=\frac{NX\cdot mz}{64}$时，等号成立。此时$mz=8\sqrt{\frac{NZ}{NX}}$，$mx=8\sqrt{\frac{NX}{NZ}}$。并可推出$\frac{mx}{mz}=\frac{NX}{NZ}$。此时$pz=1$。

通过总结公式\ref{eq:多从核二维划分最小DMA传4}，我们得知在充分利用所有从核进行二维划分的情况下，满足以下条件可使DMA的数据传输总量最小：
\begin{itemize}
  \item 当$pz \ge \frac{NZ}{mz\cdot Z_{max}} \ge 1$时，将64个从核按照$64\times 1$进行排布，可获得最小DMA数据传输总量；
  \item 当$\frac{NZ}{mz\cdot Z_{max}} < 1$时，从核按照$\frac{mx}{mz}=\frac{NX}{NZ}，pz=1$进行排布，可获得最小DMA数据传输总量。
\end{itemize}

% subsection 二维划分下最小DMA数据传输推导 (end)

\subsection{三维划分下最小DMA数据传输推导} % (fold)
\label{sub:三维划分下最小DMA数据传输推导}

% subsection 三维划分下最小DMA数据传输推导(end)


\subsection{寄存器通信}

申威26010处理器与其他主流处理器相比，最与众不同的一个功能是每个核组中的64个CPE中的寄存器通信机制。由64个CPE组成的计算核心网格中，我们有8个列通信总线和8个行通信总线，这些总线成为了CPE之间进行快速寄存器通信的通道，为不同的CPE提供了重要的数据共享能力。

寄存器级通信是通过\emph{行、列通信总线}的一对\emph{Put}和\emph{Get}API接口来实现的。发送方CPE使用\emph {Put}操作将256位寄存器数据发送到接收方CPE的\emph{传输缓冲区}，而接收方CPE使用\emph{Get}操作将256位数据从\emph{传输缓冲区}传输到本地通用寄存器中。

这种寄存器通信功能为stencil计算中的数据重用提供了完美的解决方案。对于我们在第\ref{sec:parallel}节
中描述的并行化方案中，我们需要使用DMA操作将中心数据和Halo区域的数据加载到LDM中。在寄存器通信功能的支持下，大多数CPE线程只需要加载中心区域，并通过寄存器通信操作从相邻线程获取Halo区域。如图~\ref{fig:64by1-reg}所示，只有第一个和最后一个CPE线程仍然需要初始化Halo一侧的DMA加载，其他Halo区域的DMA开销则被替换为更有效的寄存器通信方案。


\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{awp_using_register.png}
\caption{同一核组内的64个CPE使用寄存器通信。}
\label{fig:64by1-reg}
\end{figure}

使用寄存器通信方案的另一个好处是，当从寄存器中读取Halo区域时，LDM中用以存储Halo区域的空间变小。换句话说，在给定相同大小的LDM的情况下，我们可以支持更大的$ W_x $，$ W_y $和$ W_z $配置，这可以进一步提高内存带宽利用率。

\subsection{从核ID重新映射}

从核之前的寄存器通信也有局限。由于用于通信的总线只能连接同一行或者同一列的从核，只有在同一行或者同一列的CPE才能够进行寄存器通信。如果两个CPE分布在不同的行和不同的列中，则需要多个寄存器通信来实现数据交换。

正如在第\ref{sec:parallel}节中介绍的，在大多数情况下，我们采用$ 64 \times1 $的CPE线程配置。在这样的配置中，需要交换数据的多数的CPE线程将不在同一行或列中。为了解决这个问题，并尽可能减少所需的寄存器通信操作次数，我们设计了一个特定的CPE ID重新映射方案。

如图~\ref{fig:id-remapping}所示，对于奇数行，我们保持重映射的逻辑 CPE ID（括号内的ID）与原始硬件CPE ID相同（ID外部的ID括号）;对于偶数行，我们则保持重新映射的逻辑CPE ID与原始硬件CPE ID相反。

通过这种从核重新映射方案，我们能够保证需要进行数据交换的相邻线程位于同一物理行或物理列中。以左侧Halo的数据交换为例，每个CPE线程需要发送相应的Halo数据到其下一个CPE线程。对于以蓝色标记的CPE线程，发送者和接收者总是在同一行中。对于标记为红色的CPE线程，发送者和接收者则总是在同一列中。

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\columnwidth]{awp_register_remap.png}
\caption{
从核ID重新映射方案。它确保所有相邻CPE线程间的数据交换操作。 括号外的ID是原始硬件从核ID，而括号内的ID是重新映射的逻辑从核ID。}
\label{fig:id-remapping}
\end{figure}



% section 面向stencil运算的ldm高效数据复用 (end)

\section{随机边界条件与合成地震记录压缩} % (fold)
\label{sec:随机边界条件与合成地震记录压缩}

地震波震源波场的存储或地震波边界波场的储存是地震波伴随时进行相关（correlation）操作必不可少的的条件。当运行大规模集合全波形反演时，由于神威超算没有本地盘，地震波场的存储成为了程序整体效率的瓶颈之一。本小节将应用随机边界条件替代地震波场的存储，提升程序运行的整体效率。

在震源波场和伴随波场的相关操作中，由于声波方程的可逆性，只有震源波场和伴随波场中相互耦合的数据会对相关的结果（成像）造成影响。随机边界的原理与吸收边界不同，它并不会在边界处吸收波场的能量，而是在边界处添加随机速度层。当地震波到达边界时发生随机散射，以此来降低伴随波场的耦合性，减少人工边界造成的成像影响。随机边界处的散射效果越好，震源波场和伴随波场的相关性就越低。

算法\ref{alg:randomboundary}是随机边界算法的伪代码。边界的处理在遍历完成波场的内部，往往发生在边界内波场更新完毕之后。第\ref{ln:randbegin}到第\ref{ln:randend}行表示不断寻找满足稳定性条件的随机散射点，然后替换已有的边界速度，随后进入下一个边界点处理。

\begin{algorithm}[ht]
%\scriptsize
%\footnotesize
\small
\caption{随机边界算法伪代码}\label{alg:randomboundary}
\begin{algorithmic}[1]
\State \textbf{for} ( ix = 0; ix < nx; ix++ )
\State \quad\quad \textbf{for} ( iy = 0; iy < ny; iy++ )
\State \quad\quad\quad\quad \textbf{for} ( iz = 0; iz < nz; iz++ )\{
\State \quad\quad\quad\quad\quad\quad \textbf{// 其他计算处理}
\State \quad\quad\quad\quad\quad\quad \textbf{if} (is\_boundary(ix, iy, iz)) \{ \textbf{// 边界情况处理}
\State \quad\quad\quad\quad\quad\quad\quad\quad \textbf{while} (true) \{ \label{ln:randbegin}
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad r = rand();
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad vv = v(ix, iy, iz) + r * d; \textbf{// 构造散射条件，d为边界厚度}
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \textbf{if} (is\_stable(vv)) \{
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad v(ix, iy, iz) = vv; \textbf{// 散射点替换已有速度}
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad break;
\State \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \}
\State \quad\quad\quad\quad\quad\quad\quad\quad \} \label{ln:randend}
\State \quad\quad\quad\quad\quad\quad \}
\State \quad\quad\quad\quad \}
\end{algorithmic}
\end{algorithm}

另一个耗时较大的模块是合成地震记录的文件输出。精确模型的合成地震记录场作为观测合成地震记录用于全波形反演或逆时偏移算法，因此这部分输入无法省略。合成地震记录输出具备以下特点：
\begin{itemize}
  \item 输出体量大：对于中等规模的地震模拟，合成地震记录文件的大小约为10GB到1T；
  \item 每次输出等量数据：程序中并不存储所有合成地震记录（内存不足），往往输出每一个接收器或每个平面的所有时间步收集的数据；
  \item 大量数据为0：接收器距离震源的距离不一，在震源的直达波到达接收器前，接收器接收不到地震波直达或反射信号。
\end{itemize}
综合以上特点，我们使用LZ4算法对合成地震记录进行压缩，有效地减小了合成地震记录的文件规模。

% section 随机边界条件与合成地震记录压缩 (end)

\section{其他优化方法} % (fold)
\label{sec:其他优化方法}
Athread
openacc

\subsection{内存对齐}
可以让炳炜写

% section 其他优化方法 (end)

\section{实验结果与分析} % (fold)
\label{sec:实验结果与分析}

\subsection{基于神威超算的并行实现}

采用上文介绍的多层级划分方案、最小DMA数据传输总量推导、寄存器通信等等优化策略，我们在神威太湖之光上实现了二维和三维的集合全波形反演应用的主核和从核版本。并基于集合样本和波场区域分解将应用扩展到了8192个进程，充分地利用了太湖之光的大规模计算能力。

神威超算的主核的主频低、逻辑与计算处理能力较弱，从核又无法为主核分担，无法执行通信、IO等操作，性能的瓶颈集中在主核中。因此，本研究尽可能将所有函数移植到从核中，只让主核处理必要的调度、通信、IO等操作。

本文采用了OpenACC和Athread两种并行实现方式，针对不同的函数采取最优的实现。OpenACC具有编程易用性强、开发时间短、可维护性高等优势，集合全波形反演算法中大量的基本向量/矩阵运算如向量/矩阵加法、矩阵/乘法等简单的函数采用了OpenACC并行方法实现。而计算行为较为复杂的函数都采用控制粒度更细的Athread并行方法实现。按照分配线程、DMA数据读取、LDM中数据计算，DMA数据写出流程进行编程实现和性能优化。

在单节点性能取得极致之后我们开始进行多节点并行扩展。本研究中单个样本的速度模型的最大网格大小为$920\times 300 \times 500$，我们按照$8\times 2 \times 4$进行划分以获得接近立方体的子网格，每个子网格的大小为$115\times150\times125$。每个子网格分配给一个MPI进程。集合的样本数目为128个，总MPI进程数到到8192。

由于本实验反演的是中小规模的地质模型，因此并未将神威太湖之光的所有计算资源充分利用。但本研究为神威太湖之光定制的集合全波形反演应用是面向大规模三维反演成像设计的，因此充分具备反演大规模模型的能力。

\subsection{性能优化结果}

本小节展示在神威太湖之光超算平台上针对集合全波形反演应用进行性能优化的结果。图\ref{fig:集合全波形反演算法核心函数从核优化加速比}表示集合全波形反演算法核心函数从核优化加速比。\emph{Intel单核}表示该函数在Intel Xeon E5-2697v2（主频为2.7GHz）机器的性能。为了进行更加公平的对比，Intel平台的结果也是经过深度优化的，包括向量化、缓存分化快（cache blocking）、循环展开等等。\emph{主核}表示神威主核的性能，这是作为比较的基准。\emph{64从核}表示使用64个从核进行深度优化后的性能。

从图\ref{fig:集合全波形反演算法核心函数从核优化加速比}可知，Intel单核性能普遍为神威主核性能的4倍，其中重要的原因是Intel的主频较高。经过64个从核优化之后，不同函数的加速比分布在$6.5\times$至$32.3\times$。性能最好的$fd2t10s$函数取得了32.3倍的加速。这个函数是空间10阶、时间2阶的有限差分运算的核心函数，同时也是$fwd\_prop$、$back\_prop$、$cal\_steplen$等函数的核心子函数。因此，在深度优化$fd2t10s$之后，$fwd\_prop$、$back\_prop$、$cal\_steplen$等函数也相应获得了21至28倍的性能提升。

使用64个从核加速仅获得不足30倍的加速比，效率不足50\%，这似乎不是令人激动的数字。然而当我们对有限差分算法进行深入分析之后发现，空间为10阶的中心stencil运算是严重的访存受限问题，性能的提升取决于数据的复用以及对访存带宽的利用率。经过我们的细致优化（包括多层级数据划分、数组融合、寄存器通信，从核ID重映射、内存字节对齐等等），$fd2t10s$函数的DMAD带宽效率达到了26GB/s，达到了神威单核组的访存峰值带宽的74\%，在有限的LDM中，这几乎达到了有效访存带宽的极致。

$pack$函数是进行MPI通信前，将波场边界的不连续数据拷贝到连续的数组中以便组成大块的连续消息，$unpack$函数则是对应的逆向过程。这两个函数的核心是进行内存复制，主要的瓶颈是内存带宽，因此优化这两个函数仅获得7倍左右的性能提升。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{enfwi不同函数优化加速比-crop.pdf}
\caption{集合全波形反演算法核心函数从核优化加速比。}
\label{fig:集合全波形反演算法核心函数从核优化加速比}
\end{figure}



\subsection{数值实验模拟}
本小节使用数值实验模拟，比较传统全波形反演算法、震源编码全波形反演方法和集合全波形反演算法在叫不精确的初始速度模型以及有噪音的数据下的收敛情况。验证集合全波形反演方法有较大的收敛域和较小的数据噪音敏感度。

本算例中，我们对Marmousi模型进行反演，Marmousi模型是典型的二维全波形反演模型，为了使用该模型进行三维集合全波形反演，我们人为地将二维平面扩展成三维，扩展后的模型的大小为$9.2km\times 3.0km \times 5km$，空间步长为$10m$，网格的大小为$920\times 300 \times 500$。我们将震源放置在深度为$10m$的平面，在平面上每个$100m$放置一个震源，震源函数为
\begin{equation}
  f(t)=\sin(2\pi f_0t)\cdot e^{-4\pi^2f_0^2t^2/16}
\end{equation}
其中主频$f_0$为10Hz，$t$为时间。地震记录接收器（检波器）同样铺放在深度为$10m$的平面，且每个$10m$放置一个检波器，检波器铺满整个平面网格。正演的时长为$3s$。伪三维Marmousi模型正演反演数值模拟参数如表\ref{tb:伪三维Marmousi模型正演反演数值模拟参数}所示。

\begin{table}[ht]
\centering
\caption{伪三维Marmousi模型正演反演数值模拟参数}
\label{tb:伪三维Marmousi模型正演反演数值模拟参数}
\begin{tabular}{cccccc}
\hline
参数 & 反演范围($km^3$) & 分辨率 & 网格大小        & 震源频率 & 正演时长 \\\hline 
配置 & 9.2*3.0*5     & 10m   & 920*300*500   & 10Hz    & 3s  \\\hline
\end{tabular}
\end{table}

Marmousi的精确速度模型如图\ref{fig:Marmousi精确模型}所示。我们使用精确速度模型进行正演得到实际观测地震记录，然后我们在精确速度模型的慢度域进行模糊化处理，平滑的窗口的大小为为$1.8km\times 0.5km$，平滑后的模型如图\ref{fig:Marmousi模型慢域平滑后的初始模型}所示。平滑后的模型（即初始速度模型）和实际观测地震记录作为集合全波形反演的输入，目标是经过300次迭代步之后，将初速速度模型反演逼近得到如图\ref{fig:Marmousi精确模型}所示的精确速度模型。

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{marmvel.pdf}
        \caption{Marmousi精确模型。}
        \label{fig:Marmousi精确模型}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{marmvel_smoothed.pdf}
        \caption{Marmousi模型慢域平滑后的初始模型。}
        \label{fig:Marmousi模型慢域平滑后的初始模型}
    \end{subfigure}
    \caption{Marmousi精确模型和平滑后的初始模型}
    \label{fig:Marmousi精确模型和平滑后的初始模型}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{fwi.pdf}
        \caption{无噪音全波形反演结果。}
        \label{fig:无噪音全波形反演结果}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{fwi-noise.pdf}
        \caption{有噪音全波形反演结果。}
        \label{fig:有噪音全波形反演结果}
    \end{subfigure}
\end{figure}

\begin{figure}[ht]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{esfwi.pdf}
        \caption{无噪音震源编码全波形反演结果。}
        \label{fig:无噪音震源编码全波形反演结果}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{esfwi-noise.pdf}
        \caption{有噪音震源编码全波形反演结果。}
        \label{fig:有噪音震源编码全波形反演结果}
    \end{subfigure}
\end{figure}

\begin{figure}[ht]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{enfwi.pdf}
        \caption{无噪音集合全波形反演结果。}
        \label{fig:无噪音集合全波形反演结果}
    \end{subfigure}%
    ~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=2.1in]{enfwi-noise.pdf}
        \caption{有噪音集合全波形反演结果。}
        \label{fig:有噪音集合全波形反演结果}
    \end{subfigure}
    \caption{无噪音和有噪音情况下传统全波形反演、震源编码全波形反演、集合全波形反演的成像结果比较。}
\end{figure}

图\ref{fig:无噪音全波形反演结果}、\ref{fig:无噪音震源编码全波形反演结果}、\ref{fig:无噪音集合全波形反演结果}分别描述了无噪音情况下全波形反演、震源编码全波形反演、集合全波形反演算法的成像结果。我们可以看到，在初始模型较差的情况下，传统全波形反演算法只能勾画出大致的速度层状结构，但缺乏高频信息；震源编码全波形反演结果能够反演出较好的层状结构，尤其是在低速区，反演的结果几乎与真实模型相同，但也陷入了局部最优（如图\ref{fig:无噪音震源编码全波形反演结果}右上角红点所示）。集合全波形反演算法克服了全波形反演和震源编码全波形反演算法的不足，清晰的反演出最接近真实的速度模型。在高速区域的反演效果明显好于前两种算法。此外，由于全波形反演算法最终对所有样本基本进行均值化处理，最终反演的速度模型有明显的平滑效果，消除了局部最优的假象。

我们在观测地震记录中加入了幅值为全局地震记录最大值的5\%的白噪音。在较差的初始速度模型和数据噪音的双重影响下，图\ref{fig:有噪音全波形反演结果}、\ref{fig:有噪音震源编码全波形反演结果}、\ref{fig:有噪音集合全波形反演结果}分别描述了无噪音情况下全波形反演、震源编码全波形反演、集合全波形反演算法的成像结果。我们可以看到，在有噪音情况下，三种方法的成像效果都不理想。传统的全波形反演算法几乎无法勾勒出Marmousi模型的大致结构，只有少量的有效信息。震源编码全波形反演算法对数据噪音最敏感，出现了大量的人工假象。而集合全波形反演较有效地克服了数据噪音，在低速区反演出精确的模型，在高速区也较好的对抗了噪音造成的影响。

由此可见，集合全波形算法与传统全波形反演、震源编码全波形反演具有更大的收敛域，且对数据噪音更不敏感。在处理的地震成像中具有更大的应用潜力。

% section 实验结果与分析 (end)

\section{本章小结} % (fold)
\label{sec:本章小结}

本章主要介绍了面向神威太湖之光超算系统的人工地震成像算法——集合全波形反演方法。本章首先提出了集合全波形反演方法，该方法在可接受的计算效率下，比传统全波形反演方法和基于震源编码的全波形反演方法具有更大的收敛域和更低的噪音敏感度。然后本章对集合全波形整体算法以及核心的集合卡尔曼滤波分析步和地震波波场更新等模块进行计算分析，并针对神威太湖之光超算系统提出设计与优化思路。紧接着，本章详细阐述了集合全波形反演方法在神威超算系统中的多级优化方案，包括基于集合样本的多层级并行任务分解、面向stencil运算的LDM高效数据复用、随机边界条件以及其他优化方法。随后，本章综合运用上述方法实现并优化了面向神威超算系统并行集合全波形反演算法，充分发挥了神威超算系统的各项资源优势，取得了令人振奋的性能结果。最后，本章在Marmousi模型上进行了数值实验模拟，将集合全波形反演算法与传统全波形反演方法和基于震源编码的全波形反演方法进行比较，验证了集合全波形反演方法具有更大的收敛域和更低的噪音敏感度。

% section 本章小结 (end)


%\include{data/chap01}
%\include{data/chap02}


%%% 其它部分
\backmatter

%% 本科生要这几个索引，研究生不要。选择性留下。
% 插图索引
\listoffigures
% 表格索引
\listoftables
% 公式索引
\listofequations


%% 参考文献
% 注意：至少需要引用一篇参考文献，否则下面两行可能引起编译错误。
% 如果不需要参考文献，请将下面两行删除或注释掉。
% 数字式引用
\bibliographystyle{thuthesis-numeric}
% 作者-年份式引用
% \bibliographystyle{thuthesis-author-year}
\bibliography{ref/refs}


%% 致谢
\include{data/ack}

%% 附录
%\begin{appendix}
%\input{data/appendix01}
%\end{appendix}

%% 个人简历
\include{data/resume}

%% 本科生进行格式审查是需要下面这个表格，答辩可能不需要。选择性留下。
% 综合论文训练记录表
%\includepdf[pages=-]{scan-record.pdf}
\end{document}
