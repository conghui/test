% TODO: 
% * 打开\makecover
% * 添加数组融合的图片
% * 把SC论文中其他部分添加进来（还有不少新内容）
% 
\documentclass[degree=doctor]{thuthesis}
% 选项：
%   degree=[bachelor|master|doctor|postdoctor], % 必选
%   secret,                                     % 可选
%   pifootnote,                                 % 可选（建议打开）
%   openany|openright,                          % 可选，基本不用
%   arial,                                      % 可选，基本不用
%   arialtoc,                                   % 可选，基本不用
%   arialtitle                                  % 可选，基本不用

% 所有其它可能用到的包都统一放到这里了，可以根据自己的实际添加或者删除。
\usepackage{thuthesis}
\usepackage{bm}
\usepackage{lscape}

% 定义所有的图片文件在 figures 子目录下
\graphicspath{{figures/}}

% 可以在这里修改配置文件中的定义。导言区可以使用中文。
% \def\myname{薛瑞尼}
\begin{document}

%%% 封面部分
\frontmatter
\input{data/cover}
% 如果使用授权说明扫描页，将可选参数中指定为扫描得到的 PDF 文件名，例如：
% \makecover[scan-auth.pdf]
% TODO:
%\makecover
%% 目录
\tableofcontents

%% 符号对照表
\input{data/denotation}


%%% 正文部分
\mainmatter
\chapter{基于十亿亿次神威超算的高分辨率唐山大地震模拟}
\label{chap:earthquake}

\section{背景知识和算法概述}

在中国传统文化中，对学者的最高评价是“上知天文，下知地理”。自然科学与技术在上世纪取得了长足的进步，然而，地球内部的构造、运动模式、可预测性等问题对于科学家来说仍旧充满未知与神秘。因此，地震等对人类社会生命财产安全造成巨大损失的重大灾害则成为了等待着科学家破译的最终科学挑战之一\citep{anderson1989theory}。

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{seismoscope.png}
\caption{张衡在公元132年设计的地震仪的内部结构和外观\citep{hsiao2009review}。}
\label{fig:heng-scope}
\end{figure}

公元132年，东汉著名天文学家张衡设计的地动仪可能是中国历史上最早的抗震减灾科学工作之一\citep{stein2009introduction}。如图~\ref{fig:heng-scope}所示，地动仪有八个方位，每个方位上均有口含龙珠的龙头，在每条龙头的下方都有一只蟾蜍与其对应。任何一方如有地震发生，该方向龙口所含龙珠即落入蟾蜍口中，由此便可测出发生地震的方向\cite{seismoscopewiki}。张衡在约2000年前设计的地动仪已展现出与起源于1880年代的地震测量仪器极其相似的技术特征。

在20世纪，随着科技的发展，天然地震信号的采集和测量设备的测量精度越来越准确，这也使得基于统计的方法逐渐发展成为各种形式的地震风险分析预测的主要工具。

在最近的二三十年里，随着超级计算机越来越强大，数值模拟成为了探索科学问题不可或缺的一个工具。地震模拟工具就像一个“数字地动仪”，可以让科学家“模拟”或“预测”地震，从而提供对地震相关风险的定量评估，并提高我们对天然地震和底层结构和演变机制的理解。从工程的角度出发，这种工具还能够与其他机械模型耦合，并在不同的场景中进行相关测试，用以指导地震活跃地区抗震救灾的公用事业系统设计。

虽然数值地震模拟提供了一个具有宝贵研究功能的独特“实验平台”，但它也是超级计算社区传统的“巨大挑战”。

在地理空间上，地震模拟的范围需覆盖在水平面上数百公里，沿深度数十公里。 即使网格大小超过100米，这样的问题也会涉及数十亿到数万亿的未知数 \citep{cui2010scalable}。 在现在的地震工程计算需求中，要求支持10Hz以上的广泛的频率范围、网格大小在20米以内、时间步长在毫秒范围内  \citep{cui2013physics}。 尽管我们只关心地震几十秒的时间，但即使是使用最领先的超级计算机系统，这样的空间范围和时空分辨率也会给计算带来巨大的挑战。

此外，在每个网格中，计算和存储压力的也很高。 对于一个线性地震模拟，我们求解一个速度和应力张量方程的耦合系统：
\begin{equation}
\partial_{t}\bm{v}=\rho\nabla\cdot\bm{\sigma}
\end{equation}

\begin{equation}
  \partial_{t}  \bm{\sigma} = \lambda (\nabla    \cdot \bm{v}) \bm{I} + \mu \cdot (\nabla \bm{v}  + (\nabla \bm{v})^{T})
\end{equation}
其中$\bm{v}$=({$v_{x}$}, {$v_{y}$}, {$v_{z}$}) 是速度的三个空间分量，$\bm{\sigma}$=( $\sigma_{xx}$, $\sigma_{yy}$, $\sigma_{zz}$,$\sigma_{xy}$, $\sigma_{xz}$, $\sigma_{yz}$)是应力的六个分量。 对于频率较高的情况，岩石和土壤中的非线性的响应以及盆地内浅层沉积岩的非线性行为会成为重要考虑因素。 为了适应这些非线性效应，我们需要结合Drucker-Prager塑性 \citep {roten2016high}，得到如下产量应力方程：
\begin{equation}
Y(\sigma)={\rm max}(0, c \cos \varphi - ( \sigma_{m} + P_{f} ) \sin\varphi)
\end{equation}
其中$c$为凝聚量（cohesion）, $\varphi$为摩擦角度（friction angle）, $P_{f}$为流体压力（fluid
pressure），$\sigma_{m}$为平均压力（mean stress）。产量函数也用以判断是否更新应力：
\begin{equation}
\sigma_{ij} = \sigma_{m}^{\rm trial} \delta_{ij} + r s_{ij}^{\rm{trial}}
\end{equation}
其中，$r$ 是由产量应力$Y(\sigma)$计算所得。$s_{ij}$是应力偏导。因此，当从线性情况转变为非线性情况时，我们需要处理的3D矩阵则超过35个（在线性情况下，我们需要处理28个3D矩阵）\citep {roten2016high}，这几乎额外增加了内存容量和内存25％的带宽，这成为了目前在多核超级计算机获得高效率运算的最大挑战之一。

神威太湖之光在2016年的首次发布\citep{fu2016sunway}，标志着最领先的超级计算机的运算能力首次超过了100Pflops，运算核心首次超过1000万。而太湖之光的计算能力达到前所未有的水平，其性能相当于天河二号超级计算机的3倍，美国Titan超级计算机的5倍，然而其内存系统相对比较一般。 如图\ref{tb:supercomputer-comp}所示，神威太湖之光的总内存大小与其他系统相似（只比两个基于GPU的系统Piz Daint和Titan稍好），byte-to-flop的比例缺非常低，只有其他异构系统的五分之一，日本“京（K)"超级计算机的十分之一）。 这样的一个体系成为了将科学应用扩展到下一个层次的潜在而又严峻的挑战，特别是对于需要大内存空间和高内存带宽的地震模拟问题，打破内存的局限（内存墙）成为这个课题最大的挑战。

\begin{table}[!t]
\footnotesize
\caption{神威太湖之光超级计算机与其他领先超算系统的简要比较。$size_{MEM}$和$BW_{MEM}$分别代表内存的总量和带宽$\frac{BW_{MEM}}{PEAK}$表示计算内存带宽与系统峰值计算性能的比率。}
\label{tb:supercomputer-comp}
\center
\begin{tabular*}{0.8\columnwidth}{cccccc}
\hline\hline
   & PEAK & LINPACK & $size_{MEM}$  & $BW_{MEM}$ & $\frac{BW_{MEM}}{PEAK}$ \\
   & (Pflops) & (Pflops) & (TB) & (TB/s) & {BYTE per flop} \\
   \hline\hline
   TaihuLight & 125 & 93 & 1,310 & 4,473 & 0.038 \\\hline
   Tianhe-2 & 54.9 & 33.9 & 1,375 & 10,312 & 0.188 \\\hline
   Piz Daint & 25.3 & 19.6 & 425.6 & 4,256 & 0.168 \\\hline
   Titan & 27.1 & 17.6 & 710 & 5,475 & 0.202 \\\hline
   Sequoia & 20.1 & 17.2 & 1,572 & 4,188 & 0.208 \\\hline
   K & 11.28 & 10.51 & 1,410 & 5,640 & 0.5 \\\hline
\hline
\end{tabular*}
\end{table}

在AWP-ODC \citep{cui2010scalable}和CG-FDM\citep{zhang2014three}代码的基础上，我们提出了一个全面优化的设计和框架，细致地将算法和实现技术相结合，并设法把神威太湖之光超级计算机的各种性能指标——内存受限（memory bound）问题的内存相关功能，如DRAM空间，本地数据存储器（LDM）空间（SW26010系统用户控制器缓存），有效内存带宽——发挥到硬件上限的80％到90％。

我们还提出了一种实时压缩算法，能够在提供数据压缩的同时有效减少压缩引入的计算和内存开销。我们的方法可以使问题的最大规模增加一倍，性能进一步提高24％。在极端的仿真情况下，我们的系统能够为线性地震模拟提供14.2Pflops的持续性能，在非线性情况下提供了18.9Pflops的持续性能。虽然神威太湖之光的byte-to-float比例只有美国泰坦（Titan）超算的五分之一，但是我们所达到的计算效率高达15％，超过了以前在Titan \citep{roten2016high}上所获得的效率——11.8％。

通过充分发挥神威太湖之光的所有可能的性能，我们能够对唐山地震（M7.2，1976）进行一系列模拟，模拟的区域为水平面320km乘以320km，深度为40km，空间分辨率提高 从500米到8米，支持的最高频率高达18赫兹。就我们所知，这是第一次在这样的尺度上进行非线性塑性地震模拟，而频率和分辨率也是在同等模拟规模下达到最高。唐山地震的塑性地震动模拟首次使我们能够定量估计唐山地震灾区的危害，为华北地区建筑物的抗震工程的标准设计提供指导。

\section{当前研究现状与相关工作}

作为高性能计算的传统“巨大挑战”，与地震相关的最早的研究工作是在Cray T3D \citep {bao1996earthquake}的256个处理器上完成的。该工作使用非结构化网格模拟了区域大小为140公里乘以100公里乘以20公里，并且性能达到了8Tflops，这是最早的地震数值模拟工作之一。

从发生地震的概率出发，多数大规模的地震模拟工作都来自日本和美国湾区等地震活跃地区。例如，美国加州理工学院和日本JAMSTEC\citep {es-gb-2003}合作的地震模拟工作第一次获得了戈登贝尔奖（Gordon Bell Prize）。这个工作使用的数值方法是谱元法（一种高度有限元技术），最小计算单元是平均网格间距为2.9km的立方体，在地球模拟器上使用了1944个处理器对全球进行了地震波模拟，达到的性能为5TFlops。这项工作随后进行了扩展，开发成了名为SPECFEM3D\_GLOBE的软件包，它又随着新一代超级计算机的发展而不断演变。

2008年，SPECFEM3D\_GLOBE进一步将网格间距提高到800米左右，频率范围达到0.5赫兹左右，并在Ranger超级计算机上使用32000个计算核心达到了28.7Tflops的性能，以及在Jaguar超级计算所使用29000个计算核心达到35.7Tflops的性能。（Jaguar超算的性能之所以更好，主要是因为这是一个内存受限的问题，而Jaguar的内存带宽更高）。 到了2012年，SPECFEM3D\_GLOBE的代码进一步更新，能够支持GPU设备的高效利用，并扩展到896个CPU-GPU节点，性能达到35Tflops \citep {rietmann2012forward}。

另一种能够进行大规模地震模拟的数值方法是使用SeisSol软件中的任意高阶导数不连续Galerkin有限元方法（DG-FEM）。SeisSol在天河2号的8192个节点上实现了8.6Pflops的持续性能\citep{tianhe2-2014gb}，拥有1.91亿个四面体元素和960亿个自由度，实现了对1992年Landers M7.2地震的模拟。与其他方法相比，SeisSol中的DG-FEM将数值问题转化为计算受限的稠密矩阵运算，从而使Intel Xeon Phi加速器的使用效率更高。然而，高阶数值方法也增加了计算和内存的复杂性，这再次限制了可解问题的最大尺寸和计算时间。 SeisSol最近升级到了EDGE软件包 \citep {breuer2017edge}，包含了类似输入的融合地震模拟特性，这进一步将DG-FEM方法的性能在Cori-II超级计算机上提高到了10.4Pflops。

近期，在日本“京”超级计算机上也有相关的工作在探索隐式有限元求解器的潜力。 T. Ichimura和他的同事们提出了GAMERA \citep {ichimura2014physics}和GOJIRA \citep {ichimura2015implicit}，后者以29.7秒的时间步长解决了超过1万亿次的自由度。利用日本“京”超级计算机的全部（663,552）个CPU核心，该求解器的全部性能达到了1.97PFlops。然而，这两种实现更多地集中在城市中的建筑物（通常大小为几十公里）和地震波放大模拟的场景上，他们并不能在数百公里范围内进行大规模的地震模拟。

作为世界领先的地震研究联盟，也许也是跨学科、跨国家的世界上最大的地球科学合作，南加州地震中心（Southern California Earthquake Center，SCEC）主导了地震模拟领域的一些最重要的发展。 SCEC在2004年启动了了TeraShake项目，该项目从NSF资助的TeraGrid \citep{teragrid}的2,048个处理器开始，并升级为更大规模的4万个BlueGene处理器。 TeraShake项目发现了南圣安德烈亚斯断层的破裂指向性是一个震源效应，它可能与沉积盆地的激发（场地效应）耦合，从而大大增加了洛杉矶的地震危险。他们清楚地展示了大规模地震模拟的科学效益。该项目的另一个重要成果是开源软件AWP-ODC（Anelastic Wave Propagation by Olsen, Day and Cui），后来支持众多的地震研究项目。 2010年左右，AWP-ODC的仿真能力进一步提升到了能够在千万亿次超级计算机上进行模拟\citep{cui2010scalable}。通过使用Jaguar超级计算机的223,074个计算核心，在南加州的800公里乘以400公里区域实现了一个地震模拟，模拟空间分辨率为40米，最大频率为2赫兹，持续性能为220Tflops。 2013年，AWP-ODC扩展到支持GPU加速器。对于20,480×20,480×2,048网点的问题，在Titan \citep {cui2013physics}超级计算机上通过使用16,384个GPU可以实现2.33 Pflops的持续性能。 2016年SCEC进一步完善了支持非线性效应的模拟，这是高频地震模拟中要考虑的关键因素，使用Titan \citep {roten2016high}超级计算机的一半资源，性能达到了1.6 Pflops。

表 \ref{tb:rw-comp}总结了大规模地震模拟的现有工作。在大约二十年的时间里，随着机器从256个处理器发展到1000万个计算核心，问题从数百万个元素扩展到数十亿个元素（占用内存空间从GB量级扩展到了PB量级），地震模拟性能也从Gflops提高到15 Pflops。不同的软件包（SPECFEM3D的SEM，SeisSol和EDGE的DG-FEM，GAMERA和GOJIRA的隐式FEM，以及AWP-ODC的FD）采用不同的数值方法，因此我们也不可能用同一个衡量标准对他们进行统一比较。一般而言，基于有限元的方法具有更好的地形处理能力，可以用少量的网格模拟复杂的场景，但是对于非线性问题的收敛性可能会面临严重的效率问题。相比之下，有限差分（FD）方法由于其对计算和存储容量的高度需求而被认为是不切实际的解决方案，现在因为其计算规则，存储器访问和节点间通信的友好性被认为更适合于大规模并行计算环境。

综合考虑所有主流的软件代码，AWP-ODC在过去的几十年里累积了SCEC的在地球物理和计算机软件的工作，提供了最先进的功能，具有良好的塑性和非弹性衰减物理特性，能够在半天内完成大规模的地震模拟。因此，在我们的工作中，我们重新设计了基于神威太湖之光硬件架构的AWP-ODC，这是和过去完全不同的硬件架构。在几乎所有方面（计算、内存容量、内存带宽、IO带宽、通信带宽）将机器的性能压缩到极限之后，我们将地震模拟的性能从Titan超级计算机的2.3Pflops提高到太湖光的15.2Pflops，并支持模拟比原来更大4到5倍的问题。结合我们的新提出的压缩方案，神威太湖之光超算的地震模拟的性能将进一步提高到18.9 Pflops，并且可以支持18赫兹频率和8米分辨率的场景。总而言之，与AWP的Titan相比，我们的工作将科学家对地震的模拟能力提高了8倍，仿真性能提高了9倍，最大的问题尺寸提高了9倍到10倍。值得注意的是，尽管神威太湖之光超级计算机的byte-to-float只有其他超算的五分之一或者十分之一，但是经过我们的创新和努力，使得AWP-ODC这种内存限制的应用程序也能在太湖之光上达到上述的性能。

\begin{landscape}
\begin{table*}[p]
\caption{超级计算机大规模地震模拟现有工作总结。这些数字是从已发表的论文中获得的。未报告的值被标记为“-”。对于数值方法，FD指的是有限差分法，SEM指的是谱元法，DG-FEM指不连续的Galerkin有限元法。}
\label{tb:rw-comp}
\centering
\resizebox{1.55\textwidth}{!}{%
\begin{tabular}{ccccccccccc}
\hline\hline
  \multicolumn{2}{c}{Work} & Year & Machine & Arch & Scale & \# grid points & \# DOFs & Flops & Mem & Method\\\hline\hline
  \multicolumn{2}{c}{\citep{bao1996earthquake}} & 1996 &   Cray T3D & Alpha CPU  & 256 & 13.4 million & 40.2 million & 8 Gflops & 16 GB & FD \\
  &&&&& processors &  &&&& \\\hline\hline

  SPECFEM3D & \citep{es-gb-2003} & 2003 & Earth  & NEC SX-6 & 1,944 & 5.5 billion & 14.6 billion & 5 Tflops & 2.5 TB & SEM  \\
  &&&Simulator&& processors &&&& \\\cline{2-11}
   & \citep{carrington2008high} & 2008 & Ranger & 4-core Opteron & 32,000 cores & -- & -- & 28.7 Tflops & -- & SEM \\
  & & & Jaguar &  & 29,000 cores & & & 35.7 Tflops & & \\\cline{2-11}
  & \citep{rietmann2012forward} & 2012 & Cray XK6 & 16-core Opteron and & 896 GPUs & 8 billion & 22 billion & 135 Tflops & 3.5 T & SEM \\
  &&&& Fermi M2090 GPU &&&&&& \\\hline\hline
  SeiSol & \citep{tianhe2-2014gb} & 2014 & Tianhe-2 & 12-core Xeon and & 196,608 cores & 191 million & 96 billion & 8.6 Pflops & -- & DG-FEM \\
  &&&& 59-core Xeon Phi & 1,400,832  cores &  tetrahedrons &&&& \\\cline{2-11}
  EDGE & \citep{breuer2017edge} & 2017 & Cori-II & 68-core Xeon Phi & 612,000 cores & 341 million & -- & 10.4 Pflops & 32 TB & DG-FEM \\
  &&&&&  &  tetrahedrons &&&& \\\hline\hline
  GAMERA & \citep{ichimura2014physics} & 2014 & K Computer & 8-core SPARC64 & 663,552 cores & -- & 27 billion  & 0.804 Pflops & -- & implicit \\
  &&&&&  &  &&&& FEM \\\cline{2-11}
  GOJIRA & \citep{ichimura2015implicit} & 2015 & K Computer & 8-core SPARC64 & 663,552 cores & 270 billion & 1.08 trillion  & 1.97 Pflops & -- & implicit \\
  &&&&& &  &&&& FEM \\\hline\hline
  AWP-ODC & \citep{cui2010scalable} & 2010 & Jaguar & 6-core Istanbul & 223,074 cores & 436 billion & 1.31 trillion & 220 Tflops & 127 TB & FD \\
  &&&&&  &  &&&& linear\\\cline{2-11}
  & \citep{cui2013physics} & 2013 & Titan & 16-core Opteron and & 229,376 SMXs & 859 billion & 2.58 trillion & 2.33 Pflops & 250TB & FD  \\
  &&&&k20x GPU & 16,384 GPUs &  &&&& linear\\\cline{2-11}
  & \citep{roten2016high} & 2016 & Titan & same as above & 114,688 SMXs & 329 billion & 987 billion & 1.6 Pflops & 129TB & FD \\
 &&&&&  8,192 GPUs &  &&&& non-linear \\\hline\hline
 our work & without & 2017 & Sunway & 260-core SW26010 & 1,014,000 cores & 3.99 trillion & 11.98 trillion & 15.2 Pflops & 892 TB & FD \\
 &compression&&TaihuLight&& &  &&&& non-linear \\
 & with compression &  &  &  & & 7.8 trillion & 23.4 trillion &  18.9Pflops & 724TB & \\\hline\hline
\end{tabular}
}
\end{table*}
\end{landscape}

\section{神威太湖之光及其在大规模科学应用的挑战}
\label{sec:sunway}

\subsection{体系结构}
神威太湖之光的计算能力是由我国自主研发的多核申威26010（SW26010）CPU提供，其中包括4个核组（Core Group，CG）\citep {fu2016sunway}，每个核组包括一个主核（Management Processing Element，MPE），64个从核（Computing Processing Element，CPE）和一个内存控制器（Memory Controller, MC）。 一个申威26010处理器拥有260个处理单元，并且提供超过3 TFlops的峰值性能。 神威太湖之光系统共有40960个SW26010 CPU，提供125 Pflops的峰值性能，10644900个计算核心。 虽然太湖之光在计算部分具备优于其他系统的显着优势，但内存容量相对较小，每个节点只有32 GB的内存和136 GB/s的内存带宽。


\subsection{主要设计挑战}
\label{sec:sunway-challenge}

作为全球第一个拥有超过1000万个内核的超算系统，神威太湖之光上设计应用的第一个挑战就是推导出正确的并行化方案，将我们的目标应用程序映射到系统中的进程和线程中。 与其他异构多核加速器的超级计算机类似，我们也采用了两级的“MPI + X”方法。其中每个核组通常对应一个MPI进程。 在每个核组中，我们有两个不同的选择：一个是神威 OpenACC，这是一个专用的并行编译工具，支持OpenACC 2.0语法，能够将特点的代码块以从核集群为目标进行并行; 另一个是名为$ Athread$的高性能轻量级线程库，它提供与$ Pthread $类似的接口来完成细粒度的并行。 在我们的工作中，我们采用$ Athread $的方法，这意味着需要更多更复杂的编程，但同时也有更大的空间来调整计算和内存访问的方案。

第二个挑战是要打破这个超算系统的内存墙，特别是对于这种基于有限差分运算的地震模拟的内存限制问题，正如表 \ref {tb:supercomputer-comp}中所强调的那样，神威超算系统的byte-to-float比例比其他领先超算系统低5-10倍，在这种情况下，我们需要非同寻常的与内存相关的创新来充分发挥神威超算峰值性能为125 Pflops的仿真计算能力。

图~\ref {fig:sunway_mem}显示了CPE的内存层次结构。内存层次结构的顶层是每个CPE的32个浮点寄存器。本地寄存器只需要一个周期（cycle）来访问。使用CPE群集中的行和列通信总线，寄存器通信功能花费大约11个周期从同一行或列中的其他CPE的远程寄存器获取数据。第二个层次是64KB LDM，需要由程序员以精细的方式手动管理。第三个级别是每个CG带宽为34 GB/s的8 GB主内存，聚合到每个处理器的总共32 GB内存和136 GB/s带宽。

由于与Sunway处理器的3 Tflops计算性能相比，第三级的内存大小和带宽都相对较小，因此我们在这方面的很多努力都集中在最大程度地利用64 KB LDM、寄存器、以及第二级和顶层的寄存器通讯功能。即使在我们设计并行化方案时，我们也仔细地将计算部分与我们在不同级别上可以承受的内存占用情况进行映射。更多细节在Section \ref{sec:contribution}中给出。

\begin{figure}[h]
\centering
\includegraphics[width=.9\columnwidth]{memory_hierarchy.pdf}
\caption{CPE的内存层次结构。顶层是高效的寄存器，访问或通讯需要花费1或11个周期；中间是大小为64KB的LDM高速缓存，访问LDM需要4个周期；底层是所有从核共享的8GB主存，访问需要120+个周期，带宽为136GB/s。}
\label{fig:sunway_mem}
\end{figure}

\section{主要技术创新点}
\label{sec:contribution}

\subsection{贡献小结}


我们的主要贡献是提出并开发了基于神威太湖之光的一个软件框架，它可以同时支持动态破裂的产生（用以反演地震的震源）和地震波传播的模拟（大规模地震模拟的完整周期）。

该软件框架设法突破了神威太湖之光的存储器系统所带来的约束，并实现了高效的仿真能力。即便在byte-to-float比例只有其他领先超级计算机系统的1/5至1/10的情况下，该软件框架仍能够充分利用神威太湖之光前所未有的125Pflops计算能力以及其他各项资源。

我们的主要创新点是：

\begin{itemize}
\item 一个完整统一的软件框架，包括地震震源动态破裂发生器，地震波传播部分和其他辅助功能，例如地震划分功能、3D模型生成功能、计算重启控制功能以及并行I/O功能。

\item 针对地震波模拟定制的并行化方案，该方案可在进程级别和线程级别高效使用神威太湖之光所有的1000万个计算核心，并获得几乎线性的加速效果。

\item 精心设计的多层级内存利用方案，包括通过寄存器通信来实现片上stencil halo交换、通过建模分析，推导出最优的blocking划分配置，以及对齐的数组融合DMA（直接内存访问）访问。

\item 一套实时压缩/解压方案，将应用程序可用内存大小和带宽提升到一个全新的水平，该方案同时能够扩大神威太湖之光的最高性能以及能够处理的最大问题的大小。
\end{itemize}

\subsection{完整统一地震软件框架}


大规模地震模拟的整个工作流程是一个极其复杂的过程，由不同的组成部分组成，包括从动态破裂源生成、数值计算网格的生成到最耗时的地震波传播部分。这些不同的组件给超级计算机的各个方面（计算、内存、通信、存储和I/O）都带来了巨大的挑战。为了解决这些挑战，我们构建了一个集成了不同功能的统一软件框架，如图\ref{fig:framework}所示。

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{architecture.pdf}
\caption{基于神威太湖之光的统一地震模拟软件框架。该框架由动态破裂震源生成器、数值计算网格的生成器、地震波传播模块以及其他辅助模块组成。}
\label{fig:framework}
\end{figure}

我们软件框架中的动态破裂发生器是基于CG-FDM代码\citep{zhang2014three}，该模块具有初始化断层应力、执行摩擦定律控制以及通过波传播生成震源的功能。

为了支持震源和地震波传播的大规模模拟，我们开发了一个震源划分模块，它能够将一个单一的大型源输入映射到不同的源文件中，以用于不同的相应MPI进程。我们还提供了3D模型插值生成器，可将速度和密度模型重新映射到目标网格。

地震波传播部分源于AWP-ODC \citep {cui2010scalable}，但我们完全针对神威太湖之光的特殊体系结构进行了全新的设计，这个模块消耗了大部分计算周期并集成了大部分创新和优化。地震波传播主要功能包括：速度更新、应力更新、震源注入和塑性应力调整等。

I/O部分是大规模地震模拟的另一个重要因素。最棘手的挑战来自重启的检查点（checkpoint）。在我们的研究中发现，在16米分辨率的情况下，检查点所需的所有波场总计为108 TB，这显然超出了该系统所能提供的I/O带宽和容量。因此，我们整合了LZ4压缩以减小存储开销，以实现更平滑的运行，并采用诸如“组I/O”和“平衡I/O转发”等技术，实现了120GB/s的峰值I/O带宽，达到了该文件系统峰值带宽的92.3％。

\subsection{定制的并行化设计}
\label{sec:parallel}

在耗时最多的波传播部分，我们需要处理的计算内核（kernels）中包括了涉及读取和写入超过20个覆盖整个网格的变量数组。在这种情况下，许多先前的优化技术，如3.5D blocking等方案\citep {nguyen20103}，都由于极高的内存容量要求而变得不切实际，无法在神威超算上使用。因此，我们为这些不同的内核应用定制了区域分解方案，这为1000万个计算核心提供了足够的并行性，同时最大限度地降低了相关内存成本。

图~\ref{fig:dd}显示了我们的多层级并行划分方法，它首先将整体计算区域分解为每个MPI进程的不同进程，并进一步分解为每个CPE线程的不同区域，具体如下：

（1）MPI进程的二维分解：对于所有三维数组的存储，我们将$ z $轴（垂直方向）作为最快轴，将$ y $轴作为第二快轴，并将$ x $轴作为最慢的轴。在典型的地震模拟情景中，$ x $和$ y $（一般是几百公里）的尺寸通常比$ z $（一般是几十公里）大得多。因此，为了尽量减少不同进程之间的通信，在第一层级的划分中，我们并不是采用三维划分，而是将水平面分解为$ M_x $ $ M_y $个不同的分区，每个分区对应一个特定的MPI进程。在精心设计的MPI方案中，通过从AWP-ODC \citep{cui2010scalable}继承而来的计算与通信重叠功能，在极端情况下，我们可以将多达160,000（400×400）个MPI进程扩展到整个机器。

（2）每个核组内分解：在第二层级中，不是将所有网格点分配给CG内的不同核心，而是沿$ y $和$ z $轴添加一个blocking机制，以将合适大小的块分配给每一个核组，以便更有效地利用每个CPE的64 KB LDM，详见Section \ref{sec:mem-redesign}。核组内分家之后，每个核组将遍历这些不同的块来完成计算。

（3）Athreads的二维分解：我们进一步将第二层级所得到的计算单元划分为每个CPE，划分的方式是沿着$ y $和$ z $维度（每个线程则沿着$ x $的方向迭代），以便实现不同线程高效访问高速缓存。

（4）LDM缓存方案：对于每个CPE，我们使用DMA操作将合适大小的计算区域，包括中心部分和晕圈（halo）部分加载到LDM中，以便随后执行计算。 DMA操作被设计为异步的，以便与计算部分重叠。

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{blocking.pdf}
\caption{多层级区域分解方案：（1）MPI进程的二维分解； （2）每个核组内分解；（3）Athreads的二维分解；（4）LDM缓存方案。}
\label{fig:dd}
\end{figure}

\subsection{平衡与优化的内存设计方案}
\label{sec:mem-redesign}

为了突破内存限制，解决方案的关键则是高效利用SW26010处理器的内存层次结构。

SW26010的一个独特功能是在每个CG中的64个CPE之间能够进行寄存器通信，这为stencil-like计算中的数据重用提供了完美的解决方案。使用基于寄存器通信的Halo交换，在每个CG内部，CPE线程只需要加载其相应的中心计算区域，并且可以通过寄存器通信操作从相邻线程获取所需的Halo区域。但同一核组内的不同线程间的寄存器通信并不能完成所有Halo的交换，跨不同核组的边界通信仍然需要所对应的线程通过共享内存的方式从主存中通过DMA方式进行加载。不过，通过DMA方式进行加载的边界所占的比例很低（具体的推导请见下文），大量的边界交换通过寄存器完成了，因此显著提升了性能。

结合第\ref {sec:parallel}节中详述的并行化方案和寄存器通信方案，我们可以推导出一个分析模型来确定用于CPE分解和LDM缓冲的优化配置。

我们的第一组设计参数是$ C_z $和$ C_y $，如图~\ref{fig:dd}所示，它决定了每个CG中并行CPE线程的布局。第二组是$ W_z $，$ W_y $，$ W_x $，如图 \ref {fig:dd}所示，它决定了加载到每个CPE线程的LDM中的区域的大小。

$C_z$、$C_y$、$W_z$和$W_y$需要满足以下条件：

\begin{equation}
C_z \cdot C_y = 64
\label{eq:czy}
\end{equation}
\begin{equation}
W_z \cdot W_y \cdot W_x \cdot N_{array} \cdot N_{bytes\_per\_variable} < 64 \times 1024\\
\label{eq:wzy}
\end{equation}
其中$ N_{array} $表示当前计算核心（kernel）所需的3D数组的数量，$N_{bytes\_per\_variable}$表示我们用于每个变量的字节数（4表示单精度浮点数）。

我们设计目标是：（1）尽量减少冗余Halo区域读取所需的DMA负载数量; （2）通过使用最大的连续读取大小来最大化有效内存带宽。

为了实现第一个目标——最小化冗余DMA负载数量，我们进一步推导出额外的DMA负载数量（CG内的Halo是通过寄存器通信执行的，在此不予考虑）：

\begin{eqnarray}
N_{redundant\_DMA\_load} = 2 \times H \cdot N_y \cdot (\frac{N_z}{C_z\cdot W_z}-1) \notag \\
+ 2 \times H \cdot N_z \cdot (\frac{N_y}{C_y\cdot W_y}-1).
\label{eq:load}
\end{eqnarray}
其中$ H $是stencil Halo所需的点数，$ N_y $和$ N_z $是由CG处理的block的尺寸。

结合等式\ref{eq:czy}、\ref{eq:wzy}和\ref{eq:load}，我们可以很容易推导出，为了实现光晕的最小冗余DMA负载，我们需要确保$C_y\cdot W_y = C_z\cdot W_z $。

对于内存带宽部分，SW26010的每个CG连接到带宽为34 GB/s的DDR3接口。 但是，我们使用DMA操作读取或写入的数据块的块大小（block size）很大程度上决定了可以有效利用的带宽部分，如表 \ref{tb:sw-bw}所示。 只有当每次读写的连续字节数超过512时，我们开始看到合理的内存带宽利用率。

\begin{table}[!t]
\small
\caption{根据不同连续读写的字节数测量出来的DMA内存带宽。}
\label{tb:sw-bw}
\centering
\begin{tabular}{ccccc}
\hline\hline
  Block & \multicolumn{2}{c}{DMA Get (GB/s)} & \multicolumn{2}{c}{DMA Put (GB/s)} \\
  Size (BYTEs) & 1 CG & 4 CGs & 1 CG & 4 CGs \\\hline
  32 & 3.28 & 13.21 & 2.58 & 8.07 \\
  128 & 17.81 & 72.02 & 19.05 & 77.10 \\
  512 & 27.8 & 104.86 & 30.48 & 107.88 \\
  2048 & 31.3 & 119.2 & 34.2 & 133 \\
  \hline
\end{tabular}
\end{table}

在我们的模型中，连续DMA传输的块大小由$ W_z $确定，它表示沿着最快轴的尺寸大小。因此，我们需要保持$ W_z $轴尽可能大。结合我们模型的第一个和第二个目标，我们可以推导出一个较小的值$ C_z $是相对较优的，可以实现有效的内存访问行为。

因此，在大多数情况下，我们推导出$ C_z = 1 $和$ C_y = 64 $是CG中最合适的配置。 64个CPE线程中的每一个线程将初始化DMA加载操作，以便于之后获取对应的立方区域（$W_z \cdot W_y \cdot W_x$)）和用于stencil计算的$ yz $平面。

即使在采用上述的最佳并行化方案之后，在大多数情况下，由于我们需要访问大量的数组，在只有64KB的LDM中，能够分配给每个数组的缓存是极其有限的。对于每一个三维数组，只有做一个维度（快轴）的数据在内存中是连续的，按照第\ref{sec:parallel}节描述的并行划分方案，我们能够连续读取的块很小，使得DMA读取的效率很低。

例如，对于$ dvelcx $（速度更新）的内核，我们需要读取10个不同的数组（$ u $，$ v $，$ w $，$ xx $，$ yy $，$ zz $，$ xy $，$ xz $，$ yz $，$ d $，它们是不同方向上的速度和应力变量）。根据等式\ref {eq:wzy}，为了计算这个内核中空间四阶stencil，我们需要在LDM中加载至少5个slice，所以$ W_x $的最小值是5。对于参数$ W_y $，因为$（W_y-2H）$是加载到LDM中的有效区域，所以我们需要将$ W_y $设置为至少9（对于$ H = 2 $），以保持halo成本处于合理的水平。

因此，在10个输入数组的情况下，我们可以从公式\ref{eq:wzy}推导出：
\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 10 \cdot 4 < 64 \times 1024\\
\label{eq:wzy1}
\end{equation}
其中我们得到$ W_z $的最大值为32。在这种情况下，DMA块的大小为128字节，导致内存带宽利用率低，仅为峰值带宽的50％。

为了解决上述问题，我们分析了所有不同的相关内核（kernel），并确定了一组展示大多数内核常见行为的共位数组（co-located array）。 在$ dvelcx $和其他类似内核的例子中，我们确定了数组（$ u $，$ v $，$ w $，$ xx $，$ yy $，$ zz $，$ xy $，$ xz $和 $ yz $）是在不同内核之间展示相同内存访问模式的共位数组。 因此，我们制定策略将$ u $，$ v $和$ w $融合成一个包含三个元素的向量数组，并将$ xx $，$ yy $，$ zz $，$ xy $，$ xz $和$ yz $转换为一个包含六个元素的向量数组。

从等式\ref{eq:wzy}中融合数组后，只有3个单独的数组可以读取，我们得到：

\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 3 \cdot 4 < 64 \times 1024\\
\label{eq:wzy2}
\end{equation}
它给出的最大$ W_z $值约为108。在这种情况下，DMA块的大小为432字节，将内存带宽利用率提高到80％左右。 在$ dstrqc $内核的极端情况下，数组融合技术可以将DMA块的大小从84字节增加到512字节，从而将有效内存带宽从50.47 GB/s提高到104.82 GB/s。

在将上述所有技术（基于寄存器通信的Halo交换，由分析模型推导的最优blocking配置以及数组融合）相结合之后，我们推出了一种均衡的多层级内存使用方案，可以有效利用SW26010的存储器层次结构。

\subsection{实时压缩解压缩}

在充分发挥了神威太湖之光硬件的所有性能之后，我们的下一个创新就是即时压缩方案，它不仅使可用内存空间增加了一倍，而且在相同的物理内存带宽下也释放出更多的计算能力。

\begin{figure*}[h]
\centering

\includegraphics[width=1.0\textwidth]{compression.pdf}
\caption{实时压缩方案。}
\label{fig:compression}
\end{figure*}

如图\ref{fig:compression}所示，我们将我们的压缩方案阐述为四个主要部分。 （a）部分是一个预处理步骤，它使用较粗糙的分辨率执行完整模拟，以便生成统计数据（例如变量的最大值和最小值），以便随后在其压缩过程中使用高分辨率模拟。

（b）部分解释了我们的压缩方案的工作流程。 CPE按照以下顺序执行任务：使用DMA指令将压缩的变量加载到LDM中、解压缩、计算并将结果压缩为16位结果、并使用DMA指令将压缩结果存储回主存储器。

（c）部分描述了解压缩、计算和压缩工作流的缓存方案。类似于在图\ref{fig:dd}中所描述的方案，我们执行 $ yz $ 平面中的点的压缩。每次，我们将一个平面加载到LDM中，并在向量化（vectorization）启动的情况下执行解压缩。

考虑到地震波传播的特点，我们在工作中采用了三种不同的压缩方法，如（d）部分所示，所有这些方法都将32位浮点压缩为16位。为了通过压缩来提高计算性能，我们选择应用有损压缩而不是无损压缩方案，以便将我们引入的额外计算最小化。

方法（1）直接使用由IEEE 754标准定义的16位半精度浮点数，其中5位用于指数，10位用于尾数。单精度到半精度转换的固有支持使得压缩部分高效。但是，对于动态范围大于5位指数的变量，压缩方案会产生数值问题。相反，对于动态范围较小的变量，5位指数则变成浪费。

为了解决方法（1）中的描述的问题，我们的方法（2）根据第一部分中记录的最大动态范围确定所需的指数位宽，并使用余数位作为尾数。方法（2）确保全动态范围的覆盖，并且可以为具有小动态范围的尾数部分的变量保留更多位。唯一的缺点是计算成本相对较高。

方法（3）在精度和效率之间是最平衡的。根据第一部分的统计数据，我们将同一数组的所有值归一化到1到2之间的范围，这对应于指数值为零。因此，在归一化之后，我们可以移位这些比特以直接得到尾数部分作为压缩值，这显着地简化了压缩过程。

压缩方案本身会引入额外的计算，主要是整数运算，以及密集的内存和高速缓存访问。即使访问全都发生在LDM中，频繁的加载和存储可能会大大降低CPE的计算效率。因此，我们的第一个压缩版本只能达到不压缩的1/3性能。为了降低压缩方案的性能损失，我们进行了一系列优化：

\begin{itemize}
  \item 进一步增加DMA块的大小（由$ W_z $决定），以减少DMA 70％的负载量; 
  \item 针对仍能满足精度要求的情况，采用较为不复杂的算法；
  \item 将解压缩和计算代码紧密地（在汇编代码级别）耦合以最大化寄存器中的变量的寿命，从而减少对LDM的加载和存储；
\end{itemize}

在我们的最终设计中，我们对大多数速度和应力变量采用方法（3）。将压缩方案与我们的波传播计算内核相结合，我们可以获得超过24％的计算性能提升，这是因为我们的压缩方案可以使得在相同的物理带宽情况下处理更多的数据，换而言之，对于地震波传播这种memory bound的问题，我们的压缩方案使得需要处理的数据的总量降为原来的1/2，极大的缩短了所需的DMA访问时间。而且，在压缩比为2的情况下，我们可以将这台超级计算机上能够解决的最大问题扩大一倍。这是一种通过软件的形式突破硬件的限制的方法。


为了验证我们的压缩方案，我们在不同分辨率下对使用和不使用压缩方案产生的仿真结果进行了广泛比较。图\ref {fig:compress_valid}显示了宁河和沧州两个台站的压缩前后的地震图的对比。位于唐山地震断层附近的宁河县在地震期间承受了巨大的破坏，图中的蓝色实线是未压缩的结果，红色的虚线是压缩的结果。压缩结果和未压缩结果两者的尖锐起点彼此相似，而压缩结果的尾波由于压缩的准确性损失而不能完全匹配参考波形。尽管如此，压缩结果的大部分细节仍然​​保留得很好，这对于强大的地面运动模拟来说足够精确。此外，我们还比较了远离震中的沧州站。由于较长的时间传播和误差积累，压缩后的结果可以承受稍微更高的精度损失，但直到120秒仿真结束时，这些线路仍然可以很好地匹配。这些比较结果表明，虽然压缩在一定程度上引入了错误，但我们仍然可以使用这套实时压缩方案，不仅能够获得更好的性能，还能够模拟更大规模的地震波传播问题。

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{CompareCompress.eps}
\caption{分辨率为500米的唐山地震模拟压缩方案验证。}
\label{fig:compress_valid}
\end{figure}

\section{性能与扩展性}

\subsection{如何测量性能}

我们将程序运行100个时间步之后，记录运行100个时间步所花费的时间来计算每一个时间步所花费的平均时间。对于浮点数运算次数，我们使用两种不同的方法来测量，分别是数计算汇编代码中的所有浮点运算指令以及使用伴随神威太湖之光编译器一起提供的硬件性能监视器PERF。这两种方法都产生了类似的浮点数操作次数。本研究我们使用PERF工具来测量本程序所执行的平均浮点运算。请注意，为优化目的而添加的所有操作（例如压缩相关操作）都不计入FLOP的数量中，这能够更加公平地与其他类似工作做对比。

\subsection{计算核心优化结果}

对于速度物理量的更新，计算中心区域和Halo区域的内核$ dvelcx，dvelcy $是计算量最大的两个计算核心。对于应力物理量的更新，$ dstrqc $是计算量最大的计算核心。对于塑性部分，$ drprecprc\_calc $是整个程序中最耗时的部分。剩下的内核包括$ fstr $，$ drprecprc\_app $，MPI的预处理和后处理（$ unpack\_VY $，
$ gather\_VX $和$ unpack\_VX $），这消耗了总运行时间的1-2％。我们也对其进行了极致的优化，以便实现最高的性能。

图~\ref{fig:kernel-result}演示了当采用不同的优化方法时，这些不同计算核心的性能和带宽改进结果。我们可以观察到，几乎所有的计算核心经过优化之后所获得的加速比都在30x左右，并且DMA传输带宽达到了总带宽的70％到80％左右。唯一的例外是$ fstr $内核，由于其非常低的计算密度，只能实现4倍至5倍的加速。因此，经过了一系列的优化后，不同内核在总执行时间内的分布变化与优化前相比并无太大变化。

在不同的优化方案中，协位数组的融合起着重要作用，对于最耗时的内核来说性能提高了4倍。

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{awp_performance.pdf}
\caption{当采用了不同的优化技术时，不同计算内核（主要是地震波传播部分）的加速比和内存带宽。 `MPE'代表仅使用MPE的原始版本。 `PAR'是指应用我们特定的并行化方案并使用MPE和64个CPE进行计算的版本。 `MEM'是指采用所有与内存相关的优化的版本。 “CMPR”是指进一步应用即时压缩方案的版本。}
\label{fig:kernel-result}
\end{figure}

\subsection{弱扩展性结果}

图~\ref {fig:weak-scaling}描述了线性和非线性情况下的地震模拟程序的弱扩展性结果。 对于这两种情况，我们使用每个CG来计算大小为160×160的网格，并逐渐扩展到整个机器。 对于弱扩展性，我们看到从8000个进程到160,000个进程中，我们几乎实现了完美的线性加速。 在没有使用实时压缩以及进行非线性地震模拟的情况下，我们可以通过使用160,000个进程来实现了15.2 Pflops的持续性能；而在线性地震模拟的情况下则为10.7 Pflops。 采用实时压缩方案后，相同的存储器带宽能够处理更多的数据，这分别进一步将线性和非线性地震模拟的性能提高至14.2 Pflops和18.9 Pflops。

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{weak_scaling.pdf}
\caption{线性和非线性地震模拟的弱扩展性结果。测量的MPI的进程数从8000个扩展到160,000个，其中每个CG对应于一个MPI进程。}
\label{fig:weak-scaling}
\end{figure}

\subsection{强扩展性结果}

图~\ref {fig:strong-scaling}显示了基于三种不同网格大小的线性，非线性，含压缩以及不含压缩情况下的强扩展性测试结果。 我们可以看到，不管是线性或非线性地震模拟，含压缩或者是不含压缩，地震模拟软件都达到了类似的强扩展性结果。但随着进程数量的不断增加，性能出现了下降。性能的下降是由两个方面造成的：（1）计算与通信的比例减小；（2）外部Halo区域与每个进程内计算的网格体积比例的减小，这降低了AWP软件的计算和通信重叠的效果。


\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{strong_scaling.pdf}
\caption{线性和非线性地震模拟在三种不同的问题规模中的强扩展性结果。测量的MPI进程从8,000个扩展到16,000 个，其中每个CG上对应一个MPI进程。}
\label{fig:strong-scaling}
\end{figure}

\section{基于神威太湖之光的神威大地震模拟}


%\include{data/chap01}
%\include{data/chap02}


%%% 其它部分
\backmatter

%% 本科生要这几个索引，研究生不要。选择性留下。
% 插图索引
\listoffigures
% 表格索引
\listoftables
% 公式索引
\listofequations


%% 参考文献
% 注意：至少需要引用一篇参考文献，否则下面两行可能引起编译错误。
% 如果不需要参考文献，请将下面两行删除或注释掉。
% 数字式引用
\bibliographystyle{thuthesis-numeric}
% 作者-年份式引用
% \bibliographystyle{thuthesis-author-year}
\bibliography{ref/refs}


%% 致谢
\include{data/ack}

%% 附录
\begin{appendix}
\input{data/appendix01}
\end{appendix}

%% 个人简历
\include{data/resume}

%% 本科生进行格式审查是需要下面这个表格，答辩可能不需要。选择性留下。
% 综合论文训练记录表
\includepdf[pages=-]{scan-record.pdf}
\end{document}
