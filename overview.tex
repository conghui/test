%!TEX root = main.tex
\chapter{面向神威异构众核超算的并行优化方法概述}
\label{ch:面向神威异构众核超算的并行优化方法概述}
\section{太湖之光超算系统概述}
\subsection{片上异构众核申威处理器}

\section{主从核计算优化}

\subsection{多层级任务划分方法}

当处理大规模数值实验模拟时，单节点计算机无法满足应用软件所需的内存和计算时间需求，并行计算机系统应运而生。合理的任务划分是并行计算机高效运算的关键。并行计算处理的基本流程如下：通过把软件应用处理的大问题分解成若干个较小问题，分别派发到并行计算机的不同节点上参与运算，必要时在不同节点间通过网络进行数据交换，计算完毕后，将结果汇集到主计算节点。对于传统的超算架构而言，开发者需要考虑基于节点或进程级别的任务划分。神威超算以其独特的架构，为不同层别的任务划分提供了更大的优化空间，包括核组间划分、核组内划分、从核间划分、从核内划分等等。

无论在哪个层面进行任务/数据划分，都可遵循一些通用的划分原则：
\begin{itemize}
  \item 分配给每个计算单元的任务尽量均衡，避免出现某个计算单元执行大量任务，有些节点闲置等待，造成负载不均衡现象，影响整体运算性能。
  \item 降低计算单元间通信（数据交换）开销。将一个完整的大问题分解成若干小问题并让不同计算单元完成的过程中，难免需要引入计算单元中额外的数据交换开销。通过合适的划分方法降低通信开销或者使用异步通信策略能有效地降低计算单元间的通信开销。
\end{itemize}

第\ref{sec:parallel}节和第\ref{sub:多层级任务划分}节详细地介绍了多层级任务划分方法在唐山大地震模拟和集合全波形反演算法中的应用。

\subsection{寄存器通信与从核ID重映射}

申威26010处理器与其他主流处理器相比，最与众不同的一个功能是每个核组中有64个从核，且从核间能通过寄存器通信机制交换数据。由64个从核组成的$8\times8$从核阵列中，共有8个行通信总线和8个列通信总线，这些总线成为了从核间进行快速寄存器通信的通道，为不同的从核提供了重要的数据共享能力。

寄存器级通信是通过\emph{行、列通信总线}的一对\emph{Put}和\emph{Get}API接口来实现的。从核发送方使用\emph {Put}操作将256位寄存器数据发送到接收方的\emph{传输缓冲区}，而从核接收方使用\emph{Get}操作将256位数据从\emph{传输缓冲区}传输到本地通用寄存器中。使用寄存器通信方案的另一个好处是，当从寄存器中读取Halo区域时，LDM中用以存储Halo区域的空间变小。在给定LDM大小的情况下，可以将更多的数据读取到LDM中，进一步提高LDM的利用率。寄存器通信机制为有限差分这类运算提供了重要的帮助。使用基于寄存器通信的Halo交换，在每个核组内部，从核线程只需要加载其相应的中心计算区域，并通过寄存器通信操作从相邻线程获取所需的Halo区域。

然而，从核之前的寄存器通信也有局限。由于用于通信的总线只能连接同一行或者同一列的从核，只有在同一行或者同一列的CPE才能够进行寄存器通信。如果两个CPE分布在不同的行和不同的列中，则需要多个寄存器通信来实现数据交换。为了解决这个问题，并尽可能减少所需的寄存器通信操作次数，本人工作为特定的寄存器通信方式定制了从核ID重映射方案。从核ID重映射方法为每个物理从核重新定义一个逻辑编号，能够以最小的寄存器通信开销完成全局寄存器数据交换。第\ref{sub:寄存器通信}节和第\ref{sub:从核ID重新映射}节分别详细地介绍了寄存器通信和从核ID重映射在人工地震勘探中的应用。

\subsection{主从核协同可定制压缩策略}
太湖之光超级计算机作为目前世界最快的超级计算机，系统的内存上限为1.3PB。有限的内存限制了神威超算能够支持数值模拟的最大算例规模和分辨率。例如，对于唐山大地震模拟，在模拟范围为$320km\times 320km \times 40km$的情况下，最高分辨率能达到$10m$。尽管这已经是同等计算区域下最高精度的模拟，但如果想支持更高分辨率的地震模拟，即便是世界最快的神威超算也无能为力。

此外，地震模拟的核心数值算法是有限差分运算，运算的效率受限于神威超算的内存带宽。内存容量和带宽并非神威超算的优势，其最大的优势是高效的浮点运算单元，但有限差分算法却无法将浮点预算单元充分利用。访存受限的数值算法和神威超算较低的byte-to-float比例形成尖锐的矛盾。

本文工作基于神威超算的特殊体系结构，提出实时压缩/解压缩设计方案，旨在解决上述挑战。压缩方案通过将程序内部变量和外部数据进行压缩，降低了神威主核内存和文件系统的容量开销，使得相同物理内存能够支持更大规模算例。另一方面，神威特殊的主从核结构、编程可控的LDM高速缓存使得压缩数据可在从核中进行解压和浮点运算。由于压缩后的数据量变小，数据通过DMA从主存传输到从核LDM的总时间也随时缩短，而压缩/解压引入的额外计算也让闲置的从核计算单元得以利用，因此较为理想地解决了第二个挑战。

主从核协同压缩方案计算流程如下所示。首先将外部输入数据（如震源、模型）压缩后存储在硬盘中，地震模拟程序将压缩后的数据去读到主存中，从核通过$dma\_get$等接口将数据从主存传输到从核LDM中，此时数据仍处于压缩状态；从核在LDM中对数据进行解压还原成常规的单精度浮点数，并使用浮点运算单元完成所需计算；计算完毕后再压缩结果数据，并通过$dmg\_put$等接口将结果传回主存。整个压缩和解压过程在LDM高速缓存中完成。第\ref{sec:实时压缩/解压缩设计方案}详细介绍了实时压缩/解压缩方案在唐山大地震模拟中的应用，将将唐山大地震的最大分辨率进一步提升到$8m$。

\section{大规模通信与IO优化方法}

\subsection{虚拟网格与计算通信重叠}
当并行规模逐渐扩展时，应用的整体通信总量、通信链、延迟也随之增大，通信开销甚至会成为整体应用的主要瓶颈。与第\ref{sec:内存带宽优化}节的内存带宽优化类似，优化通信也可从三方面出发：减少通信总量、增大通信带宽与计算通信重叠。

为了减小通信总量，本文工作使用了虚拟网格技术。对于像有限差分运算的网格边界更新，虚拟网格技术使用额外的虚拟网格边界存储边界信息，这些信息可以用来更新其他网格，从而其他网格无需交换边界信息，而是从虚拟网格中计算得到。其本质是以计算换通信。例如三维弹性波地震传播中，三个速度分量和六个应力分量在每个时间步都得到更新，同时这九个分量在每个时间步也需要进行MPI通信。使用虚拟网格技术，只需将速度分量的三个数组交换边界而无需进行应力分量六个数组的边界通信。

同步通信模型中的通信路径由多个$MPI\_Send$和$MPI\_Recv$接口串联组成，计算区域越大，通信链越长。同步模型的延迟与通信链中的计算核心数量高度相关，在大规模模拟中非常低效。本文工作使用异步MPI通信模型，通信延迟并不依赖通信链长度，并融合了高效的计算通信重叠技术。第\ref{sub:虚拟网格与计算通信重叠}将详细介绍了虚拟网格技术和计算通信重叠技术在唐山大地震模拟中波场传播模块中的应用。

\subsection{分组IO与平衡IO}
大规模的科学计算需要强大的算力，即便拥有大型超算的支持，计算时间仍然可能需要十几个小时甚至几十个小时。更多计算核心的参与也意味着发生硬件和软件错误的概率更大。重启模块的设计正是为了解决这个问题，但在重启模块的过程中，十六万进程需要分别将进程的内部状态变量输出到网络硬盘中，在没有优化的情况下，这几乎是不可实现的。为了实现大规模的平滑IO操作，本文提出了IO分组、平衡IO和LZ4压缩组合策略。

神威超算使用Lustre分布式文件系统，每个节点无本地磁盘。当多进程发起IO请求时，文件系统代理负责处理请求，并将IO数据写入/读取到不同的磁盘。当进程数量达到上万甚至十万量级时，文件系统代理同时处理大量的IO请求效率很低。此外，神威超算文件系统的每个目录文件数量超过10,000时，也同样影响IO效率。本文提出IO分组策略。当160,000进程同时发起IO请求时，将进程分成若干组，并将IO请求分散到不同的文件系统目录。IO分组能有效降低文件系统代理的压力，降低调度开销，提高效率。

在IO分组的基础上，本文工作还提出了平衡IO策略。平衡IO策略的目的减少不同文件代理的负载不均衡，最大化利用文件系统的带宽。此外，本文对大文件输出还采取了LZ4压缩方法对其进行压缩，能有效降低输出文件大小，节约IO带宽和存储空间。第\ref{sub:IO优化}节详细介绍了分组IO策略、平衡IO策略和压缩方法在唐山大地震模拟重启模块中的应用。

\section{本章小结}
本章首先介绍了太湖之光超级计算机独特的异构众核神威26010芯片架构、神威超算的软硬件并行计算环境以及与其他世界领先超算的对比。神威超算的超大计算规模、强大的浮点运算能力给计算密集型的应用带来了福音。但是，神威超算有限的内存容量和内存带宽又给访存受限程序带来了巨大的挑战和优化空间。因此，本章随后从内存带宽优化、从核计算优化以及通信和IO优化等全方面概述了基于神威超算独特的架构并行优化方法。
