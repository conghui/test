%!TEX root = main.tex
\chapter{面向申威异构众核处理器的并行优化方法} % (fold)
\label{cha:面向申威异构众核处理器的并行优化方法}

\section{申威异构众核处理器与并行计算模型} % (fold)

\subsection{申威20610处理器概述} % (fold)
\label{sub:申威20610处理器概述}

太湖之光使用了我国自主研发的片上异构众核申威26010处理器（如图\ref{fig:sunwaycpu}所示），其中包括4个核组（Core Group，CG）\citep {fu2016sunway}，每个核组包括一个主核（Management Processing Element，MPE），64个从核（Computing Processing Element，CPE）和一个内存控制器（Memory Controller, MC）。 每个申威26010处理器拥有260个计算核心，每个计算核心的主频为1.45GHz，双精度浮点运算峰值性能为3TFlops，功耗为300W。

\begin{figure}[ht]
\centering
\includegraphics[width=1\columnwidth,page=3]{SW26010架构-crop.pdf}
\caption{国产片上异构众核申威26010处理器架构。}
\label{fig:sunwaycpu}
\end{figure}

申威处26010理器的主核和从核均采用64位精简指令集。主核与Intel核心类似，可以在内核态和用户态执行，支持中断、乱序执行、内存管理等功能，适用于执行调度、管理、通信和IO等功能。从核在主核的基础上进行精简，只在用户态运行，不支持乱序执行、中断等管理功能，只支持简单的分支预测。每个核组中的64个从核组成$8\times8$从核阵列，阵列中每一行和每一列各拥有一条数据总线，总线连接的从核可通过寄存器通信方法共享数据。

\begin{figure}[ht]
\centering
\includegraphics[width=.8\columnwidth]{memory_hieararchy-crop.pdf}
\caption{CPE的内存层次结构。顶层是高效的寄存器，访问或通讯需要花费1或11个周期；中间是大小为64KB的LDM高速缓存，访问LDM需要4个周期；底层是所有从核共享的8GB主存，访问需要120+个周期，带宽为136GB/s。}
\label{fig:sunway_mem}
\end{figure}

图~\ref {fig:sunway_mem}描述了申威26010处理器的内存层次结构和访问延迟。每个申威CPU的4个核组配备32GB内存，每个核组包含一个内存控制单元（memory controller, MC），可以访问8GB内存。在目前这一代申威处理器中，每个核组连接到一个带宽为34 GB/s的DDR3接口。 虽然每个申威处理器的峰值性能可达3TFlops，但内存带宽仅为136GB/s。每个从核包含16KB的指令缓存，以及64KB的高速本地直接内存（local direct memory, LDM）。LDM高速缓存的带宽为与CPU中的L1缓存相当，但需要开发者针对不同应用进行精细的手动管理，这是应用优化的关键之一。此外，每个从核包含32个浮点寄存器与4个寄存器通信缓冲区。其中访问本地寄存器需要一个周期，通过每一行或每一列的数据总线以及寄存器通信缓冲区进行寄存器通信需要11个周期完成。良好的寄存器通信策略能有效减少全局内存访问，节约DMA数据传输带宽，提高数据复用率。

% subsection 申威20610处理器概述 (end)

\subsection{并行计算模型} % (fold)
\label{sub:并行计算模型}



神威超算支持多种并行开发模型。在不同的CPU/核组/进程间采用消息传递接口（Message Passing Interface, MPI），一个CPU的4核组也可采用线程级OpenMP模型。此外，针对神威超算异构众核结构，最适用的线程级编程模型为OpenACC和Athread。

申威处理器支持的OpenACC并行编程模型遵从OpenACC-2.0标准。OpenACC编程模型与OpenMP编程模型的设计思想类似，用户只需在核心计算循环代码块前添加OpenACC指导语句，并用自动化并行编译工具编译后可生成从核并行代码，目的是为了降低编程难度。目前除了申威处理器以外，NVIDIA公司也在大力推广OpenACC。对于NVIDIA公司而言，OpenACC的编程门槛和编程复杂度远低于CUDA，因此更易于让科学计算从业者接受。OpenACC虽然降低了编程的门槛和复杂度，但却对编译器提出了更高的要求。OpenACC的性能一方面取决了用户是否正确使用OpenACC指导语句，更重要的是取决于编译器能否将OpenACC指导语句和源代码转换为高效的并行计算代码，并充分利用硬件的各种特性提升计算性能。目前申威26010处理器的OpenACC编程模型适用于简单的循环操作，在复杂的计算中无法发挥最优性能。


Athread并行模型通过使用Athread线程库手动编写从核代码的方式细粒度地控制每一个从核。OpenACC编译器对指导语句编译的本质也是将OpenACC代码转换为Athread代码，并调用从核编译器生成从核代码。因此可以在OpenACC代码转换后的Athread代码中进行二次开发，这一定程度上能降低开发工作量。Athread线程库与Pthread线程库类似，支持对每个从核独立的计算、调度、同步等等。此外，Athread还支持申威26010处理器独有特性，如LDM高速缓存策略设计、从核间寄存器通信和向量化等等。OpenACC通常无法处理结构复杂的循环结构，Athread线程库则能达到较高的性能，但相应地提高了开发成本。

为了获得极致的计算性能，本研究使用Athread线程库进行开发，并在Athread线程库的基础上设计了一系列性能优化的方案，这将在后续的章节详细介绍。

% subsection 并行计算模型 (end)

\subsection{正演算法在申威处理器的计算挑战} % (fold)

太湖之光在2016年的首次发布\citep{fu2016sunway}，标志着最领先的超级计算机的运算能力首次超过了100Pflops，运算核心首次超过1000万。而太湖之光的计算能力达到前所未有的水平，其性能相当于天河二号超级计算机的3倍，美国Titan超级计算机的5倍，然而其内存系统相对比较一般。 如图\ref{tb:supercomputer-comp}所示，太湖之光的总内存大小与其他系统相似（只比两个基于GPU的系统Piz Daint和Titan稍好），byte-to-flop的比例非常低，只有其他异构系统的五分之一，日本“京（K)"超级计算机的十分之一。 这样的一个体系成为了将科学应用扩展到下一个层次的潜在而又严峻的挑战，特别是对于需要大内存空间和高内存带宽的地震模拟问题，打破内存的局限（内存墙）成为本研究最大的挑战。

\begin{table}[ht]
\footnotesize
\caption{太湖之光超级计算机与其他领先超算系统的简要比较。$size_{MEM}$和$BW_{MEM}$分别代表内存的总量和带宽$\frac{BW_{MEM}}{PEAK}$表示计算内存带宽与系统峰值计算性能的比率。}
\label{tb:supercomputer-comp}
\center
\begin{tabular*}{0.8\columnwidth}{cccccc}
\hline\hline
   & PEAK & LINPACK & $size_{MEM}$  & $BW_{MEM}$ & $\frac{BW_{MEM}}{PEAK}$ \\
   & (Pflops) & (Pflops) & (TB) & (TB/s) & {BYTE per flop} \\
   \hline
   TaihuLight & 125 & 93 & 1,310 & 4,473 & 0.038 \\\hline
   Tianhe-2 & 54.9 & 33.9 & 1,375 & 10,312 & 0.188 \\\hline
   Piz Daint & 25.3 & 19.6 & 425.6 & 4,256 & 0.168 \\\hline
   Titan & 27.1 & 17.6 & 710 & 5,475 & 0.202 \\\hline
   Sequoia & 20.1 & 17.2 & 1,572 & 4,188 & 0.208 \\\hline
   K & 11.28 & 10.51 & 1,410 & 5,640 & 0.5 \\\hline
\hline
\end{tabular*}
\end{table}

与神威超算超大的计算规模和强大的计算能力相比，内存容量和内存带宽则是神威超算的一大不足。虽然每个节点只有32GB内存，但神威超算超过40,000个节点以及高效的网络通信，内存容量的不足可通过将应用扩展到更多的节点来弥补。神威超算每个节点的内存带宽为136GB/s，仅为Intel KNL的1/3以及NVIDIA GPU的1/5，给许多访存受限问题提出了巨大的挑战。本研究的正演算法是基于波动方程的显示时间有限差分算法，该算法的核心计算模式为$Stencil$运算，是典型的访存受限问题。在三维弹性波地震模拟涵盖了30-50个三维数组，这进一步加剧了访存的挑战。

如第\ref{sub:神威超算软硬件环境}节所述，深度的从核优化使用Athread并行模型，并合理利用LDM高速缓存。从核计算的基本流程为：（1）主存中的数据通过DMA方式传输LDM高速缓存；（2）从核访问LDM高速缓存中的数据并执行相关运算；（3）将运算结果从LDM高速缓存通过DMA方式传输到主存中。

$Stencil$计算的主要耗时在于DMA传输时间，这由两方面因素决定：DMA数据传输总量和DMA传输带宽。在神威从核运算中，每个核组有$8\times8$从核阵列，每个从核分别需要通过DMA方式将数据在主存和LDM中进行传输。与MPI通信类似，核组主存的划分方式、64个从核的处理主核数据时的逻辑排列方式，也会对DMA数据传输总量造成影响。当数组的数量增大时，影响也随之增大。


虽然神威超算每个核组的峰值内存带宽为34GB/s，然而在实际开发中，大多数应用都无法达到。DMA传输带宽与DMA传输的连续数据块大小有密切的正相关关系（如表
\ref{tb:sw-bw}所示）。由表可见，当DMA连续传输块大小不足128字节时，DMA的传输带宽不足理论带宽的一半。只有当DMA连续传输块大小超过512字节时，才可以达到较合理的内存带宽利用率。

\begin{table*}[thb]
%\renewcommand{\arraystretch}{1}
%\small
%\tiny
\caption{单核组DMA连续传输块大小对应的实测带宽(GB/s)}
\label{tb:sw-bw}
\centering
\begin{tabular}{|c|c|c||c|c|c|}
  \hline
  块大小（字节） & $Get$ & $Put$ & 块大小（字节） & $Get$ & $Put$ \\
  \hline
  32&4.31&2.56& 512&27.42&30.34\\
  \hline
  64&9.00&9.20& 576&25.96&28.91\\
  \hline
  128&17.25&18.83& 640&29.05&32.00\\
  \hline
  192&17.94&19.82& 1024&29.79&33.44\\
  \hline
  256&22.44&25.80& 2048&31.32&35.19\\
  \hline
  384&22.88&24.67& 4096&32.05&36.01\\
  \hline
\end{tabular}
\end{table*}

本研究的三维弹性波地震模拟包含30-50个数组。需要将所有的三维数组频繁读写到LDM中，读写和划分的策略影响着DMA数据传输的总量。另一方面，由于数组总量多，使得每个数组在LDM中容纳的元素很少，每次DMA操作是连续的数据块大小也很小，极大地制约了DMA传输带宽。因此，$Stencil$运算在申威处理器的性能优化主要关注于如何降低DMA数据传输总量和提升DMA传输带宽。

\section{最小DMA数据传输方案} % (fold)
\label{sec:最小DMA数据传输方案}

\subsection{单从核数据传输方案推导}

申威26010芯片高效的从核计算需要从LDM中读取数据而非从共享主存中获取，但每个从核仅有64KB高速缓存LDM，在涉及大量数组的kernel中，每个数组能够分配的LDM缓存非常有限。此外，对于stencil这类访存受限问题，从核优化的另外一个重点在于缩短数据从主存DMA传输到LDM的时间。缩短DMA传输的时间有两个方法，减少数据传输总量和提升DMA传输带宽。

在stencil的计算中，每更新一个中心点需要访问周围的其他格点。LDM空间有限，无法储存整个二维或者三维数组（波场），因此需要对波场进行划分。划分后的每个子波场包含中心波场以及边界出所需额外访问的Halo区域。不同的划分会对应不同的Halo区域，从而影响DMA数据传输总量。本小节推导出在stencil运算二维划分下最小的DMA数据传输总量。

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\columnwidth]{单从核2D划分最小DMA传输.pdf}
  \caption{单从核2D划分最小DMA传输。}
  \label{fig:cpe-2d-derive}
\end{figure}

如图\ref{fig:cpe-2d-derive}(a)所示，设数组的大小为$NZ\times NX$，其中Z轴是数组的快轴。Halo区域的宽度为$H$，每个数组在LDM所占的空间为$S=(wx+2H)\cdot(wz+2H)$，计算所需的格点都存储在这片空间中，我们称之为LDM窗口。窗口可以沿Z轴或者X轴滑动（如图\ref{fig:cpe-2d-derive}(b)(c)所示）：
\begin{itemize}
  \item 沿着X轴滑动时，原窗口最右列的Halo区域能够被新窗口完全复用，新窗口只需读取右边四列数据。
  \item 沿着Z轴滑动时，原窗口最下行的Halo区域能够被新窗口完全复用，新窗口只需读取下面四行数据。
\end{itemize}

虽然两种滑动方式的DMA传输数据量相同，但效率却有差异。但由于数组的快轴是Z轴，沿着X轴滑动时，新窗口需调用4次DMA，每次DMA的大小为5个元素；而沿着Z轴滑动时，新窗口需调用5次DMA，每次DMA的大小为4个元素。由表\ref{tb:sw-bw}可知，DMA传输的连续数据块越大能获得更大的带宽，因此窗口沿着X轴从左向右滑动更有利于提高DMA传输效率。

公式\ref{eq:单从核二维划分最小DMA传输}描述二维划分下最小的DMA数据传输总量
\begin{equation}
  \min_{wx,wz} D(wx,wz) = (wz+2H)\cdot(NX+2H)\cdot\frac{NZ}{wz}，
  \label{eq:单从核二维划分最小DMA传输}
\end{equation}
其中$(wz+2H)\cdot(NX+2H)$表示每次沿着X轴滑动所需传输的数据总量，$\frac{NZ}{wz}$表示再次从左至右滑动的次数（轮数）。公式\ref{eq:单从核二维划分最小DMA传输}化简后可得公式\ref{eq:单从核二维划分最小DMA传输2}。
\begin{equation}
  \min_{wz} D_{1cpe}(wz) = (1+\frac{2H}{wz})\cdot(NX+2H)\cdot NZ，
  \label{eq:单从核二维划分最小DMA传输2}
\end{equation}

我们可以看到，$wz$与$D_{1cpe}(wz)$呈反相关，当$wz$越大时，$\frac{NZ}{wz}$越小，即再次从左至右滑动的次数越少，由于每次从左至右滑动时都需要额外读取上下边界的Halo区域，因此轮数越少，传输总量$D_{1cpe}(wz)$越小。

\subsection{64从核数据传输方案推导}
公式\ref{eq:单从核二维划分最小DMA传输2}描述的是一个从核进行DMA传输的最优情况。一个核组中有64个从核，64个从核的不同排列和划分方案也会对DMA的数据传输总量造成影响。现推导单核组64从核共同进行计算和DMA传输时，最小的传输总量。

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\columnwidth]{多从核2D划分最小DMA传输.pdf}
  \caption{多从核2D划分最小DMA传输。}
  \label{fig:multi-cpe-2d-derive}
\end{figure}

如图\ref{fig:multi-cpe-2d-derive}所示，设每个MPI进程需要更新的网格为$NZ\times NX$，每个核组的从核按照$mz \times mx$进行划分。每个从核需要处理的网格需要进一步划分才能放进64KB的LDM中，因此对单从核内部进一步按照$pz\times px$划分，划分后的每一个小块能加载到LDM中，LDM窗口的大小为$(wz+2H)\times(wx+2H)$。我们需要推导出当$mz$和$mx$如何划分时，能够使得多从核划分下的DMA传输数据量最小。此时，每个从核LDM最小DMA数据传输的优化函数变为
\begin{equation}
  \min_{mx,mz,px,pz} D_{64cpe}(mx,mz,px,pz) = (\frac{NZ}{mz\cdot pz}+2H)\cdot(\frac{NX}{mx}+2H)\cdot pz，
  \label{eq:多从核二维划分最小DMA传输}
\end{equation}
化简可得
\begin{equation}
  \min_{mx,mz,pz} D_{64cpe}(mx,mz,pz) = (\frac{NZ}{mz}+2H\cdot pz)\cdot(\frac{NX}{mx}+2H),
  \label{eq:多从核二维划分最小DMA传2}
\end{equation}
且当64个从核都进行参与时，满足以下条件
\begin{equation*}
    \left\{\begin{matrix}
mx\times mz=64\\
1\le mz \le 64 \\
pz \ge 1 \\
1 \le \frac{NZ}{mz\cdot pz} \le Z_{max}
\end{matrix}\right.
\Leftrightarrow
  \left\{\begin{matrix}
mx\times mz=64\\
1\le mz \le 64 \\
1 \le pz \\
\frac{NZ}{mz\cdot Z_{max}} \le pz \le \frac{NZ}{mz}
\end{matrix}\right.。
\end{equation*}
其中，$Z_{max}$为每个从核LDM中最多可用来分配给Z轴的空间。将$mx=61/mz$代入到公式\ref{eq:多从核二维划分最小DMA传2}中，公式\ref{eq:多从核二维划分最小DMA传2}可得
\begin{equation}
  \min_{mz,pz} D_{64cpe}(mz,pz) = \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot mz \cdot pz}{64} \cdot 2H + pz\cdot 4H^2，
  \label{eq:多从核二维划分最小DMA传3}
\end{equation}

（1）若$pz \ge \frac{NZ}{mz\cdot Z_{max}} \ge 1$，代入公式\ref{eq:多从核二维划分最小DMA传3}可得
\begin{equation}
  \min_{mz} D_{64cpe}(mz) \ge \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot NZ}{64\cdot Z_{max}} \cdot 2H + \frac{NZ}{mz\cdot Z_{max}}\cdot 4H^2，
  \label{eq:多从核二维划分最小DMA传4}
\end{equation}
则$D_{64cpe}(mz) $与$mz$呈反相关，当$mz$越大，$D_{64cpe}(mz) $越小。当$mz=64, mx=1, pz = \frac{NZ}{64\cdot Z_{max}}$时，$\min_{mz} D_{64cpe}(mz=64)$最小。

（2）若$\frac{NZ}{mz\cdot Z_{max}} < 1$，则$pz\ge 1$，代入公式\ref{eq:多从核二维划分最小DMA传3}可得
\begin{equation}
\begin{aligned}
  \min_{mz,pz} D_{64cpe}(mz,pz) &\ge \frac{NX\cdot NZ}{64}+\frac{NZ}{mz}\cdot 2H+\frac{NX\cdot mz}{64} \cdot 2H + 4H^2 \\
  &\ge \frac{NX\cdot NZ}{64}+2H\cdot2\sqrt{\frac{NZ}{mz}\cdot\frac{NX\cdot mz}{64} } + 4H^2 \\
  &=\frac{NX\cdot NZ}{64}+\frac{H}{2}\sqrt{NX\cdot NZ}+ 4H^2
  ，
\end{aligned}
  \label{eq:多从核二维划分最小DMA传5}
\end{equation}
当且仅当$\frac{NZ}{mz}=\frac{NX\cdot mz}{64}$时，等号成立。此时$mz=8\sqrt{\frac{NZ}{NX}}$，$mx=8\sqrt{\frac{NX}{NZ}}$。并可推出$\frac{mx}{mz}=\frac{NX}{NZ}$。此时$pz=1$。

通过总结公式\ref{eq:多从核二维划分最小DMA传4}，我们得知在充分利用所有从核进行二维划分的情况下，满足以下条件可使DMA的数据传输总量最小：
\begin{itemize}
  \item 当$pz \ge \frac{NZ}{mz\cdot Z_{max}} \ge 1$时，将64个从核按照$64\times 1$进行排布，可获得最小DMA数据传输总量；
  \item 当$\frac{NZ}{mz\cdot Z_{max}} < 1$时，从核按照$\frac{mx}{mz}=\frac{NX}{NZ}，pz=1$进行排布，可获得最小DMA数据传输总量。
\end{itemize}

\subsection{基于寄存器通信的从核Halo更新}
\label{sub:寄存器通信}

最小的DMA数据传输总量的推导的前提是一个核组内的64个从核间能够进行数据交互，且交互的成本很低以至于可以忽略从核间数据交互的成本。申威26010处理器与其他主流处理器相比，最与众不同的一个功能是每个核组中的64个CPE中的寄存器通信机制。由64个CPE组成的计算核心网格中，我们有8个列通信总线和8个行通信总线，这些总线成为了CPE之间进行快速寄存器通信的通道，为不同的CPE提供了高速的数据共享能力。这为$Stencil$计算中的数据重用提供了完美的解决方案。

使用基于寄存器通信的Halo交换，在每个CG内部，CPE线程只需要加载其相应的中心计算区域，并且可以通过寄存器通信操作从相邻线程获取所需的Halo区域。但同一核组内的不同线程间的寄存器通信并不能完成所有Halo的交换，跨不同核组的边界通信仍然需要所对应的线程通过共享内存的方式从主存中通过DMA方式进行加载。

寄存器级通信是通过\emph{行、列通信总线}的一对\emph{Put}和\emph{Get}API接口来实现的。发送方CPE使用\emph {Put}操作将256位寄存器数据发送到接收方CPE的\emph{传输缓冲区}，而接收方CPE使用\emph{Get}操作将256位数据从\emph{传输缓冲区}传输到本地通用寄存器中。

寄存器是从核中非常稀有且宝贵的资源。每个从核只有8个256位的寄存器可用于通信。寄存器的“发送”和“接受”是同步操作，批量发送将导致接收方缓冲区饱满，从核阻塞发送方线程的正常执行。不当的寄存器通信操作反而会降低计算性能，甚至会导致不同从核线程死锁。本研究设计了间隔收发同步通信策略实现寄存器高效通信，通信的步骤如下：
\begin{itemize}
  \item 编号为奇数的从核统一执行寄存器发送操作，将1个256位的向量发送给编号为偶数的从核寄存器缓冲区；
  \item 同时，编号为偶数的从核执行寄存器接收操作，等待其他寄存器向其发送数据；
  \item 奇数寄存器发送、偶数寄存器接收完毕之后，以同样的策略执行偶数寄存器发送、奇数寄存器接收操作；
  \item 以同样的方式通信下一个256位向量。
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{寄存器通信-crop.pdf}
\caption{同一核组内的64个CPE使用寄存器通信。}
\label{fig:64by1-reg}
\end{figure}

由于用于通信的总线只能连接同一行或者同一列的从核，只有在同一行或者同一列的CPE才能够进行寄存器通信。如果两个CPE分布在不同的行和不同的列中，则需要多个寄存器通信来实现数据交换。

正如在第\ref{sec:最小DMA数据传输方案}节中的推导，在大多数情况下我们采用$ 64 \times1 $的CPE线程配置。在这样的配置中，需要交换数据的多数的CPE线程将不在同一行或列中。为了解决这个问题，并尽可能减少所需的寄存器通信操作次数，我们设计了一个特定的CPE ID重新映射方案。

如图~\ref{fig:id-remapping}所示，对于奇数行，我们保持重映射的逻辑 CPE ID（括号内的ID）与原始硬件CPE ID相同（ID外部的ID括号）；对于偶数行，我们则保持重新映射的逻辑CPE ID与原始硬件CPE ID相反。从核ID重新映射之后，可以快速索引到正确的目标通信从核线程号。

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\columnwidth]{从核ID重映射-crop.pdf}
\caption{
从核ID重新映射方案。它确保所有相邻CPE线程间的数据交换操作。 括号外的ID是原始硬件从核ID，而括号内的ID是重新映射的逻辑从核ID。}
\label{fig:id-remapping}
\end{figure}

\section{增大DMA数据传输带宽方案} % (fold)

最小DMA数据传输方法侧重降低DMA数据传输总量，本节提出增大DMA数据传输带宽的两种方案——共位数组融合和数据布局转换，旨在缩短DMA数据传输时间。

\subsection{共位数组融合}
\subsubsection{朴素共位数组融合}

第\ref{sec:最小DMA数据传输方案}节推导出了最小DMA数据传输方案。然而，即便采用上述的最佳并行方案，由于三维弹性波介质中的地震波正演需要访问大量的数组，在只有64KB的LDM中，能够分配给每个数组的缓存空间也非常有限的。三维$Stencil$运算操作需要当前格点上下左右前后六个面相近的格点，因此载入LDM的是划分后的三维数据，这在内存中只有一个维度是连续。DMA数据传输只能沿着快轴进行多次DMA传输。在申威26010处理器中，DMA传输启动开销虽然不大，但DMA传输的效率受到DMA连续传输数据块大小的严重影响。

例如，地震波正演的速度更新内核需要读取10个不同的三维数组：$ u $，$ v $，$ w $，$ xx $，$ yy $，$ zz $，$ xy $，$ xz $，$ yz $，$ d $，分别是速度的三个分量、应力的六个分量以及密度。根据第\ref{sec:最小DMA数据传输方案}节的推导可得
\begin{equation}
\begin{aligned}
C_z &\cdot C_y = 64  \\
W_z \cdot W_y \cdot W_x \cdot N_{array} &\cdot N_{bytes\_per\_variable} < 64 \times 1024
\end{aligned}
\label{eq:wzy}
\end{equation}
其中$ N_{array} $表示当前计算函数所需的3D数组的数量，$N_{bytes\_per\_variable}$表示每个变量的字节数。为了完成空间四阶的中心差分运算，LDM缓存中至少需要沿着$x$方向加载五个平面（以第三个平面为中心，前后两个面各为差分长度）。因此$ W_x $的最小值为5。对于参数$ W_y $，因为$（W_y-2H）$是加载到LDM中的有效区域，需要将$ W_y $设置为至少9（对于$ H = 2 $）以保持Halo成本处于合理的水平。因此，在10个输入数组的情况下可以从公式\ref{eq:wzy}推导出：
\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 10 \cdot 4 < 64 \times 1024\\
\label{eq:wzy1}
\end{equation}
可得$ W_z $的最大值为32。在这种情况下DMA传输的连续块的大小为128字节。由表\ref{tb:sw-bw}可知对应的带宽约为17GB/s，仅为内存峰值带宽的50\%，利用率较低。

三维弹性波正演中DMA批量传输块小的现象在二维或声波方程中并不明显。在二维声波方程中不需要考虑应力和空间分量，程序中数组的总量约为三维弹性波情景中的20\%甚至更低，这使得二维声波方程中每个数组可利用的LDM容量是三维弹性波中的5倍，DMA批量传输数据块也随着增大。不过二维声波方程扩展到三维弹性波方程虽然增大了数组的总量，但物理变量却变动较小，包含应力的增加以及速度由原来的二维扩展到三维。这启发了共位数组融合思想的提出。

仍旧以速度更新函数为例，虽然该函数涉及到10个三维数组，但这10个数组可分成3组，分别代表速度（3个分量）、应力（6个分量）和密度。三个速度分量是彼此对称的，所执行的计算操作类似。六个应力分量也是如此。本研究称这三个速度分量（$ u $，$ v $，$ w $）或这六个应力分量（$ xx $，$ yy $，$ zz $，$ xy $，$ xz $和 $ yz $）为共位数组（co-located array），共位数组融合则分别将三个速度分量或六个应力分量融合成一个大数组。共位数组融合的过程如图\ref{fig:naive-fusion}所示。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth,page=1]{分块共位数组融合-crop.pdf}
\caption{朴素共位数组融合示例。}
\label{fig:naive-fusion}
\end{figure}

共位数组融合之后，从等式\ref{eq:wzy}中可以看到，只有三个单独的数组可以读取，则有
\begin{equation}
W_z \cdot 9 \cdot 5 \cdot 3 \cdot 4 < 64 \times 1024\\
\label{eq:wzy2}
\end{equation}
可得$ W_z $的最大值约为108。此时，DMA传输连续数据块大小为432字节，内存带宽利用率可提高到80％左右。 

在地震波正演的极端情况下（应力更新函数），共位数组融合技术可以将DMA传输连续快大小从84字节增加到512字节，从而将有效内存带宽从50.47 GB/s提高到104.82 GB/s，极大的提升了内存带宽。

共位数组融合方法显著地提升了DMA传输的带宽，但这种数组融合方法还面临着一个问题：无法向量化。本研究称之为朴素共位数组融合方法。朴素共位数组融合方法可以将任意数量的数组进行融合，由于不同数组的元素相互交错排列而无法实现向量化。虽然向量化对$Stencil$等内存受限运算无显著的性能提升，但能显著提升计算受限的问题（如矩阵运算、卷积运算）的计算效率。本研究在朴素共位数组融合的基础上提出分块共位数组融合，使之支持向量化。

\subsubsection{分块共位数组融合}

申威26010处理器支持位宽为256比特的向量化操作，等价于支持4个连续double类型元素的算术运算。申威处理器建造之初为了更好的支持双精度浮点数的科学运算，向量化操作并不区分双精度和单精度，因此尽管单精度浮点数的位宽只有双精度浮点数的一半，申威处理器也支持4个连续float类型元素的向量化运算。分块共位数组融合策略以4个连续的元素为一个单元块，然后使用朴素共位数组融合，融合过程如图\ref{fig:block-fusion}所示。


\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth,page=2]{分块共位数组融合-crop.pdf}
\caption{分块共位数组融合示例。}
\label{fig:block-fusion}
\end{figure}

不管需要融合的数组数量为多少，始终以同一个数组的4个元素为一块，保证向量化的正常运算。

\subsection{数据布局转换} % (fold)
\label{sub:数据布局转换}

共位数组融合方案能有效增大DMA数据传输块大小，但只适用于能够融合的数组，如速度的三个分量、应力的六个分量以及其他具有相同读写模式的变量。然而在完整的大地震模拟程序中，并非所有变量都能参与融合。对于不能进行数组融合的变量，DMA数据传输仍旧面临着传输数据块小的困境。本小节针对个别无法参与共位数组融合的变量提出另外一种增大DMA数据传输块大小的方法——数据布局转换。

数据布局转换方法根据特定的从核划分方案对数组进行预处理，重排数组中的元素，使之在从核读写时能够增大DMA数据传输块大小。数据布局转换的基本思想如图\ref{fig:layout-trans}所示。在原始的并行策略中，每个从核需要读写对应的分块（如图\ref{fig:layout-trans}(b))所示，由于原始数据在内存中的排列仅沿着一个维度连续，每个从核仅能通过多次读取小块来加载从核所需的2D平面。每次DMA传输的连续数据块小，DMA性能低下。

为了保证每个从核所需的2D平面数据在内存中是连续的，在初始化阶段每个从核首先对自己需要处理的2D平面数据进行数据布局转换，转换后的结果如图\ref{fig:layout-trans}(b)所示，转换过程如下：

\begin{itemize}
  \item 主核为需要执行数据布局转换的数组新开辟空间。由于每个从核需要执行$Stencil$运算，新开辟的空间能够容纳$Stencil$运算中的Halo部分。
  \item 需要处理完整数据边界的从核从主存中读取边界，处理中心区域的从核的Halo区域通过寄存器通信方式从邻居从核获得。
  \item 从核处理的中心区域部分则按照图\ref{fig:layout-trans}(c)所示的地址变换算法计算新的地址。
\end{itemize}   

数据布局转换后，从核DMA数据传输块的大小从原来的$wz$变为$wz\times wy$，通常情况能够获得很好的DMA带宽。

\begin{figure}[ht]
\centering
\includegraphics[width=1\columnwidth,page=1]{datalayouttransform-crop.pdf}
\caption{数据布局转换示例。}
\label{fig:layout-trans}
\end{figure}

数据布局转换方法也有局限性，它能有效提升无法参与共位数组融合的变量。然而如果该数组在每次计算更新后需要与邻近进程通信交换边界，数据布局转换则会打破需要进行通信的元素位置。原来位于边界的元素执行地址转换之后可能位于三维数组的其他位置，边界通信时无法按照连续的内存地址索引这些元素，只能通过地址逆转换还原数据，而这将进入额外的计算和访存开销。因此本研究并不建议需要与邻居进程通信的变量使用数据布局转换策略。在真实的地震模拟中，需要执行MPI通信的物理量为速度和应力，他们可以使用共位数组融合。无法使用共位数组融合的物理量通常为介质参数如密度($\rho$），它们在整个程序中始终为常量，数据布局转换策略正好为这些数据提供了很好的增大DMA数据传输带宽的解决方案。

\subsection{内存对齐}

内存对齐是另外一种减小DMA数据传输总量的方式。而在DMA数据传输中，每次的DMA传输都按128字节对齐。未按照128字节对齐分配的数组在DMA数据传输时会降低DMA传输效率。例如，若DMA连续数据传输的大小为256字节，若主存中的数据未按照128字节对齐，则需要发起3次DMA传输，共传输384字节，有效数据为256字节。若主存中数据按照128字节对齐，则只需要发起2次DMA传输，效率为前者的1.5倍。

上文介绍的共位数组融合和数据布局转换方法并未考虑内存对齐，本小节讨论每一种方法如何内存对齐的情况。对于$Stencil$运算，从核的DMA数据传输可分为两部分，Halo区域和中心区域传输。

朴素共位数组融合的数组数量不确定，因此只能在数组起始地址按128字节对齐，每个从核的计算起始地址和边界从核的Halo起始地址无法保证按128字节对齐。只有一个从核能获得字节对齐带来的带宽收益。

分块数组融合按照向量化的要求保证每个数组有4个元素连续排列。地震模拟应用中数据类型通常使用单精度浮点数（$float$），每个数组连续的字节数为16。如果正好有8个数组需要融合，则正好可实现128字节的内存对齐，然而实际情况并无法保证每次分块数组融合正好是8个数组（此处假设为7个）。对于此类情况有两种对齐方式：Halo起始地址对齐和计算中心起始地址对齐，任意一种方式都容易实现，但只能获得其中一项带宽收益。本研究提出Halo地址地址和中心计算起始地址同时对齐的方法：
\begin{itemize}
  \item Halo区域的起始地址对齐要求可以通过分配内存时按128字节对齐分配内存。
  \item 计算中心区域起始位置取决于Halo区域的长度，Halo区域的长度取决了差分算子长度和融合的数组数量。如果Halo区域的字节长度不能整除128，则计算中心区域的起始地址无法按照128字节对齐。这种情况下本研究通过扩展Halo区域长度，使之能被128整除。
  \item 尾部区域也采用同样的方式进行区域扩展，保证所有从核读取的Halo区域和计算中心区域也按照128字节内存对齐。
\end{itemize}

数据布局转换不涉及多个数组合并，因此内存对齐较为简单。首先在内存分配时保证按128字节对齐，然后调整每个从核的划分，是的每个从核读取的起始地址也按128字节对齐。

%数据布局转换的过程如图1所示，首先MPE开一个空间，之后CPE先进行中间区域的地址转换，之后判断是否为某一个边界，如果为边界则需要单独读取边界，如果不是边界则需要和邻居CPE交换边界来完成转换。而地址转换的地址转换为：首先，将整个z轴在原数组按照z4进行分层，而新数组则以z4+2H分层，但层数保持不变，旧的数组z轴长度为p_{z2}p_{z3}z_4也，即z_2，而新的数组z轴长度为p_{z2}p_{z3}(z4+2H)，而y轴长度保持不变。因为我们是按照一个平面一个平面地转换，因此先计算x方向的偏移量，新数组为i_{x}p_{z2}p_{z3}(y_2+2H)(z_4+2H)。在新的数组中，数据在一个层中排列完之后再排列下一层，层的序号为(i_z p_{z3} + z_{id})，再乘以y轴长度y2+2H则为该层之前的偏移。之后在该层内部先计算y轴偏移(i_y p_{y3}+y_{id})(z_4+2H)，之后再加上z轴上边界H偏移，即为该区域需要转换的首地址，而旧数组地址的计算为常规计算。之后通过拷贝操作来将旧数组拷贝到新数组。

% subsection 数据布局转换 (end)

% section 数据结构变换 (end)

\section{实时压缩/解压缩设计方案}
\label{sec:实时压缩/解压缩设计方案}
\subsection{实时压缩必要性分析与设计思路}

太湖之光超级计算机作为目前世界最快的超级计算机，系统的内存上限为1PB。有限的内存限制了神威超算能够支持数值模拟的最大问题规模和分辨率。例如，对于唐山大地震模拟，在模拟范围为$320km\times 320km \times 40km$的情况下，最高分辨率能达到$10m$。尽管这已经是同等计算区域下最高精度的模拟，但如果想支持更高分辨率的地震模拟，即便是世界最快的神威超算也无法支持。

此外，地震模拟的核心数值算法是有限差分运算，运算的效率受限于神威超算的内存带宽。内存容量和带宽并非神威超算的优势，其最大的优势是高效的浮点运算单元，但有限差分算法却无法将浮点预算单元充分利用。访存受限的数值算法和神威超算较低的byte-to-float比例形成尖锐的矛盾。

本文工作基于神威超算的特殊体系结构，提出实时压缩/解压缩设计方案，旨在解决上述挑战。压缩方案通过将程序内部变量和外部数据进行压缩，降低了神威主核内存和文件系统的容量开销，使得相同物理内存能够支持更大规模算例。另一方面，神威特殊的主从核结构、编程可控的LDM高速缓存使得压缩数据可在从核中进行解压和浮点运算。由于压缩后的数据量变小，数据通过DMA从主存传输到从核LDM的总时间也随时缩短，而压缩/解压引入的额外计算也让闲置的从核计算单元得以利用，因此较为理想地解决了第二个挑战。

图\ref{fig:compression-workflow}描述了主从核协同压缩方案计算流程。首先将外部输入数据（如震源、模型）压缩后存储在硬盘中，地震模拟程序将压缩后的数据去读到主存中，从核通过$dma\_get$等接口将数据从主存传输到从核LDM中，此时数据仍处于压缩状态；从核在LDM中对数据进行解压还原成常规的单精度浮点数，并使用浮点运算单元完成所需计算；计算完毕后再压缩结果数据，并通过$dmg\_put$等接口将结果传回主存。

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{主从核协同压缩方案-crop.pdf}
\caption{主从核协同压缩方案计算流程。}
\label{fig:compression-workflow}
\end{figure}

\subsection{单精度浮点数压缩算法}

压缩方案的本质是以计算时间换取内存/存储空间，势必引入额外的计算开销。大地震模拟本是计算量大、耗时长的科学应用，如果压缩方案引入的额外计算开销太高，即便能够模拟更大规模的地震算例，也不具备现实意义。因此，高效的压缩算法是也压缩方案设计中一大重要环节。

传统的压缩算法如行程长度压缩算法(Run Length Encoding, RLE)、哈夫曼编码、Lempel-Ziv等对字节流进行压缩。这并不适合有限差分运算模式。例如四阶中心差分运算需要获取中心格点以及周围的各两个格点，这在内存中是不连续的，几乎无法从压缩后的数据中索引差分计算所需的格点。因此，本文工作根据有限差分运算特殊的计算模式定制了压缩算法。

设计压缩方案的另外一个考量是选择无损压缩或是有损压缩。无损压缩能够保持计算精度但需要较大的计算量且压缩率较低，而无损压缩在牺牲适量精度的情况下获得了较低的计算开销和较高的压缩率。地震模拟的数据通常用32位单精度浮点数表示，结果以地震波快照的形式呈现，因此对数据的精度有较高的容忍度。为了提高计算效率，本文提出并采用三种有损压缩算法，在可接受精度的前提下，将32位浮点数压缩成16位浮点数。

\subsubsection{IEEE 754标准半精度浮点数压缩方法}
第一种压缩方法将IEEE 754单精度浮点数压缩成IEEE标准半精度浮点数。单精度浮点数的符号位、指数位和尾数位分别有1、8、23位，半精度浮点数的符号位、指数位和尾数位分别为1、5、10位（如图\ref{fig:ieeefloathalf}所示）。IEEE标准单精度浮点数与半精度浮点数之间的转换已有大量成熟工作\cite{van2008fast}，本文工作将Scipy\cite{jones2014scipy}中的单精度-半精度浮点数转换开源代码移植到神威从核。

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth,page=1]{压缩浮点数表示形式-crop.pdf}
\caption{IEEE标准单精度-半精度浮点数表示方式。}
\label{fig:ieeefloathalf}
\end{figure}

在不考虑上溢（overflow）、下溢（underflow）、非规范化（denormalized）数的情况下，标准单精度-半精度浮点数转换核心算法如算法\ref{alg:ieeefloat}所示。
\begin{algorithm}[ht]
%\scriptsize
%\footnotesize
\small
\caption{IEEE标准单精度-半精度浮点数转换核心}\label{alg:ieeefloat}
\begin{algorithmic}[1]
\State \textbf{// 16位半精度转32位单精度}
\State uint32\_t \textbf{float16\_to\_float32}(uint16\_t h) \{
\State \quad\quad return ((h\&0x8000)<<16) | (((h\&0x7c00)+0x1C000)<<13) | ((h\&0x03FF)<<13);
\State \}
\State
\State \textbf{// 32位单精度转16位半精度}
\State uint16\_t \textbf{float32\_to\_float16}(uint32\_t f) \{
\State \quad\quad uint32\_t x = *((uint32\_t*)\&f);
\State \quad\quad return ((x>>16)\&0x8000)|((((x\&0x7f800000)-0x38000000)>>13)\&0x7c00)|((x>>13)\&0x03ff);
\State \}
\end{algorithmic}
\end{algorithm}

\subsubsection{动态指数位压缩方法}
IEEE单精度-半精度的转换算法简明高效，但由于半精度浮点数使用固定的指数位和尾数位，对于指数范围大于五位的浮点数，压缩方案会产生数值问题。另一方面，对于指数范围较小的变量，固定的五位指数位则变成了浪费。

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth,page=2]{压缩浮点数表示形式-crop.pdf}
\caption{动态指数位浮点数表示形式。}
\label{fig:ieeefloatdynamic}
\end{figure}

为了解决上述问题，本人工作提出了动态指数位压缩方法（如图\ref{fig:ieeefloatdynamic}所示）。这个压缩方法中的指数位和尾数位是动态确定的，不同的数组会得到不同的结果。指数位和尾数位的计算方法如公式\ref{eq:floatdynamic}所示：
\begin{equation}
  \begin{aligned}
    N_e &= \log(E_{max} - E_{min}) \\
    N_f &= 15 - N_e
  \end{aligned}
  \label{eq:floatdynamic}
\end{equation}
其中$N_e$表示指数位，$N_f$表示尾数位，$E_{max}$和$E_{min}$分别表示被压缩数组中元素最大可能指数位数和尾数尾数。

本方法最多可用15位表示指数（实际情况不会出现15位指数），远大于标准单精度浮点数的8位，足以保证全动态范围覆盖。对于指数范围较小数据，本方法则可为尾数部分保留更多表示位数，提高压缩后数据的精度。但是，由于涉及对数或除法运算，本方法唯一的缺点是计算成本相对较高。

\subsubsection{浮点数归一化压缩方法}
在IEEE标准的单精度浮点数表示中，大部分为归一化数（normalized value）。令符号位、指数位、尾数位分别为$S$、$E$和$F$，则归一化浮点数的数值为
\begin{equation}
  f(S,F,E) = (-1)^S \cdot 1.F^{E-127}
\label{eq:floatrepresent}
\end{equation}

浮点数归一化压缩方法对浮点数进行归一化，将同一数组的所有值归一化到1到2之间的范围（如公式\ref{eq:floatnormalized}所示），根据则归一化浮点数的表示方法（公式\ref{eq:floatrepresent}）可知，该浮点数的指数位$E$恒等于127。
\begin{equation}
\begin{aligned}
  v_{normalized} &= \frac{v - v_{min}}{v_{max} - v_{min}} \\
  v_{compressed} &= (v_{normalized} << 9) >> 7
\end{aligned}
\label{eq:floatnormalized}
\end{equation}

对范围为（1，2）的浮点数进行压缩，由于指数位恒等于127，压缩时无需对指数位进行存储，只需保留符号位和尾数位即可。公式\ref{eq:floatnormalized}中将归一化后的浮点数左移9位，然后右移7位得到16位数，即为压缩后的结果（如图所示\ref{fig:ieeefloatnormalized}）。值得注意的是，在左移的过程中，符号位消失，因此需要在左移之前将符号位保存在第16位尾数中。

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth,page=3]{压缩浮点数表示形式-crop.pdf}
\caption{指数位归一化浮点数表示形式。}
\label{fig:ieeefloatnormalized}
\end{figure}

上述展示的三种压缩方法仅针对地震模拟应用处理了大部分常规浮点数，并未覆盖浮点数全部范围，如$NaN$，$INF$等等，这些数值并未在地震模拟应用中出现。

\subsection{压缩算法性能与精度分析}

压缩方案本身会引入额外的计算（主要是整数运算）以及密集的内存和高速缓存访问。即使所有的压缩/解压缩操作都发生在LDM中，频繁的LDM读写也会严重降低CPE的计算效率。因此，本文工作的第一个压缩版本只能达到不压缩的1/3性能。为了降低压缩方案的性能损失，我们进行了一系列优化：

\begin{itemize}
  \item 根据不同的物理量对精度敏感度不同，在满足精度要求的情况下，选择计算复杂度较低的算法；
  \item 在从核LDM中对压缩数据执行分块解压缩，以降低减少重复解压次数和LDM读写次数。
  \item 将解压缩和计算代码紧密地（在汇编代码级别）耦合以最大化寄存器中的变量的寿命，减少对LDM的加载和存储次数；
\end{itemize}

将压缩方案与地震波传播内核相结合并经过深度优化后可获得24\%的计算性能提升。这是因为压缩方案可以使得在相同的物理带宽情况下处理更多的数据。换而言之，对于地震波传播这类访存受限问题，压缩方案使得需要处理的数据总量降为原来的一半，极大地缩短了DMA数据传输时间。另一方面，在压缩率为50\%（32位浮点数压缩为16位数）的情况下，神威超算上可解决的最大问题规模扩大了一倍。这是一种通过软件形式突破硬件内存限制的方法。

实时压缩算法的精度对地震模拟有至关重要的影响，本研究通过理论和实践分析每个变量的精度敏感性，为其定制合适的压缩算法。经过精细的调优操作为不同变量选取了可接受精度范围内最大化性能的压缩算法。压缩算法的精度算例分析将在唐山大地震算例中详细描述。

% 流水线指令重排优化

% chapter 面向申威26010异构众核架构的并行优化方法 (end)